{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "from sklearn.decomposition import PCA\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler \n",
    "# from sklearn.model_selection import StratifiedKFold\n",
    "# from sklearn.metrics import log_loss\n",
    "# from tqdm.notebook import tqdm\n",
    "# from time import time\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.multiclass import OneVsRestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir('lish-moa')\n",
    "train_features = pd.read_csv('lish-moa/train_features.csv')\n",
    "train_targets_scored = pd.read_csv('lish-moa/train_targets_scored.csv')\n",
    "train_targets_nonscored = pd.read_csv('lish-moa/train_targets_nonscored.csv')\n",
    "test_features = pd.read_csv('lish-moa/test_features.csv')\n",
    "submission = pd.read_csv('lish-moa/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cp_type\n",
       "ctl_vehicle        0\n",
       "trt_cp         16844\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is inspired by the discussion here: https://www.kaggle.com/c/lish-moa/discussion/180165\n",
    "train = train_features.merge(train_targets_scored, on = 'sig_id')\n",
    "cols = [c for c in train_targets_scored.columns] + ['cp_type']\n",
    "train[cols].groupby('cp_type').sum().sum(1)\n",
    "# As a result, if cp_type is ctl_vehicle, we predict all to be zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing and Feature Engineering\n",
    "my plan here is to try PCA or tsne, then use Random Forest and XGBoost. Lastly, Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sig_id</th>\n",
       "      <th>cp_type</th>\n",
       "      <th>cp_time</th>\n",
       "      <th>cp_dose</th>\n",
       "      <th>g-0</th>\n",
       "      <th>g-1</th>\n",
       "      <th>g-2</th>\n",
       "      <th>g-3</th>\n",
       "      <th>g-4</th>\n",
       "      <th>g-5</th>\n",
       "      <th>...</th>\n",
       "      <th>c-90</th>\n",
       "      <th>c-91</th>\n",
       "      <th>c-92</th>\n",
       "      <th>c-93</th>\n",
       "      <th>c-94</th>\n",
       "      <th>c-95</th>\n",
       "      <th>c-96</th>\n",
       "      <th>c-97</th>\n",
       "      <th>c-98</th>\n",
       "      <th>c-99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id_000644bb2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0620</td>\n",
       "      <td>0.5577</td>\n",
       "      <td>-0.2479</td>\n",
       "      <td>-0.6208</td>\n",
       "      <td>-0.1944</td>\n",
       "      <td>-1.0120</td>\n",
       "      <td>...</td>\n",
       "      <td>0.2862</td>\n",
       "      <td>0.2584</td>\n",
       "      <td>0.8076</td>\n",
       "      <td>0.5523</td>\n",
       "      <td>-0.1912</td>\n",
       "      <td>0.6584</td>\n",
       "      <td>-0.3981</td>\n",
       "      <td>0.2139</td>\n",
       "      <td>0.3801</td>\n",
       "      <td>0.4176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id_000779bfc</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0743</td>\n",
       "      <td>0.4087</td>\n",
       "      <td>0.2991</td>\n",
       "      <td>0.0604</td>\n",
       "      <td>1.0190</td>\n",
       "      <td>0.5207</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.4265</td>\n",
       "      <td>0.7543</td>\n",
       "      <td>0.4708</td>\n",
       "      <td>0.0230</td>\n",
       "      <td>0.2957</td>\n",
       "      <td>0.4899</td>\n",
       "      <td>0.1522</td>\n",
       "      <td>0.1241</td>\n",
       "      <td>0.6077</td>\n",
       "      <td>0.7371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id_000a6266a</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.6280</td>\n",
       "      <td>0.5817</td>\n",
       "      <td>1.5540</td>\n",
       "      <td>-0.0764</td>\n",
       "      <td>-0.0323</td>\n",
       "      <td>1.2390</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.7250</td>\n",
       "      <td>-0.6297</td>\n",
       "      <td>0.6103</td>\n",
       "      <td>0.0223</td>\n",
       "      <td>-1.3240</td>\n",
       "      <td>-0.3174</td>\n",
       "      <td>-0.6417</td>\n",
       "      <td>-0.2187</td>\n",
       "      <td>-1.4080</td>\n",
       "      <td>0.6931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id_0015fd391</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.5138</td>\n",
       "      <td>-0.2491</td>\n",
       "      <td>-0.2656</td>\n",
       "      <td>0.5288</td>\n",
       "      <td>4.0620</td>\n",
       "      <td>-0.8095</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.0990</td>\n",
       "      <td>-0.6441</td>\n",
       "      <td>-5.6300</td>\n",
       "      <td>-1.3780</td>\n",
       "      <td>-0.8632</td>\n",
       "      <td>-1.2880</td>\n",
       "      <td>-1.6210</td>\n",
       "      <td>-0.8784</td>\n",
       "      <td>-0.3876</td>\n",
       "      <td>-0.8154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id_001626bd3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.3254</td>\n",
       "      <td>-0.4009</td>\n",
       "      <td>0.9700</td>\n",
       "      <td>0.6919</td>\n",
       "      <td>1.4180</td>\n",
       "      <td>-0.8244</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0042</td>\n",
       "      <td>0.0048</td>\n",
       "      <td>0.6670</td>\n",
       "      <td>1.0690</td>\n",
       "      <td>0.5523</td>\n",
       "      <td>-0.3031</td>\n",
       "      <td>0.1094</td>\n",
       "      <td>0.2885</td>\n",
       "      <td>-0.3786</td>\n",
       "      <td>0.7125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23809</th>\n",
       "      <td>id_fffb1ceed</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1394</td>\n",
       "      <td>-0.0636</td>\n",
       "      <td>-0.1112</td>\n",
       "      <td>-0.5080</td>\n",
       "      <td>-0.4713</td>\n",
       "      <td>0.7201</td>\n",
       "      <td>...</td>\n",
       "      <td>0.1969</td>\n",
       "      <td>0.0262</td>\n",
       "      <td>-0.8121</td>\n",
       "      <td>0.3434</td>\n",
       "      <td>0.5372</td>\n",
       "      <td>-0.3246</td>\n",
       "      <td>0.0631</td>\n",
       "      <td>0.9171</td>\n",
       "      <td>0.5258</td>\n",
       "      <td>0.4680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23810</th>\n",
       "      <td>id_fffb70c0c</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.3260</td>\n",
       "      <td>0.3478</td>\n",
       "      <td>-0.3743</td>\n",
       "      <td>0.9905</td>\n",
       "      <td>-0.7178</td>\n",
       "      <td>0.6621</td>\n",
       "      <td>...</td>\n",
       "      <td>0.4286</td>\n",
       "      <td>0.4426</td>\n",
       "      <td>0.0423</td>\n",
       "      <td>-0.3195</td>\n",
       "      <td>-0.8086</td>\n",
       "      <td>-0.9798</td>\n",
       "      <td>-0.2084</td>\n",
       "      <td>-0.1224</td>\n",
       "      <td>-0.2715</td>\n",
       "      <td>0.3689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23811</th>\n",
       "      <td>id_fffc1c3f4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.3942</td>\n",
       "      <td>0.3756</td>\n",
       "      <td>0.3109</td>\n",
       "      <td>-0.7389</td>\n",
       "      <td>0.5505</td>\n",
       "      <td>-0.0159</td>\n",
       "      <td>...</td>\n",
       "      <td>0.5409</td>\n",
       "      <td>0.3755</td>\n",
       "      <td>0.7343</td>\n",
       "      <td>0.2807</td>\n",
       "      <td>0.4116</td>\n",
       "      <td>0.6422</td>\n",
       "      <td>0.2256</td>\n",
       "      <td>0.7592</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.3808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23812</th>\n",
       "      <td>id_fffcb9e7c</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.6660</td>\n",
       "      <td>0.2324</td>\n",
       "      <td>0.4392</td>\n",
       "      <td>0.2044</td>\n",
       "      <td>0.8531</td>\n",
       "      <td>-0.0343</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.1105</td>\n",
       "      <td>0.4258</td>\n",
       "      <td>-0.2012</td>\n",
       "      <td>0.1506</td>\n",
       "      <td>1.5230</td>\n",
       "      <td>0.7101</td>\n",
       "      <td>0.1732</td>\n",
       "      <td>0.7015</td>\n",
       "      <td>-0.6290</td>\n",
       "      <td>0.0740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23813</th>\n",
       "      <td>id_ffffdd77b</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.8598</td>\n",
       "      <td>1.0240</td>\n",
       "      <td>-0.1361</td>\n",
       "      <td>0.7952</td>\n",
       "      <td>-0.3611</td>\n",
       "      <td>-3.6750</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.3890</td>\n",
       "      <td>-1.7450</td>\n",
       "      <td>-6.6300</td>\n",
       "      <td>-4.0950</td>\n",
       "      <td>-7.3860</td>\n",
       "      <td>-1.4160</td>\n",
       "      <td>-3.5770</td>\n",
       "      <td>-0.4775</td>\n",
       "      <td>-2.1500</td>\n",
       "      <td>-4.2520</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>23814 rows × 876 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             sig_id  cp_type  cp_time  cp_dose     g-0     g-1     g-2  \\\n",
       "0      id_000644bb2        0        0        0  1.0620  0.5577 -0.2479   \n",
       "1      id_000779bfc        0        2        0  0.0743  0.4087  0.2991   \n",
       "2      id_000a6266a        0        1        0  0.6280  0.5817  1.5540   \n",
       "3      id_0015fd391        0        1        0 -0.5138 -0.2491 -0.2656   \n",
       "4      id_001626bd3        0        2        1 -0.3254 -0.4009  0.9700   \n",
       "...             ...      ...      ...      ...     ...     ...     ...   \n",
       "23809  id_fffb1ceed        0        0        1  0.1394 -0.0636 -0.1112   \n",
       "23810  id_fffb70c0c        0        0        1 -1.3260  0.3478 -0.3743   \n",
       "23811  id_fffc1c3f4        1        1        1  0.3942  0.3756  0.3109   \n",
       "23812  id_fffcb9e7c        0        0        0  0.6660  0.2324  0.4392   \n",
       "23813  id_ffffdd77b        0        2        0 -0.8598  1.0240 -0.1361   \n",
       "\n",
       "          g-3     g-4     g-5  ...    c-90    c-91    c-92    c-93    c-94  \\\n",
       "0     -0.6208 -0.1944 -1.0120  ...  0.2862  0.2584  0.8076  0.5523 -0.1912   \n",
       "1      0.0604  1.0190  0.5207  ... -0.4265  0.7543  0.4708  0.0230  0.2957   \n",
       "2     -0.0764 -0.0323  1.2390  ... -0.7250 -0.6297  0.6103  0.0223 -1.3240   \n",
       "3      0.5288  4.0620 -0.8095  ... -2.0990 -0.6441 -5.6300 -1.3780 -0.8632   \n",
       "4      0.6919  1.4180 -0.8244  ...  0.0042  0.0048  0.6670  1.0690  0.5523   \n",
       "...       ...     ...     ...  ...     ...     ...     ...     ...     ...   \n",
       "23809 -0.5080 -0.4713  0.7201  ...  0.1969  0.0262 -0.8121  0.3434  0.5372   \n",
       "23810  0.9905 -0.7178  0.6621  ...  0.4286  0.4426  0.0423 -0.3195 -0.8086   \n",
       "23811 -0.7389  0.5505 -0.0159  ...  0.5409  0.3755  0.7343  0.2807  0.4116   \n",
       "23812  0.2044  0.8531 -0.0343  ... -0.1105  0.4258 -0.2012  0.1506  1.5230   \n",
       "23813  0.7952 -0.3611 -3.6750  ... -3.3890 -1.7450 -6.6300 -4.0950 -7.3860   \n",
       "\n",
       "         c-95    c-96    c-97    c-98    c-99  \n",
       "0      0.6584 -0.3981  0.2139  0.3801  0.4176  \n",
       "1      0.4899  0.1522  0.1241  0.6077  0.7371  \n",
       "2     -0.3174 -0.6417 -0.2187 -1.4080  0.6931  \n",
       "3     -1.2880 -1.6210 -0.8784 -0.3876 -0.8154  \n",
       "4     -0.3031  0.1094  0.2885 -0.3786  0.7125  \n",
       "...       ...     ...     ...     ...     ...  \n",
       "23809 -0.3246  0.0631  0.9171  0.5258  0.4680  \n",
       "23810 -0.9798 -0.2084 -0.1224 -0.2715  0.3689  \n",
       "23811  0.6422  0.2256  0.7592  0.6656  0.3808  \n",
       "23812  0.7101  0.1732  0.7015 -0.6290  0.0740  \n",
       "23813 -1.4160 -3.5770 -0.4775 -2.1500 -4.2520  \n",
       "\n",
       "[23814 rows x 876 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# removing cases when cp_type is ctl_vehicle\n",
    "#train = train[train['cp_type']!='ctl_vehicle'].reset_index(drop=True)\n",
    "# transform the TWO categorical variables into numerical variables\n",
    "# this is label encoding, may try one hot encoding later\n",
    "train['cp_time'] = train['cp_time'].map({24:0, 48:1, 72:2})\n",
    "train['cp_dose'] = train['cp_dose'].map({'D1':0,'D2':1})\n",
    "train['cp_type'] = train['cp_type'].map({'trt_cp':0,'ctl_vehicle':1})\n",
    "#make id index\n",
    "#train = train.set_index(['sig_id'])\n",
    "\n",
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sig_id</th>\n",
       "      <th>cp_type</th>\n",
       "      <th>cp_time</th>\n",
       "      <th>cp_dose</th>\n",
       "      <th>g-0</th>\n",
       "      <th>g-1</th>\n",
       "      <th>g-2</th>\n",
       "      <th>g-3</th>\n",
       "      <th>g-4</th>\n",
       "      <th>g-5</th>\n",
       "      <th>...</th>\n",
       "      <th>c-90</th>\n",
       "      <th>c-91</th>\n",
       "      <th>c-92</th>\n",
       "      <th>c-93</th>\n",
       "      <th>c-94</th>\n",
       "      <th>c-95</th>\n",
       "      <th>c-96</th>\n",
       "      <th>c-97</th>\n",
       "      <th>c-98</th>\n",
       "      <th>c-99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id_000644bb2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0620</td>\n",
       "      <td>0.5577</td>\n",
       "      <td>-0.2479</td>\n",
       "      <td>-0.6208</td>\n",
       "      <td>-0.1944</td>\n",
       "      <td>-1.0120</td>\n",
       "      <td>...</td>\n",
       "      <td>0.2862</td>\n",
       "      <td>0.2584</td>\n",
       "      <td>0.8076</td>\n",
       "      <td>0.5523</td>\n",
       "      <td>-0.1912</td>\n",
       "      <td>0.6584</td>\n",
       "      <td>-0.3981</td>\n",
       "      <td>0.2139</td>\n",
       "      <td>0.3801</td>\n",
       "      <td>0.4176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id_000779bfc</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0743</td>\n",
       "      <td>0.4087</td>\n",
       "      <td>0.2991</td>\n",
       "      <td>0.0604</td>\n",
       "      <td>1.0190</td>\n",
       "      <td>0.5207</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.4265</td>\n",
       "      <td>0.7543</td>\n",
       "      <td>0.4708</td>\n",
       "      <td>0.0230</td>\n",
       "      <td>0.2957</td>\n",
       "      <td>0.4899</td>\n",
       "      <td>0.1522</td>\n",
       "      <td>0.1241</td>\n",
       "      <td>0.6077</td>\n",
       "      <td>0.7371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id_000a6266a</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.6280</td>\n",
       "      <td>0.5817</td>\n",
       "      <td>1.5540</td>\n",
       "      <td>-0.0764</td>\n",
       "      <td>-0.0323</td>\n",
       "      <td>1.2390</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.7250</td>\n",
       "      <td>-0.6297</td>\n",
       "      <td>0.6103</td>\n",
       "      <td>0.0223</td>\n",
       "      <td>-1.3240</td>\n",
       "      <td>-0.3174</td>\n",
       "      <td>-0.6417</td>\n",
       "      <td>-0.2187</td>\n",
       "      <td>-1.4080</td>\n",
       "      <td>0.6931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id_0015fd391</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.5138</td>\n",
       "      <td>-0.2491</td>\n",
       "      <td>-0.2656</td>\n",
       "      <td>0.5288</td>\n",
       "      <td>4.0620</td>\n",
       "      <td>-0.8095</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.0990</td>\n",
       "      <td>-0.6441</td>\n",
       "      <td>-5.6300</td>\n",
       "      <td>-1.3780</td>\n",
       "      <td>-0.8632</td>\n",
       "      <td>-1.2880</td>\n",
       "      <td>-1.6210</td>\n",
       "      <td>-0.8784</td>\n",
       "      <td>-0.3876</td>\n",
       "      <td>-0.8154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id_001626bd3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.3254</td>\n",
       "      <td>-0.4009</td>\n",
       "      <td>0.9700</td>\n",
       "      <td>0.6919</td>\n",
       "      <td>1.4180</td>\n",
       "      <td>-0.8244</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0042</td>\n",
       "      <td>0.0048</td>\n",
       "      <td>0.6670</td>\n",
       "      <td>1.0690</td>\n",
       "      <td>0.5523</td>\n",
       "      <td>-0.3031</td>\n",
       "      <td>0.1094</td>\n",
       "      <td>0.2885</td>\n",
       "      <td>-0.3786</td>\n",
       "      <td>0.7125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23809</th>\n",
       "      <td>id_fffb1ceed</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1394</td>\n",
       "      <td>-0.0636</td>\n",
       "      <td>-0.1112</td>\n",
       "      <td>-0.5080</td>\n",
       "      <td>-0.4713</td>\n",
       "      <td>0.7201</td>\n",
       "      <td>...</td>\n",
       "      <td>0.1969</td>\n",
       "      <td>0.0262</td>\n",
       "      <td>-0.8121</td>\n",
       "      <td>0.3434</td>\n",
       "      <td>0.5372</td>\n",
       "      <td>-0.3246</td>\n",
       "      <td>0.0631</td>\n",
       "      <td>0.9171</td>\n",
       "      <td>0.5258</td>\n",
       "      <td>0.4680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23810</th>\n",
       "      <td>id_fffb70c0c</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.3260</td>\n",
       "      <td>0.3478</td>\n",
       "      <td>-0.3743</td>\n",
       "      <td>0.9905</td>\n",
       "      <td>-0.7178</td>\n",
       "      <td>0.6621</td>\n",
       "      <td>...</td>\n",
       "      <td>0.4286</td>\n",
       "      <td>0.4426</td>\n",
       "      <td>0.0423</td>\n",
       "      <td>-0.3195</td>\n",
       "      <td>-0.8086</td>\n",
       "      <td>-0.9798</td>\n",
       "      <td>-0.2084</td>\n",
       "      <td>-0.1224</td>\n",
       "      <td>-0.2715</td>\n",
       "      <td>0.3689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23811</th>\n",
       "      <td>id_fffc1c3f4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.3942</td>\n",
       "      <td>0.3756</td>\n",
       "      <td>0.3109</td>\n",
       "      <td>-0.7389</td>\n",
       "      <td>0.5505</td>\n",
       "      <td>-0.0159</td>\n",
       "      <td>...</td>\n",
       "      <td>0.5409</td>\n",
       "      <td>0.3755</td>\n",
       "      <td>0.7343</td>\n",
       "      <td>0.2807</td>\n",
       "      <td>0.4116</td>\n",
       "      <td>0.6422</td>\n",
       "      <td>0.2256</td>\n",
       "      <td>0.7592</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.3808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23812</th>\n",
       "      <td>id_fffcb9e7c</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.6660</td>\n",
       "      <td>0.2324</td>\n",
       "      <td>0.4392</td>\n",
       "      <td>0.2044</td>\n",
       "      <td>0.8531</td>\n",
       "      <td>-0.0343</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.1105</td>\n",
       "      <td>0.4258</td>\n",
       "      <td>-0.2012</td>\n",
       "      <td>0.1506</td>\n",
       "      <td>1.5230</td>\n",
       "      <td>0.7101</td>\n",
       "      <td>0.1732</td>\n",
       "      <td>0.7015</td>\n",
       "      <td>-0.6290</td>\n",
       "      <td>0.0740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23813</th>\n",
       "      <td>id_ffffdd77b</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.8598</td>\n",
       "      <td>1.0240</td>\n",
       "      <td>-0.1361</td>\n",
       "      <td>0.7952</td>\n",
       "      <td>-0.3611</td>\n",
       "      <td>-3.6750</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.3890</td>\n",
       "      <td>-1.7450</td>\n",
       "      <td>-6.6300</td>\n",
       "      <td>-4.0950</td>\n",
       "      <td>-7.3860</td>\n",
       "      <td>-1.4160</td>\n",
       "      <td>-3.5770</td>\n",
       "      <td>-0.4775</td>\n",
       "      <td>-2.1500</td>\n",
       "      <td>-4.2520</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>23814 rows × 876 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             sig_id  cp_type  cp_time  cp_dose     g-0     g-1     g-2  \\\n",
       "0      id_000644bb2        0        0        0  1.0620  0.5577 -0.2479   \n",
       "1      id_000779bfc        0        2        0  0.0743  0.4087  0.2991   \n",
       "2      id_000a6266a        0        1        0  0.6280  0.5817  1.5540   \n",
       "3      id_0015fd391        0        1        0 -0.5138 -0.2491 -0.2656   \n",
       "4      id_001626bd3        0        2        1 -0.3254 -0.4009  0.9700   \n",
       "...             ...      ...      ...      ...     ...     ...     ...   \n",
       "23809  id_fffb1ceed        0        0        1  0.1394 -0.0636 -0.1112   \n",
       "23810  id_fffb70c0c        0        0        1 -1.3260  0.3478 -0.3743   \n",
       "23811  id_fffc1c3f4        1        1        1  0.3942  0.3756  0.3109   \n",
       "23812  id_fffcb9e7c        0        0        0  0.6660  0.2324  0.4392   \n",
       "23813  id_ffffdd77b        0        2        0 -0.8598  1.0240 -0.1361   \n",
       "\n",
       "          g-3     g-4     g-5  ...    c-90    c-91    c-92    c-93    c-94  \\\n",
       "0     -0.6208 -0.1944 -1.0120  ...  0.2862  0.2584  0.8076  0.5523 -0.1912   \n",
       "1      0.0604  1.0190  0.5207  ... -0.4265  0.7543  0.4708  0.0230  0.2957   \n",
       "2     -0.0764 -0.0323  1.2390  ... -0.7250 -0.6297  0.6103  0.0223 -1.3240   \n",
       "3      0.5288  4.0620 -0.8095  ... -2.0990 -0.6441 -5.6300 -1.3780 -0.8632   \n",
       "4      0.6919  1.4180 -0.8244  ...  0.0042  0.0048  0.6670  1.0690  0.5523   \n",
       "...       ...     ...     ...  ...     ...     ...     ...     ...     ...   \n",
       "23809 -0.5080 -0.4713  0.7201  ...  0.1969  0.0262 -0.8121  0.3434  0.5372   \n",
       "23810  0.9905 -0.7178  0.6621  ...  0.4286  0.4426  0.0423 -0.3195 -0.8086   \n",
       "23811 -0.7389  0.5505 -0.0159  ...  0.5409  0.3755  0.7343  0.2807  0.4116   \n",
       "23812  0.2044  0.8531 -0.0343  ... -0.1105  0.4258 -0.2012  0.1506  1.5230   \n",
       "23813  0.7952 -0.3611 -3.6750  ... -3.3890 -1.7450 -6.6300 -4.0950 -7.3860   \n",
       "\n",
       "         c-95    c-96    c-97    c-98    c-99  \n",
       "0      0.6584 -0.3981  0.2139  0.3801  0.4176  \n",
       "1      0.4899  0.1522  0.1241  0.6077  0.7371  \n",
       "2     -0.3174 -0.6417 -0.2187 -1.4080  0.6931  \n",
       "3     -1.2880 -1.6210 -0.8784 -0.3876 -0.8154  \n",
       "4     -0.3031  0.1094  0.2885 -0.3786  0.7125  \n",
       "...       ...     ...     ...     ...     ...  \n",
       "23809 -0.3246  0.0631  0.9171  0.5258  0.4680  \n",
       "23810 -0.9798 -0.2084 -0.1224 -0.2715  0.3689  \n",
       "23811  0.6422  0.2256  0.7592  0.6656  0.3808  \n",
       "23812  0.7101  0.1732  0.7015 -0.6290  0.0740  \n",
       "23813 -1.4160 -3.5770 -0.4775 -2.1500 -4.2520  \n",
       "\n",
       "[23814 rows x 876 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GENES = [col for col in train_features.columns if col.startswith('g-')]\n",
    "CELLS = [col for col in train_features.columns if col.startswith('c-')]\n",
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #not utilized\n",
    "# #standardize data\n",
    "# for col in (GENES + CELLS):\n",
    "#     transformer = QuantileTransformer(random_state=86, output_distribution=\"normal\")\n",
    "#     vec = train[col].values.reshape(len(train[col].values), 1)\n",
    "#     transformer.fit(vec)\n",
    "\n",
    "#     train[col] = transformer.transform(vec).reshape(1, len(train[col].values))[0]\n",
    "# #this step is utterly unnecessary, I guess this values were standardized before releasing\n",
    "# train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# this part is for presetation\n",
    "PCA-In our data we have about 775 gene expression variables and 100 cell viability variables and 23000+ records in the training dataset. Building an ensemble learning model on this dataset would take a large amount of time, and also we noticed that a lot of cell viability variables are correlated to each other. Therefore we would be looking at dimensionality reduction to overcome these issues. Here we would be implementing Principal Component Analysis(PCA) to achieve dimensionality reduction.\n",
    "When to use PCA?\n",
    "When we want to reduce the number of independent variables but still don't want to lose the information available from those variables.\n",
    "When we want to ensure our variables are independent of each other\n",
    "When we are comfortable with making our independent variables less interpretable\n",
    "Steps we will follow to implement PCA -\n",
    "Remove outliers and standardize the variables\n",
    "Covariance Matrix computation and calculation of Eigen Values. The PCA function in sklearn package takes care of these details and we don't have to worry about them\n",
    "Plot the explained variance by Principal Components and select the number of principal components to include."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pca = PCA().fit(train)\n",
    "\n",
    "# fig, ax = plt.subplots()\n",
    "# xi = np.arange(0,874, step=1)\n",
    "# y = np.cumsum(pca.explained_variance_ratio_)\n",
    "\n",
    "# plt.ylim(0.0,1.2)\n",
    "# plt.plot(xi, y)\n",
    "\n",
    "# plt.xlabel('Number of Components')\n",
    "# #plt.xticks(np.arange(0, 750, step=50)) #change from 0-based array index to 1-based human-readable label\n",
    "# plt.ylabel('Cumulative variance (%)')\n",
    "# plt.title('The number of components needed to explain variance')\n",
    "\n",
    "# plt.axhline(y=0.90, linestyle='-')\n",
    "# plt.text(0.75, 0.3, '95% cut-off threshold')#, color = 'red', fontsize=16)\n",
    "\n",
    "# ax.grid(axis='x')\n",
    "# plt.show()\n",
    "# #we need ~550 pc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-10-77a055c1dc10>:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  col[col>3]=3\n",
      "<ipython-input-10-77a055c1dc10>:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  col[col<-3]=-3\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cp_type</th>\n",
       "      <th>cp_time</th>\n",
       "      <th>cp_dose</th>\n",
       "      <th>g-0</th>\n",
       "      <th>g-1</th>\n",
       "      <th>g-2</th>\n",
       "      <th>g-3</th>\n",
       "      <th>g-4</th>\n",
       "      <th>g-5</th>\n",
       "      <th>g-6</th>\n",
       "      <th>...</th>\n",
       "      <th>c-90</th>\n",
       "      <th>c-91</th>\n",
       "      <th>c-92</th>\n",
       "      <th>c-93</th>\n",
       "      <th>c-94</th>\n",
       "      <th>c-95</th>\n",
       "      <th>c-96</th>\n",
       "      <th>c-97</th>\n",
       "      <th>c-98</th>\n",
       "      <th>c-99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0620</td>\n",
       "      <td>0.5577</td>\n",
       "      <td>-0.2479</td>\n",
       "      <td>-0.6208</td>\n",
       "      <td>-0.1944</td>\n",
       "      <td>-1.0120</td>\n",
       "      <td>-1.0220</td>\n",
       "      <td>...</td>\n",
       "      <td>0.2862</td>\n",
       "      <td>0.2584</td>\n",
       "      <td>0.8076</td>\n",
       "      <td>0.5523</td>\n",
       "      <td>-0.1912</td>\n",
       "      <td>0.6584</td>\n",
       "      <td>-0.3981</td>\n",
       "      <td>0.2139</td>\n",
       "      <td>0.3801</td>\n",
       "      <td>0.4176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0743</td>\n",
       "      <td>0.4087</td>\n",
       "      <td>0.2991</td>\n",
       "      <td>0.0604</td>\n",
       "      <td>1.0190</td>\n",
       "      <td>0.5207</td>\n",
       "      <td>0.2341</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.4265</td>\n",
       "      <td>0.7543</td>\n",
       "      <td>0.4708</td>\n",
       "      <td>0.0230</td>\n",
       "      <td>0.2957</td>\n",
       "      <td>0.4899</td>\n",
       "      <td>0.1522</td>\n",
       "      <td>0.1241</td>\n",
       "      <td>0.6077</td>\n",
       "      <td>0.7371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.6280</td>\n",
       "      <td>0.5817</td>\n",
       "      <td>1.5540</td>\n",
       "      <td>-0.0764</td>\n",
       "      <td>-0.0323</td>\n",
       "      <td>1.2390</td>\n",
       "      <td>0.1715</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.7250</td>\n",
       "      <td>-0.6297</td>\n",
       "      <td>0.6103</td>\n",
       "      <td>0.0223</td>\n",
       "      <td>-1.3240</td>\n",
       "      <td>-0.3174</td>\n",
       "      <td>-0.6417</td>\n",
       "      <td>-0.2187</td>\n",
       "      <td>-1.4080</td>\n",
       "      <td>0.6931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.5138</td>\n",
       "      <td>-0.2491</td>\n",
       "      <td>-0.2656</td>\n",
       "      <td>0.5288</td>\n",
       "      <td>3.0000</td>\n",
       "      <td>-0.8095</td>\n",
       "      <td>-1.9590</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.0990</td>\n",
       "      <td>-0.6441</td>\n",
       "      <td>-3.0000</td>\n",
       "      <td>-1.3780</td>\n",
       "      <td>-0.8632</td>\n",
       "      <td>-1.2880</td>\n",
       "      <td>-1.6210</td>\n",
       "      <td>-0.8784</td>\n",
       "      <td>-0.3876</td>\n",
       "      <td>-0.8154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.3254</td>\n",
       "      <td>-0.4009</td>\n",
       "      <td>0.9700</td>\n",
       "      <td>0.6919</td>\n",
       "      <td>1.4180</td>\n",
       "      <td>-0.8244</td>\n",
       "      <td>-0.2800</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0042</td>\n",
       "      <td>0.0048</td>\n",
       "      <td>0.6670</td>\n",
       "      <td>1.0690</td>\n",
       "      <td>0.5523</td>\n",
       "      <td>-0.3031</td>\n",
       "      <td>0.1094</td>\n",
       "      <td>0.2885</td>\n",
       "      <td>-0.3786</td>\n",
       "      <td>0.7125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23809</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1394</td>\n",
       "      <td>-0.0636</td>\n",
       "      <td>-0.1112</td>\n",
       "      <td>-0.5080</td>\n",
       "      <td>-0.4713</td>\n",
       "      <td>0.7201</td>\n",
       "      <td>0.5773</td>\n",
       "      <td>...</td>\n",
       "      <td>0.1969</td>\n",
       "      <td>0.0262</td>\n",
       "      <td>-0.8121</td>\n",
       "      <td>0.3434</td>\n",
       "      <td>0.5372</td>\n",
       "      <td>-0.3246</td>\n",
       "      <td>0.0631</td>\n",
       "      <td>0.9171</td>\n",
       "      <td>0.5258</td>\n",
       "      <td>0.4680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23810</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.3260</td>\n",
       "      <td>0.3478</td>\n",
       "      <td>-0.3743</td>\n",
       "      <td>0.9905</td>\n",
       "      <td>-0.7178</td>\n",
       "      <td>0.6621</td>\n",
       "      <td>-0.2252</td>\n",
       "      <td>...</td>\n",
       "      <td>0.4286</td>\n",
       "      <td>0.4426</td>\n",
       "      <td>0.0423</td>\n",
       "      <td>-0.3195</td>\n",
       "      <td>-0.8086</td>\n",
       "      <td>-0.9798</td>\n",
       "      <td>-0.2084</td>\n",
       "      <td>-0.1224</td>\n",
       "      <td>-0.2715</td>\n",
       "      <td>0.3689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23811</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.3942</td>\n",
       "      <td>0.3756</td>\n",
       "      <td>0.3109</td>\n",
       "      <td>-0.7389</td>\n",
       "      <td>0.5505</td>\n",
       "      <td>-0.0159</td>\n",
       "      <td>-0.2541</td>\n",
       "      <td>...</td>\n",
       "      <td>0.5409</td>\n",
       "      <td>0.3755</td>\n",
       "      <td>0.7343</td>\n",
       "      <td>0.2807</td>\n",
       "      <td>0.4116</td>\n",
       "      <td>0.6422</td>\n",
       "      <td>0.2256</td>\n",
       "      <td>0.7592</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.3808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23812</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.6660</td>\n",
       "      <td>0.2324</td>\n",
       "      <td>0.4392</td>\n",
       "      <td>0.2044</td>\n",
       "      <td>0.8531</td>\n",
       "      <td>-0.0343</td>\n",
       "      <td>0.0323</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.1105</td>\n",
       "      <td>0.4258</td>\n",
       "      <td>-0.2012</td>\n",
       "      <td>0.1506</td>\n",
       "      <td>1.5230</td>\n",
       "      <td>0.7101</td>\n",
       "      <td>0.1732</td>\n",
       "      <td>0.7015</td>\n",
       "      <td>-0.6290</td>\n",
       "      <td>0.0740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23813</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.8598</td>\n",
       "      <td>1.0240</td>\n",
       "      <td>-0.1361</td>\n",
       "      <td>0.7952</td>\n",
       "      <td>-0.3611</td>\n",
       "      <td>-3.0000</td>\n",
       "      <td>-1.2420</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.0000</td>\n",
       "      <td>-1.7450</td>\n",
       "      <td>-3.0000</td>\n",
       "      <td>-3.0000</td>\n",
       "      <td>-3.0000</td>\n",
       "      <td>-1.4160</td>\n",
       "      <td>-3.0000</td>\n",
       "      <td>-0.4775</td>\n",
       "      <td>-2.1500</td>\n",
       "      <td>-3.0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>23814 rows × 875 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       cp_type  cp_time  cp_dose     g-0     g-1     g-2     g-3     g-4  \\\n",
       "0            0        0        0  1.0620  0.5577 -0.2479 -0.6208 -0.1944   \n",
       "1            0        2        0  0.0743  0.4087  0.2991  0.0604  1.0190   \n",
       "2            0        1        0  0.6280  0.5817  1.5540 -0.0764 -0.0323   \n",
       "3            0        1        0 -0.5138 -0.2491 -0.2656  0.5288  3.0000   \n",
       "4            0        2        1 -0.3254 -0.4009  0.9700  0.6919  1.4180   \n",
       "...        ...      ...      ...     ...     ...     ...     ...     ...   \n",
       "23809        0        0        1  0.1394 -0.0636 -0.1112 -0.5080 -0.4713   \n",
       "23810        0        0        1 -1.3260  0.3478 -0.3743  0.9905 -0.7178   \n",
       "23811        1        1        1  0.3942  0.3756  0.3109 -0.7389  0.5505   \n",
       "23812        0        0        0  0.6660  0.2324  0.4392  0.2044  0.8531   \n",
       "23813        0        2        0 -0.8598  1.0240 -0.1361  0.7952 -0.3611   \n",
       "\n",
       "          g-5     g-6  ...    c-90    c-91    c-92    c-93    c-94    c-95  \\\n",
       "0     -1.0120 -1.0220  ...  0.2862  0.2584  0.8076  0.5523 -0.1912  0.6584   \n",
       "1      0.5207  0.2341  ... -0.4265  0.7543  0.4708  0.0230  0.2957  0.4899   \n",
       "2      1.2390  0.1715  ... -0.7250 -0.6297  0.6103  0.0223 -1.3240 -0.3174   \n",
       "3     -0.8095 -1.9590  ... -2.0990 -0.6441 -3.0000 -1.3780 -0.8632 -1.2880   \n",
       "4     -0.8244 -0.2800  ...  0.0042  0.0048  0.6670  1.0690  0.5523 -0.3031   \n",
       "...       ...     ...  ...     ...     ...     ...     ...     ...     ...   \n",
       "23809  0.7201  0.5773  ...  0.1969  0.0262 -0.8121  0.3434  0.5372 -0.3246   \n",
       "23810  0.6621 -0.2252  ...  0.4286  0.4426  0.0423 -0.3195 -0.8086 -0.9798   \n",
       "23811 -0.0159 -0.2541  ...  0.5409  0.3755  0.7343  0.2807  0.4116  0.6422   \n",
       "23812 -0.0343  0.0323  ... -0.1105  0.4258 -0.2012  0.1506  1.5230  0.7101   \n",
       "23813 -3.0000 -1.2420  ... -3.0000 -1.7450 -3.0000 -3.0000 -3.0000 -1.4160   \n",
       "\n",
       "         c-96    c-97    c-98    c-99  \n",
       "0     -0.3981  0.2139  0.3801  0.4176  \n",
       "1      0.1522  0.1241  0.6077  0.7371  \n",
       "2     -0.6417 -0.2187 -1.4080  0.6931  \n",
       "3     -1.6210 -0.8784 -0.3876 -0.8154  \n",
       "4      0.1094  0.2885 -0.3786  0.7125  \n",
       "...       ...     ...     ...     ...  \n",
       "23809  0.0631  0.9171  0.5258  0.4680  \n",
       "23810 -0.2084 -0.1224 -0.2715  0.3689  \n",
       "23811  0.2256  0.7592  0.6656  0.3808  \n",
       "23812  0.1732  0.7015 -0.6290  0.0740  \n",
       "23813 -3.0000 -0.4775 -2.1500 -3.0000  \n",
       "\n",
       "[23814 rows x 875 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def remove_outliers(col):\n",
    "    col[col>3]=3\n",
    "    col[col<-3]=-3\n",
    "    return col\n",
    "train=train.iloc[:,1:].apply(remove_outliers)\n",
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>c-0</th>\n",
       "      <th>c-1</th>\n",
       "      <th>c-2</th>\n",
       "      <th>c-3</th>\n",
       "      <th>c-4</th>\n",
       "      <th>c-5</th>\n",
       "      <th>c-6</th>\n",
       "      <th>c-7</th>\n",
       "      <th>c-8</th>\n",
       "      <th>c-9</th>\n",
       "      <th>...</th>\n",
       "      <th>c-90</th>\n",
       "      <th>c-91</th>\n",
       "      <th>c-92</th>\n",
       "      <th>c-93</th>\n",
       "      <th>c-94</th>\n",
       "      <th>c-95</th>\n",
       "      <th>c-96</th>\n",
       "      <th>c-97</th>\n",
       "      <th>c-98</th>\n",
       "      <th>c-99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.0600</td>\n",
       "      <td>0.1083</td>\n",
       "      <td>0.6864</td>\n",
       "      <td>0.4043</td>\n",
       "      <td>0.4213</td>\n",
       "      <td>-0.6797</td>\n",
       "      <td>0.2888</td>\n",
       "      <td>0.4323</td>\n",
       "      <td>-0.3381</td>\n",
       "      <td>0.3407</td>\n",
       "      <td>...</td>\n",
       "      <td>0.2862</td>\n",
       "      <td>0.2584</td>\n",
       "      <td>0.8076</td>\n",
       "      <td>0.5523</td>\n",
       "      <td>-0.1912</td>\n",
       "      <td>0.6584</td>\n",
       "      <td>-0.3981</td>\n",
       "      <td>0.2139</td>\n",
       "      <td>0.3801</td>\n",
       "      <td>0.4176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0927</td>\n",
       "      <td>0.2723</td>\n",
       "      <td>0.2772</td>\n",
       "      <td>0.7776</td>\n",
       "      <td>0.3679</td>\n",
       "      <td>0.5696</td>\n",
       "      <td>0.2835</td>\n",
       "      <td>1.4080</td>\n",
       "      <td>0.3745</td>\n",
       "      <td>0.6775</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.4265</td>\n",
       "      <td>0.7543</td>\n",
       "      <td>0.4708</td>\n",
       "      <td>0.0230</td>\n",
       "      <td>0.2957</td>\n",
       "      <td>0.4899</td>\n",
       "      <td>0.1522</td>\n",
       "      <td>0.1241</td>\n",
       "      <td>0.6077</td>\n",
       "      <td>0.7371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.1312</td>\n",
       "      <td>-1.4640</td>\n",
       "      <td>0.3394</td>\n",
       "      <td>-1.7790</td>\n",
       "      <td>0.2188</td>\n",
       "      <td>0.5826</td>\n",
       "      <td>-0.7513</td>\n",
       "      <td>0.0543</td>\n",
       "      <td>0.7182</td>\n",
       "      <td>-0.4159</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.7250</td>\n",
       "      <td>-0.6297</td>\n",
       "      <td>0.6103</td>\n",
       "      <td>0.0223</td>\n",
       "      <td>-1.3240</td>\n",
       "      <td>-0.3174</td>\n",
       "      <td>-0.6417</td>\n",
       "      <td>-0.2187</td>\n",
       "      <td>-1.4080</td>\n",
       "      <td>0.6931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.3998</td>\n",
       "      <td>-3.0000</td>\n",
       "      <td>-2.7350</td>\n",
       "      <td>-1.9630</td>\n",
       "      <td>-2.8610</td>\n",
       "      <td>-1.2670</td>\n",
       "      <td>-2.5830</td>\n",
       "      <td>-0.5036</td>\n",
       "      <td>-3.0000</td>\n",
       "      <td>-1.8510</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.0990</td>\n",
       "      <td>-0.6441</td>\n",
       "      <td>-3.0000</td>\n",
       "      <td>-1.3780</td>\n",
       "      <td>-0.8632</td>\n",
       "      <td>-1.2880</td>\n",
       "      <td>-1.6210</td>\n",
       "      <td>-0.8784</td>\n",
       "      <td>-0.3876</td>\n",
       "      <td>-0.8154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.3774</td>\n",
       "      <td>0.7364</td>\n",
       "      <td>-0.1659</td>\n",
       "      <td>0.2341</td>\n",
       "      <td>1.0060</td>\n",
       "      <td>0.3204</td>\n",
       "      <td>-0.0852</td>\n",
       "      <td>-0.2284</td>\n",
       "      <td>-0.2533</td>\n",
       "      <td>-0.3174</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0042</td>\n",
       "      <td>0.0048</td>\n",
       "      <td>0.6670</td>\n",
       "      <td>1.0690</td>\n",
       "      <td>0.5523</td>\n",
       "      <td>-0.3031</td>\n",
       "      <td>0.1094</td>\n",
       "      <td>0.2885</td>\n",
       "      <td>-0.3786</td>\n",
       "      <td>0.7125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23809</th>\n",
       "      <td>0.4224</td>\n",
       "      <td>0.1871</td>\n",
       "      <td>-0.4822</td>\n",
       "      <td>0.3713</td>\n",
       "      <td>0.4754</td>\n",
       "      <td>0.9512</td>\n",
       "      <td>0.4650</td>\n",
       "      <td>0.3005</td>\n",
       "      <td>0.0338</td>\n",
       "      <td>-0.7734</td>\n",
       "      <td>...</td>\n",
       "      <td>0.1969</td>\n",
       "      <td>0.0262</td>\n",
       "      <td>-0.8121</td>\n",
       "      <td>0.3434</td>\n",
       "      <td>0.5372</td>\n",
       "      <td>-0.3246</td>\n",
       "      <td>0.0631</td>\n",
       "      <td>0.9171</td>\n",
       "      <td>0.5258</td>\n",
       "      <td>0.4680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23810</th>\n",
       "      <td>0.2144</td>\n",
       "      <td>0.4350</td>\n",
       "      <td>0.1174</td>\n",
       "      <td>1.3960</td>\n",
       "      <td>-0.6772</td>\n",
       "      <td>0.2316</td>\n",
       "      <td>-0.5396</td>\n",
       "      <td>0.0581</td>\n",
       "      <td>0.6067</td>\n",
       "      <td>-0.4622</td>\n",
       "      <td>...</td>\n",
       "      <td>0.4286</td>\n",
       "      <td>0.4426</td>\n",
       "      <td>0.0423</td>\n",
       "      <td>-0.3195</td>\n",
       "      <td>-0.8086</td>\n",
       "      <td>-0.9798</td>\n",
       "      <td>-0.2084</td>\n",
       "      <td>-0.1224</td>\n",
       "      <td>-0.2715</td>\n",
       "      <td>0.3689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23811</th>\n",
       "      <td>1.0650</td>\n",
       "      <td>0.6329</td>\n",
       "      <td>0.3742</td>\n",
       "      <td>0.1237</td>\n",
       "      <td>0.6147</td>\n",
       "      <td>0.4589</td>\n",
       "      <td>-0.2372</td>\n",
       "      <td>1.1160</td>\n",
       "      <td>0.4623</td>\n",
       "      <td>0.2830</td>\n",
       "      <td>...</td>\n",
       "      <td>0.5409</td>\n",
       "      <td>0.3755</td>\n",
       "      <td>0.7343</td>\n",
       "      <td>0.2807</td>\n",
       "      <td>0.4116</td>\n",
       "      <td>0.6422</td>\n",
       "      <td>0.2256</td>\n",
       "      <td>0.7592</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.3808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23812</th>\n",
       "      <td>0.5377</td>\n",
       "      <td>1.3240</td>\n",
       "      <td>0.9679</td>\n",
       "      <td>0.1419</td>\n",
       "      <td>1.2230</td>\n",
       "      <td>0.3404</td>\n",
       "      <td>-0.1589</td>\n",
       "      <td>0.8667</td>\n",
       "      <td>0.7749</td>\n",
       "      <td>-0.1458</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.1105</td>\n",
       "      <td>0.4258</td>\n",
       "      <td>-0.2012</td>\n",
       "      <td>0.1506</td>\n",
       "      <td>1.5230</td>\n",
       "      <td>0.7101</td>\n",
       "      <td>0.1732</td>\n",
       "      <td>0.7015</td>\n",
       "      <td>-0.6290</td>\n",
       "      <td>0.0740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23813</th>\n",
       "      <td>-2.3890</td>\n",
       "      <td>-2.0350</td>\n",
       "      <td>-1.1080</td>\n",
       "      <td>-3.0000</td>\n",
       "      <td>-3.0000</td>\n",
       "      <td>-2.0280</td>\n",
       "      <td>-3.0000</td>\n",
       "      <td>-1.3220</td>\n",
       "      <td>-1.9920</td>\n",
       "      <td>-1.6540</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.0000</td>\n",
       "      <td>-1.7450</td>\n",
       "      <td>-3.0000</td>\n",
       "      <td>-3.0000</td>\n",
       "      <td>-3.0000</td>\n",
       "      <td>-1.4160</td>\n",
       "      <td>-3.0000</td>\n",
       "      <td>-0.4775</td>\n",
       "      <td>-2.1500</td>\n",
       "      <td>-3.0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>23814 rows × 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          c-0     c-1     c-2     c-3     c-4     c-5     c-6     c-7     c-8  \\\n",
       "0     -0.0600  0.1083  0.6864  0.4043  0.4213 -0.6797  0.2888  0.4323 -0.3381   \n",
       "1      0.0927  0.2723  0.2772  0.7776  0.3679  0.5696  0.2835  1.4080  0.3745   \n",
       "2     -0.1312 -1.4640  0.3394 -1.7790  0.2188  0.5826 -0.7513  0.0543  0.7182   \n",
       "3     -0.3998 -3.0000 -2.7350 -1.9630 -2.8610 -1.2670 -2.5830 -0.5036 -3.0000   \n",
       "4     -0.3774  0.7364 -0.1659  0.2341  1.0060  0.3204 -0.0852 -0.2284 -0.2533   \n",
       "...       ...     ...     ...     ...     ...     ...     ...     ...     ...   \n",
       "23809  0.4224  0.1871 -0.4822  0.3713  0.4754  0.9512  0.4650  0.3005  0.0338   \n",
       "23810  0.2144  0.4350  0.1174  1.3960 -0.6772  0.2316 -0.5396  0.0581  0.6067   \n",
       "23811  1.0650  0.6329  0.3742  0.1237  0.6147  0.4589 -0.2372  1.1160  0.4623   \n",
       "23812  0.5377  1.3240  0.9679  0.1419  1.2230  0.3404 -0.1589  0.8667  0.7749   \n",
       "23813 -2.3890 -2.0350 -1.1080 -3.0000 -3.0000 -2.0280 -3.0000 -1.3220 -1.9920   \n",
       "\n",
       "          c-9  ...    c-90    c-91    c-92    c-93    c-94    c-95    c-96  \\\n",
       "0      0.3407  ...  0.2862  0.2584  0.8076  0.5523 -0.1912  0.6584 -0.3981   \n",
       "1      0.6775  ... -0.4265  0.7543  0.4708  0.0230  0.2957  0.4899  0.1522   \n",
       "2     -0.4159  ... -0.7250 -0.6297  0.6103  0.0223 -1.3240 -0.3174 -0.6417   \n",
       "3     -1.8510  ... -2.0990 -0.6441 -3.0000 -1.3780 -0.8632 -1.2880 -1.6210   \n",
       "4     -0.3174  ...  0.0042  0.0048  0.6670  1.0690  0.5523 -0.3031  0.1094   \n",
       "...       ...  ...     ...     ...     ...     ...     ...     ...     ...   \n",
       "23809 -0.7734  ...  0.1969  0.0262 -0.8121  0.3434  0.5372 -0.3246  0.0631   \n",
       "23810 -0.4622  ...  0.4286  0.4426  0.0423 -0.3195 -0.8086 -0.9798 -0.2084   \n",
       "23811  0.2830  ...  0.5409  0.3755  0.7343  0.2807  0.4116  0.6422  0.2256   \n",
       "23812 -0.1458  ... -0.1105  0.4258 -0.2012  0.1506  1.5230  0.7101  0.1732   \n",
       "23813 -1.6540  ... -3.0000 -1.7450 -3.0000 -3.0000 -3.0000 -1.4160 -3.0000   \n",
       "\n",
       "         c-97    c-98    c-99  \n",
       "0      0.2139  0.3801  0.4176  \n",
       "1      0.1241  0.6077  0.7371  \n",
       "2     -0.2187 -1.4080  0.6931  \n",
       "3     -0.8784 -0.3876 -0.8154  \n",
       "4      0.2885 -0.3786  0.7125  \n",
       "...       ...     ...     ...  \n",
       "23809  0.9171  0.5258  0.4680  \n",
       "23810 -0.1224 -0.2715  0.3689  \n",
       "23811  0.7592  0.6656  0.3808  \n",
       "23812  0.7015 -0.6290  0.0740  \n",
       "23813 -0.4775 -2.1500 -3.0000  \n",
       "\n",
       "[23814 rows x 100 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#pc for cells\n",
    "\n",
    "train_c=train.loc[:, CELLS]\n",
    "x = StandardScaler().fit_transform(train_c)\n",
    "pca_c = PCA(n_components=80)\n",
    "principalComponents = pca_c.fit_transform(x)\n",
    "principalDf_c = pd.DataFrame(data = principalComponents)\n",
    "\n",
    "plt.plot(np.cumsum(pca_c.explained_variance_ratio_))\n",
    "plt.xlabel('number of components')\n",
    "plt.ylabel('cumulative explained variance')\n",
    "plt.axhline(y=0.80, linestyle='-')\n",
    "plt.title('Cumulative Explained Variance for Cell Viability Variable PCAs')\n",
    "train_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>g-0</th>\n",
       "      <th>g-1</th>\n",
       "      <th>g-2</th>\n",
       "      <th>g-3</th>\n",
       "      <th>g-4</th>\n",
       "      <th>g-5</th>\n",
       "      <th>g-6</th>\n",
       "      <th>g-7</th>\n",
       "      <th>g-8</th>\n",
       "      <th>g-9</th>\n",
       "      <th>...</th>\n",
       "      <th>g-762</th>\n",
       "      <th>g-763</th>\n",
       "      <th>g-764</th>\n",
       "      <th>g-765</th>\n",
       "      <th>g-766</th>\n",
       "      <th>g-767</th>\n",
       "      <th>g-768</th>\n",
       "      <th>g-769</th>\n",
       "      <th>g-770</th>\n",
       "      <th>g-771</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0620</td>\n",
       "      <td>0.5577</td>\n",
       "      <td>-0.2479</td>\n",
       "      <td>-0.6208</td>\n",
       "      <td>-0.1944</td>\n",
       "      <td>-1.0120</td>\n",
       "      <td>-1.0220</td>\n",
       "      <td>-0.0326</td>\n",
       "      <td>0.5548</td>\n",
       "      <td>-0.0921</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.5055</td>\n",
       "      <td>-0.3167</td>\n",
       "      <td>1.0930</td>\n",
       "      <td>0.0084</td>\n",
       "      <td>0.8611</td>\n",
       "      <td>-0.5582</td>\n",
       "      <td>0.3008</td>\n",
       "      <td>1.6490</td>\n",
       "      <td>0.2968</td>\n",
       "      <td>-0.0224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0743</td>\n",
       "      <td>0.4087</td>\n",
       "      <td>0.2991</td>\n",
       "      <td>0.0604</td>\n",
       "      <td>1.0190</td>\n",
       "      <td>0.5207</td>\n",
       "      <td>0.2341</td>\n",
       "      <td>0.3372</td>\n",
       "      <td>-0.4047</td>\n",
       "      <td>0.8507</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.5338</td>\n",
       "      <td>0.0224</td>\n",
       "      <td>-0.4831</td>\n",
       "      <td>0.2128</td>\n",
       "      <td>-0.6999</td>\n",
       "      <td>-0.1214</td>\n",
       "      <td>-0.1626</td>\n",
       "      <td>-0.3340</td>\n",
       "      <td>-0.3289</td>\n",
       "      <td>-0.2718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.6280</td>\n",
       "      <td>0.5817</td>\n",
       "      <td>1.5540</td>\n",
       "      <td>-0.0764</td>\n",
       "      <td>-0.0323</td>\n",
       "      <td>1.2390</td>\n",
       "      <td>0.1715</td>\n",
       "      <td>0.2155</td>\n",
       "      <td>0.0065</td>\n",
       "      <td>1.2300</td>\n",
       "      <td>...</td>\n",
       "      <td>2.5770</td>\n",
       "      <td>0.2356</td>\n",
       "      <td>1.3230</td>\n",
       "      <td>-1.3730</td>\n",
       "      <td>-0.2682</td>\n",
       "      <td>0.8427</td>\n",
       "      <td>0.5797</td>\n",
       "      <td>0.3143</td>\n",
       "      <td>0.8133</td>\n",
       "      <td>0.7923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.5138</td>\n",
       "      <td>-0.2491</td>\n",
       "      <td>-0.2656</td>\n",
       "      <td>0.5288</td>\n",
       "      <td>3.0000</td>\n",
       "      <td>-0.8095</td>\n",
       "      <td>-1.9590</td>\n",
       "      <td>0.1792</td>\n",
       "      <td>-0.1321</td>\n",
       "      <td>-1.0600</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.1292</td>\n",
       "      <td>3.0000</td>\n",
       "      <td>1.2720</td>\n",
       "      <td>-0.4733</td>\n",
       "      <td>-2.0560</td>\n",
       "      <td>0.5699</td>\n",
       "      <td>0.1996</td>\n",
       "      <td>0.4374</td>\n",
       "      <td>0.1588</td>\n",
       "      <td>-0.0343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.3254</td>\n",
       "      <td>-0.4009</td>\n",
       "      <td>0.9700</td>\n",
       "      <td>0.6919</td>\n",
       "      <td>1.4180</td>\n",
       "      <td>-0.8244</td>\n",
       "      <td>-0.2800</td>\n",
       "      <td>-0.1498</td>\n",
       "      <td>-0.8789</td>\n",
       "      <td>0.8630</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.6904</td>\n",
       "      <td>2.0540</td>\n",
       "      <td>-0.3131</td>\n",
       "      <td>-0.0809</td>\n",
       "      <td>0.3910</td>\n",
       "      <td>1.7660</td>\n",
       "      <td>-1.0020</td>\n",
       "      <td>-0.7534</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>-0.6269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23809</th>\n",
       "      <td>0.1394</td>\n",
       "      <td>-0.0636</td>\n",
       "      <td>-0.1112</td>\n",
       "      <td>-0.5080</td>\n",
       "      <td>-0.4713</td>\n",
       "      <td>0.7201</td>\n",
       "      <td>0.5773</td>\n",
       "      <td>0.3055</td>\n",
       "      <td>-0.4726</td>\n",
       "      <td>0.1269</td>\n",
       "      <td>...</td>\n",
       "      <td>0.7790</td>\n",
       "      <td>0.5393</td>\n",
       "      <td>0.4112</td>\n",
       "      <td>-0.5059</td>\n",
       "      <td>0.0240</td>\n",
       "      <td>-0.2297</td>\n",
       "      <td>0.7221</td>\n",
       "      <td>0.5099</td>\n",
       "      <td>-0.1423</td>\n",
       "      <td>0.3806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23810</th>\n",
       "      <td>-1.3260</td>\n",
       "      <td>0.3478</td>\n",
       "      <td>-0.3743</td>\n",
       "      <td>0.9905</td>\n",
       "      <td>-0.7178</td>\n",
       "      <td>0.6621</td>\n",
       "      <td>-0.2252</td>\n",
       "      <td>-0.5565</td>\n",
       "      <td>0.5112</td>\n",
       "      <td>0.6727</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.0858</td>\n",
       "      <td>0.3606</td>\n",
       "      <td>-0.0248</td>\n",
       "      <td>0.0672</td>\n",
       "      <td>-0.5901</td>\n",
       "      <td>-0.1022</td>\n",
       "      <td>0.5247</td>\n",
       "      <td>0.5438</td>\n",
       "      <td>-0.1875</td>\n",
       "      <td>-0.4751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23811</th>\n",
       "      <td>0.3942</td>\n",
       "      <td>0.3756</td>\n",
       "      <td>0.3109</td>\n",
       "      <td>-0.7389</td>\n",
       "      <td>0.5505</td>\n",
       "      <td>-0.0159</td>\n",
       "      <td>-0.2541</td>\n",
       "      <td>0.1745</td>\n",
       "      <td>-0.0340</td>\n",
       "      <td>0.4865</td>\n",
       "      <td>...</td>\n",
       "      <td>0.1796</td>\n",
       "      <td>0.3488</td>\n",
       "      <td>0.0927</td>\n",
       "      <td>0.5166</td>\n",
       "      <td>-0.3099</td>\n",
       "      <td>-0.5946</td>\n",
       "      <td>0.9778</td>\n",
       "      <td>0.2326</td>\n",
       "      <td>-0.6191</td>\n",
       "      <td>0.3603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23812</th>\n",
       "      <td>0.6660</td>\n",
       "      <td>0.2324</td>\n",
       "      <td>0.4392</td>\n",
       "      <td>0.2044</td>\n",
       "      <td>0.8531</td>\n",
       "      <td>-0.0343</td>\n",
       "      <td>0.0323</td>\n",
       "      <td>0.0463</td>\n",
       "      <td>0.4299</td>\n",
       "      <td>-0.7985</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.1410</td>\n",
       "      <td>1.9590</td>\n",
       "      <td>0.8224</td>\n",
       "      <td>1.2500</td>\n",
       "      <td>-3.0000</td>\n",
       "      <td>-2.8720</td>\n",
       "      <td>0.1794</td>\n",
       "      <td>0.3109</td>\n",
       "      <td>-0.3491</td>\n",
       "      <td>-0.4741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23813</th>\n",
       "      <td>-0.8598</td>\n",
       "      <td>1.0240</td>\n",
       "      <td>-0.1361</td>\n",
       "      <td>0.7952</td>\n",
       "      <td>-0.3611</td>\n",
       "      <td>-3.0000</td>\n",
       "      <td>-1.2420</td>\n",
       "      <td>0.9146</td>\n",
       "      <td>3.0000</td>\n",
       "      <td>1.2460</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.5552</td>\n",
       "      <td>3.0000</td>\n",
       "      <td>0.9145</td>\n",
       "      <td>0.8314</td>\n",
       "      <td>1.0610</td>\n",
       "      <td>-0.4017</td>\n",
       "      <td>1.5410</td>\n",
       "      <td>0.3633</td>\n",
       "      <td>-3.0000</td>\n",
       "      <td>2.2190</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>23814 rows × 772 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          g-0     g-1     g-2     g-3     g-4     g-5     g-6     g-7     g-8  \\\n",
       "0      1.0620  0.5577 -0.2479 -0.6208 -0.1944 -1.0120 -1.0220 -0.0326  0.5548   \n",
       "1      0.0743  0.4087  0.2991  0.0604  1.0190  0.5207  0.2341  0.3372 -0.4047   \n",
       "2      0.6280  0.5817  1.5540 -0.0764 -0.0323  1.2390  0.1715  0.2155  0.0065   \n",
       "3     -0.5138 -0.2491 -0.2656  0.5288  3.0000 -0.8095 -1.9590  0.1792 -0.1321   \n",
       "4     -0.3254 -0.4009  0.9700  0.6919  1.4180 -0.8244 -0.2800 -0.1498 -0.8789   \n",
       "...       ...     ...     ...     ...     ...     ...     ...     ...     ...   \n",
       "23809  0.1394 -0.0636 -0.1112 -0.5080 -0.4713  0.7201  0.5773  0.3055 -0.4726   \n",
       "23810 -1.3260  0.3478 -0.3743  0.9905 -0.7178  0.6621 -0.2252 -0.5565  0.5112   \n",
       "23811  0.3942  0.3756  0.3109 -0.7389  0.5505 -0.0159 -0.2541  0.1745 -0.0340   \n",
       "23812  0.6660  0.2324  0.4392  0.2044  0.8531 -0.0343  0.0323  0.0463  0.4299   \n",
       "23813 -0.8598  1.0240 -0.1361  0.7952 -0.3611 -3.0000 -1.2420  0.9146  3.0000   \n",
       "\n",
       "          g-9  ...   g-762   g-763   g-764   g-765   g-766   g-767   g-768  \\\n",
       "0     -0.0921  ... -0.5055 -0.3167  1.0930  0.0084  0.8611 -0.5582  0.3008   \n",
       "1      0.8507  ... -0.5338  0.0224 -0.4831  0.2128 -0.6999 -0.1214 -0.1626   \n",
       "2      1.2300  ...  2.5770  0.2356  1.3230 -1.3730 -0.2682  0.8427  0.5797   \n",
       "3     -1.0600  ... -0.1292  3.0000  1.2720 -0.4733 -2.0560  0.5699  0.1996   \n",
       "4      0.8630  ... -0.6904  2.0540 -0.3131 -0.0809  0.3910  1.7660 -1.0020   \n",
       "...       ...  ...     ...     ...     ...     ...     ...     ...     ...   \n",
       "23809  0.1269  ...  0.7790  0.5393  0.4112 -0.5059  0.0240 -0.2297  0.7221   \n",
       "23810  0.6727  ... -0.0858  0.3606 -0.0248  0.0672 -0.5901 -0.1022  0.5247   \n",
       "23811  0.4865  ...  0.1796  0.3488  0.0927  0.5166 -0.3099 -0.5946  0.9778   \n",
       "23812 -0.7985  ... -0.1410  1.9590  0.8224  1.2500 -3.0000 -2.8720  0.1794   \n",
       "23813  1.2460  ... -0.5552  3.0000  0.9145  0.8314  1.0610 -0.4017  1.5410   \n",
       "\n",
       "        g-769   g-770   g-771  \n",
       "0      1.6490  0.2968 -0.0224  \n",
       "1     -0.3340 -0.3289 -0.2718  \n",
       "2      0.3143  0.8133  0.7923  \n",
       "3      0.4374  0.1588 -0.0343  \n",
       "4     -0.7534  0.5000 -0.6269  \n",
       "...       ...     ...     ...  \n",
       "23809  0.5099 -0.1423  0.3806  \n",
       "23810  0.5438 -0.1875 -0.4751  \n",
       "23811  0.2326 -0.6191  0.3603  \n",
       "23812  0.3109 -0.3491 -0.4741  \n",
       "23813  0.3633 -3.0000  2.2190  \n",
       "\n",
       "[23814 rows x 772 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#pc for gene\n",
    "\n",
    "train_g=train.loc[:, GENES]\n",
    "x = StandardScaler().fit_transform(train_g)\n",
    "pca_g = PCA(n_components=500)\n",
    "principalComponents = pca_g.fit_transform(x)\n",
    "principalDf_g = pd.DataFrame(data = principalComponents)\n",
    "\n",
    "plt.plot(np.cumsum(pca_g.explained_variance_ratio_))\n",
    "plt.xlabel('number of components')\n",
    "plt.ylabel('cumulative explained variance')\n",
    "plt.axhline(y=0.80, linestyle='-')\n",
    "plt.title('Cumulative Explained Variance for Gene Expression Variable PCAs')\n",
    "train_g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For now, I will choose 0.8 -- that gives me 240 principle components for genes and 26 pc for cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cp_type</th>\n",
       "      <th>cp_time</th>\n",
       "      <th>cp_dose</th>\n",
       "      <th>pca_c1</th>\n",
       "      <th>pca_c2</th>\n",
       "      <th>pca_c3</th>\n",
       "      <th>pca_c4</th>\n",
       "      <th>pca_c5</th>\n",
       "      <th>pca_c6</th>\n",
       "      <th>pca_c7</th>\n",
       "      <th>...</th>\n",
       "      <th>pca_g231</th>\n",
       "      <th>pca_g232</th>\n",
       "      <th>pca_g233</th>\n",
       "      <th>pca_g234</th>\n",
       "      <th>pca_g235</th>\n",
       "      <th>pca_g236</th>\n",
       "      <th>pca_g237</th>\n",
       "      <th>pca_g238</th>\n",
       "      <th>pca_g239</th>\n",
       "      <th>pca_g240</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-4.817148</td>\n",
       "      <td>1.062926</td>\n",
       "      <td>-0.589636</td>\n",
       "      <td>-0.575730</td>\n",
       "      <td>-1.081031</td>\n",
       "      <td>-0.548887</td>\n",
       "      <td>-0.472183</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.223639</td>\n",
       "      <td>0.133575</td>\n",
       "      <td>-0.871727</td>\n",
       "      <td>0.101917</td>\n",
       "      <td>-0.206851</td>\n",
       "      <td>0.133056</td>\n",
       "      <td>0.101464</td>\n",
       "      <td>0.886158</td>\n",
       "      <td>-1.586277</td>\n",
       "      <td>0.294565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>-4.997670</td>\n",
       "      <td>-0.215408</td>\n",
       "      <td>-0.167323</td>\n",
       "      <td>0.232766</td>\n",
       "      <td>-0.274137</td>\n",
       "      <td>-0.077560</td>\n",
       "      <td>-0.286073</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.277424</td>\n",
       "      <td>-0.404145</td>\n",
       "      <td>0.019088</td>\n",
       "      <td>-0.091105</td>\n",
       "      <td>1.217435</td>\n",
       "      <td>0.218514</td>\n",
       "      <td>0.131991</td>\n",
       "      <td>0.087781</td>\n",
       "      <td>0.908611</td>\n",
       "      <td>0.074996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.045162</td>\n",
       "      <td>0.530170</td>\n",
       "      <td>0.417546</td>\n",
       "      <td>-0.039593</td>\n",
       "      <td>0.172192</td>\n",
       "      <td>-0.004675</td>\n",
       "      <td>-0.399574</td>\n",
       "      <td>...</td>\n",
       "      <td>0.663110</td>\n",
       "      <td>0.117897</td>\n",
       "      <td>-0.097634</td>\n",
       "      <td>-1.115766</td>\n",
       "      <td>-0.511548</td>\n",
       "      <td>-0.065278</td>\n",
       "      <td>1.467347</td>\n",
       "      <td>1.536210</td>\n",
       "      <td>0.105558</td>\n",
       "      <td>-0.532529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>14.455510</td>\n",
       "      <td>5.272821</td>\n",
       "      <td>-1.244755</td>\n",
       "      <td>3.424090</td>\n",
       "      <td>-2.339055</td>\n",
       "      <td>0.635806</td>\n",
       "      <td>0.409224</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.204965</td>\n",
       "      <td>1.321544</td>\n",
       "      <td>-0.513018</td>\n",
       "      <td>-0.844421</td>\n",
       "      <td>-0.180099</td>\n",
       "      <td>0.774417</td>\n",
       "      <td>-0.422997</td>\n",
       "      <td>-0.497456</td>\n",
       "      <td>0.557420</td>\n",
       "      <td>0.644898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>-3.931571</td>\n",
       "      <td>0.596221</td>\n",
       "      <td>0.255306</td>\n",
       "      <td>-0.510133</td>\n",
       "      <td>0.111545</td>\n",
       "      <td>0.150078</td>\n",
       "      <td>0.163462</td>\n",
       "      <td>...</td>\n",
       "      <td>0.256454</td>\n",
       "      <td>1.235980</td>\n",
       "      <td>-0.273024</td>\n",
       "      <td>-0.274744</td>\n",
       "      <td>-0.240125</td>\n",
       "      <td>0.948021</td>\n",
       "      <td>-0.026208</td>\n",
       "      <td>0.051163</td>\n",
       "      <td>0.376393</td>\n",
       "      <td>0.401757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23809</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-3.921035</td>\n",
       "      <td>-0.098002</td>\n",
       "      <td>-0.460143</td>\n",
       "      <td>1.025173</td>\n",
       "      <td>0.441582</td>\n",
       "      <td>0.137145</td>\n",
       "      <td>-0.885680</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.633394</td>\n",
       "      <td>-0.556416</td>\n",
       "      <td>1.185370</td>\n",
       "      <td>-0.453098</td>\n",
       "      <td>-0.987932</td>\n",
       "      <td>1.650052</td>\n",
       "      <td>-1.709162</td>\n",
       "      <td>0.274779</td>\n",
       "      <td>-0.316664</td>\n",
       "      <td>-0.103777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23810</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.648615</td>\n",
       "      <td>-0.030032</td>\n",
       "      <td>-0.499233</td>\n",
       "      <td>-2.038884</td>\n",
       "      <td>0.518411</td>\n",
       "      <td>0.671420</td>\n",
       "      <td>-0.300262</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.892414</td>\n",
       "      <td>0.609377</td>\n",
       "      <td>0.745017</td>\n",
       "      <td>0.760582</td>\n",
       "      <td>-0.413170</td>\n",
       "      <td>-0.988060</td>\n",
       "      <td>-1.773566</td>\n",
       "      <td>-0.038574</td>\n",
       "      <td>0.344984</td>\n",
       "      <td>-0.023455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23811</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-5.895690</td>\n",
       "      <td>0.718708</td>\n",
       "      <td>-0.829521</td>\n",
       "      <td>0.271426</td>\n",
       "      <td>-0.187698</td>\n",
       "      <td>-0.408464</td>\n",
       "      <td>-0.029757</td>\n",
       "      <td>...</td>\n",
       "      <td>1.165753</td>\n",
       "      <td>0.014225</td>\n",
       "      <td>0.771122</td>\n",
       "      <td>-0.840293</td>\n",
       "      <td>-0.690562</td>\n",
       "      <td>-0.172586</td>\n",
       "      <td>0.728383</td>\n",
       "      <td>-0.955340</td>\n",
       "      <td>0.545388</td>\n",
       "      <td>-0.434309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23812</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-5.389047</td>\n",
       "      <td>0.856159</td>\n",
       "      <td>-0.879516</td>\n",
       "      <td>-0.391346</td>\n",
       "      <td>0.937524</td>\n",
       "      <td>-0.979720</td>\n",
       "      <td>1.128265</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.134373</td>\n",
       "      <td>-0.348500</td>\n",
       "      <td>-0.969733</td>\n",
       "      <td>-0.336896</td>\n",
       "      <td>-0.675034</td>\n",
       "      <td>0.703557</td>\n",
       "      <td>-0.019859</td>\n",
       "      <td>-0.184609</td>\n",
       "      <td>-0.928936</td>\n",
       "      <td>-0.047285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23813</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>20.052371</td>\n",
       "      <td>1.379439</td>\n",
       "      <td>1.238830</td>\n",
       "      <td>0.002209</td>\n",
       "      <td>-0.789444</td>\n",
       "      <td>1.398503</td>\n",
       "      <td>0.354249</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.718479</td>\n",
       "      <td>0.270305</td>\n",
       "      <td>0.180601</td>\n",
       "      <td>0.916041</td>\n",
       "      <td>1.505893</td>\n",
       "      <td>-0.484610</td>\n",
       "      <td>0.356615</td>\n",
       "      <td>-2.208863</td>\n",
       "      <td>2.106624</td>\n",
       "      <td>-1.478129</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>23814 rows × 269 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       cp_type  cp_time  cp_dose     pca_c1    pca_c2    pca_c3    pca_c4  \\\n",
       "0            0        0        0  -4.817148  1.062926 -0.589636 -0.575730   \n",
       "1            0        2        0  -4.997670 -0.215408 -0.167323  0.232766   \n",
       "2            0        1        0   0.045162  0.530170  0.417546 -0.039593   \n",
       "3            0        1        0  14.455510  5.272821 -1.244755  3.424090   \n",
       "4            0        2        1  -3.931571  0.596221  0.255306 -0.510133   \n",
       "...        ...      ...      ...        ...       ...       ...       ...   \n",
       "23809        0        0        1  -3.921035 -0.098002 -0.460143  1.025173   \n",
       "23810        0        0        1  -1.648615 -0.030032 -0.499233 -2.038884   \n",
       "23811        1        1        1  -5.895690  0.718708 -0.829521  0.271426   \n",
       "23812        0        0        0  -5.389047  0.856159 -0.879516 -0.391346   \n",
       "23813        0        2        0  20.052371  1.379439  1.238830  0.002209   \n",
       "\n",
       "         pca_c5    pca_c6    pca_c7  ...  pca_g231  pca_g232  pca_g233  \\\n",
       "0     -1.081031 -0.548887 -0.472183  ... -0.223639  0.133575 -0.871727   \n",
       "1     -0.274137 -0.077560 -0.286073  ... -0.277424 -0.404145  0.019088   \n",
       "2      0.172192 -0.004675 -0.399574  ...  0.663110  0.117897 -0.097634   \n",
       "3     -2.339055  0.635806  0.409224  ... -1.204965  1.321544 -0.513018   \n",
       "4      0.111545  0.150078  0.163462  ...  0.256454  1.235980 -0.273024   \n",
       "...         ...       ...       ...  ...       ...       ...       ...   \n",
       "23809  0.441582  0.137145 -0.885680  ... -0.633394 -0.556416  1.185370   \n",
       "23810  0.518411  0.671420 -0.300262  ... -0.892414  0.609377  0.745017   \n",
       "23811 -0.187698 -0.408464 -0.029757  ...  1.165753  0.014225  0.771122   \n",
       "23812  0.937524 -0.979720  1.128265  ... -0.134373 -0.348500 -0.969733   \n",
       "23813 -0.789444  1.398503  0.354249  ... -0.718479  0.270305  0.180601   \n",
       "\n",
       "       pca_g234  pca_g235  pca_g236  pca_g237  pca_g238  pca_g239  pca_g240  \n",
       "0      0.101917 -0.206851  0.133056  0.101464  0.886158 -1.586277  0.294565  \n",
       "1     -0.091105  1.217435  0.218514  0.131991  0.087781  0.908611  0.074996  \n",
       "2     -1.115766 -0.511548 -0.065278  1.467347  1.536210  0.105558 -0.532529  \n",
       "3     -0.844421 -0.180099  0.774417 -0.422997 -0.497456  0.557420  0.644898  \n",
       "4     -0.274744 -0.240125  0.948021 -0.026208  0.051163  0.376393  0.401757  \n",
       "...         ...       ...       ...       ...       ...       ...       ...  \n",
       "23809 -0.453098 -0.987932  1.650052 -1.709162  0.274779 -0.316664 -0.103777  \n",
       "23810  0.760582 -0.413170 -0.988060 -1.773566 -0.038574  0.344984 -0.023455  \n",
       "23811 -0.840293 -0.690562 -0.172586  0.728383 -0.955340  0.545388 -0.434309  \n",
       "23812 -0.336896 -0.675034  0.703557 -0.019859 -0.184609 -0.928936 -0.047285  \n",
       "23813  0.916041  1.505893 -0.484610  0.356615 -2.208863  2.106624 -1.478129  \n",
       "\n",
       "[23814 rows x 269 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#this part was inspried by one of the notebooks\n",
    "pca_c=principalDf_c.iloc[:,:26]\n",
    "names=[]\n",
    "for i in range(1,27):\n",
    "    var='pca_c'+str(i)\n",
    "    names.append(var)\n",
    "pca_c.columns=names\n",
    "\n",
    "pca_g=principalDf_g.iloc[:,:240]\n",
    "names=[]\n",
    "for i in range(1,241):\n",
    "    var='pca_g'+str(i)\n",
    "    names.append(var)\n",
    "pca_g.columns=names\n",
    "\n",
    "\n",
    "pca_cg=pd.merge(pca_c, pca_g, left_index=True, right_index=True)\n",
    "train_pca = pd.merge(train.iloc[:,:3], pca_cg, left_index=True, right_index=True)\n",
    "train_pca"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOW THAT WE HAVE CREATED OUR PRINCIPLE COMPONENTS, IT'S TIME TO START MODEL FITTING\n",
    "I WILL FIRST TRY LOGISTIC REGRESSION, THE MOST BASIC CLASSIFER. \n",
    "NOTE THAT LOGISTIC REGRESSION ONLY SUPPORTS BINARY CLASSIFICATION AS DEFAULT, BUT WE CAN SOLVE THAT EASILY BY USING  MultiOutputClassifier wrapper in sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cp_time</th>\n",
       "      <th>cp_dose</th>\n",
       "      <th>pca_c1</th>\n",
       "      <th>pca_c2</th>\n",
       "      <th>pca_c3</th>\n",
       "      <th>pca_c4</th>\n",
       "      <th>pca_c5</th>\n",
       "      <th>pca_c6</th>\n",
       "      <th>pca_c7</th>\n",
       "      <th>pca_c8</th>\n",
       "      <th>...</th>\n",
       "      <th>pca_g231</th>\n",
       "      <th>pca_g232</th>\n",
       "      <th>pca_g233</th>\n",
       "      <th>pca_g234</th>\n",
       "      <th>pca_g235</th>\n",
       "      <th>pca_g236</th>\n",
       "      <th>pca_g237</th>\n",
       "      <th>pca_g238</th>\n",
       "      <th>pca_g239</th>\n",
       "      <th>pca_g240</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-4.817148</td>\n",
       "      <td>1.062926</td>\n",
       "      <td>-0.589636</td>\n",
       "      <td>-0.575730</td>\n",
       "      <td>-1.081031</td>\n",
       "      <td>-0.548887</td>\n",
       "      <td>-0.472183</td>\n",
       "      <td>0.122344</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.223639</td>\n",
       "      <td>0.133575</td>\n",
       "      <td>-0.871727</td>\n",
       "      <td>0.101917</td>\n",
       "      <td>-0.206851</td>\n",
       "      <td>0.133056</td>\n",
       "      <td>0.101464</td>\n",
       "      <td>0.886158</td>\n",
       "      <td>-1.586277</td>\n",
       "      <td>0.294565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>-4.997670</td>\n",
       "      <td>-0.215408</td>\n",
       "      <td>-0.167323</td>\n",
       "      <td>0.232766</td>\n",
       "      <td>-0.274137</td>\n",
       "      <td>-0.077560</td>\n",
       "      <td>-0.286073</td>\n",
       "      <td>0.377455</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.277424</td>\n",
       "      <td>-0.404145</td>\n",
       "      <td>0.019088</td>\n",
       "      <td>-0.091105</td>\n",
       "      <td>1.217435</td>\n",
       "      <td>0.218514</td>\n",
       "      <td>0.131991</td>\n",
       "      <td>0.087781</td>\n",
       "      <td>0.908611</td>\n",
       "      <td>0.074996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.045162</td>\n",
       "      <td>0.530170</td>\n",
       "      <td>0.417546</td>\n",
       "      <td>-0.039593</td>\n",
       "      <td>0.172192</td>\n",
       "      <td>-0.004675</td>\n",
       "      <td>-0.399574</td>\n",
       "      <td>-0.263215</td>\n",
       "      <td>...</td>\n",
       "      <td>0.663110</td>\n",
       "      <td>0.117897</td>\n",
       "      <td>-0.097634</td>\n",
       "      <td>-1.115766</td>\n",
       "      <td>-0.511548</td>\n",
       "      <td>-0.065278</td>\n",
       "      <td>1.467347</td>\n",
       "      <td>1.536210</td>\n",
       "      <td>0.105558</td>\n",
       "      <td>-0.532529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>14.455510</td>\n",
       "      <td>5.272821</td>\n",
       "      <td>-1.244755</td>\n",
       "      <td>3.424090</td>\n",
       "      <td>-2.339055</td>\n",
       "      <td>0.635806</td>\n",
       "      <td>0.409224</td>\n",
       "      <td>1.756998</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.204965</td>\n",
       "      <td>1.321544</td>\n",
       "      <td>-0.513018</td>\n",
       "      <td>-0.844421</td>\n",
       "      <td>-0.180099</td>\n",
       "      <td>0.774417</td>\n",
       "      <td>-0.422997</td>\n",
       "      <td>-0.497456</td>\n",
       "      <td>0.557420</td>\n",
       "      <td>0.644898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>-3.931571</td>\n",
       "      <td>0.596221</td>\n",
       "      <td>0.255306</td>\n",
       "      <td>-0.510133</td>\n",
       "      <td>0.111545</td>\n",
       "      <td>0.150078</td>\n",
       "      <td>0.163462</td>\n",
       "      <td>-0.400974</td>\n",
       "      <td>...</td>\n",
       "      <td>0.256454</td>\n",
       "      <td>1.235980</td>\n",
       "      <td>-0.273024</td>\n",
       "      <td>-0.274744</td>\n",
       "      <td>-0.240125</td>\n",
       "      <td>0.948021</td>\n",
       "      <td>-0.026208</td>\n",
       "      <td>0.051163</td>\n",
       "      <td>0.376393</td>\n",
       "      <td>0.401757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23809</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-3.921035</td>\n",
       "      <td>-0.098002</td>\n",
       "      <td>-0.460143</td>\n",
       "      <td>1.025173</td>\n",
       "      <td>0.441582</td>\n",
       "      <td>0.137145</td>\n",
       "      <td>-0.885680</td>\n",
       "      <td>0.619521</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.633394</td>\n",
       "      <td>-0.556416</td>\n",
       "      <td>1.185370</td>\n",
       "      <td>-0.453098</td>\n",
       "      <td>-0.987932</td>\n",
       "      <td>1.650052</td>\n",
       "      <td>-1.709162</td>\n",
       "      <td>0.274779</td>\n",
       "      <td>-0.316664</td>\n",
       "      <td>-0.103777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23810</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.648615</td>\n",
       "      <td>-0.030032</td>\n",
       "      <td>-0.499233</td>\n",
       "      <td>-2.038884</td>\n",
       "      <td>0.518411</td>\n",
       "      <td>0.671420</td>\n",
       "      <td>-0.300262</td>\n",
       "      <td>0.293229</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.892414</td>\n",
       "      <td>0.609377</td>\n",
       "      <td>0.745017</td>\n",
       "      <td>0.760582</td>\n",
       "      <td>-0.413170</td>\n",
       "      <td>-0.988060</td>\n",
       "      <td>-1.773566</td>\n",
       "      <td>-0.038574</td>\n",
       "      <td>0.344984</td>\n",
       "      <td>-0.023455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23811</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-5.895690</td>\n",
       "      <td>0.718708</td>\n",
       "      <td>-0.829521</td>\n",
       "      <td>0.271426</td>\n",
       "      <td>-0.187698</td>\n",
       "      <td>-0.408464</td>\n",
       "      <td>-0.029757</td>\n",
       "      <td>0.342337</td>\n",
       "      <td>...</td>\n",
       "      <td>1.165753</td>\n",
       "      <td>0.014225</td>\n",
       "      <td>0.771122</td>\n",
       "      <td>-0.840293</td>\n",
       "      <td>-0.690562</td>\n",
       "      <td>-0.172586</td>\n",
       "      <td>0.728383</td>\n",
       "      <td>-0.955340</td>\n",
       "      <td>0.545388</td>\n",
       "      <td>-0.434309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23812</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-5.389047</td>\n",
       "      <td>0.856159</td>\n",
       "      <td>-0.879516</td>\n",
       "      <td>-0.391346</td>\n",
       "      <td>0.937524</td>\n",
       "      <td>-0.979720</td>\n",
       "      <td>1.128265</td>\n",
       "      <td>-0.438819</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.134373</td>\n",
       "      <td>-0.348500</td>\n",
       "      <td>-0.969733</td>\n",
       "      <td>-0.336896</td>\n",
       "      <td>-0.675034</td>\n",
       "      <td>0.703557</td>\n",
       "      <td>-0.019859</td>\n",
       "      <td>-0.184609</td>\n",
       "      <td>-0.928936</td>\n",
       "      <td>-0.047285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23813</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>20.052371</td>\n",
       "      <td>1.379439</td>\n",
       "      <td>1.238830</td>\n",
       "      <td>0.002209</td>\n",
       "      <td>-0.789444</td>\n",
       "      <td>1.398503</td>\n",
       "      <td>0.354249</td>\n",
       "      <td>-0.627378</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.718479</td>\n",
       "      <td>0.270305</td>\n",
       "      <td>0.180601</td>\n",
       "      <td>0.916041</td>\n",
       "      <td>1.505893</td>\n",
       "      <td>-0.484610</td>\n",
       "      <td>0.356615</td>\n",
       "      <td>-2.208863</td>\n",
       "      <td>2.106624</td>\n",
       "      <td>-1.478129</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>23814 rows × 268 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       cp_time  cp_dose     pca_c1    pca_c2    pca_c3    pca_c4    pca_c5  \\\n",
       "0            0        0  -4.817148  1.062926 -0.589636 -0.575730 -1.081031   \n",
       "1            2        0  -4.997670 -0.215408 -0.167323  0.232766 -0.274137   \n",
       "2            1        0   0.045162  0.530170  0.417546 -0.039593  0.172192   \n",
       "3            1        0  14.455510  5.272821 -1.244755  3.424090 -2.339055   \n",
       "4            2        1  -3.931571  0.596221  0.255306 -0.510133  0.111545   \n",
       "...        ...      ...        ...       ...       ...       ...       ...   \n",
       "23809        0        1  -3.921035 -0.098002 -0.460143  1.025173  0.441582   \n",
       "23810        0        1  -1.648615 -0.030032 -0.499233 -2.038884  0.518411   \n",
       "23811        1        1  -5.895690  0.718708 -0.829521  0.271426 -0.187698   \n",
       "23812        0        0  -5.389047  0.856159 -0.879516 -0.391346  0.937524   \n",
       "23813        2        0  20.052371  1.379439  1.238830  0.002209 -0.789444   \n",
       "\n",
       "         pca_c6    pca_c7    pca_c8  ...  pca_g231  pca_g232  pca_g233  \\\n",
       "0     -0.548887 -0.472183  0.122344  ... -0.223639  0.133575 -0.871727   \n",
       "1     -0.077560 -0.286073  0.377455  ... -0.277424 -0.404145  0.019088   \n",
       "2     -0.004675 -0.399574 -0.263215  ...  0.663110  0.117897 -0.097634   \n",
       "3      0.635806  0.409224  1.756998  ... -1.204965  1.321544 -0.513018   \n",
       "4      0.150078  0.163462 -0.400974  ...  0.256454  1.235980 -0.273024   \n",
       "...         ...       ...       ...  ...       ...       ...       ...   \n",
       "23809  0.137145 -0.885680  0.619521  ... -0.633394 -0.556416  1.185370   \n",
       "23810  0.671420 -0.300262  0.293229  ... -0.892414  0.609377  0.745017   \n",
       "23811 -0.408464 -0.029757  0.342337  ...  1.165753  0.014225  0.771122   \n",
       "23812 -0.979720  1.128265 -0.438819  ... -0.134373 -0.348500 -0.969733   \n",
       "23813  1.398503  0.354249 -0.627378  ... -0.718479  0.270305  0.180601   \n",
       "\n",
       "       pca_g234  pca_g235  pca_g236  pca_g237  pca_g238  pca_g239  pca_g240  \n",
       "0      0.101917 -0.206851  0.133056  0.101464  0.886158 -1.586277  0.294565  \n",
       "1     -0.091105  1.217435  0.218514  0.131991  0.087781  0.908611  0.074996  \n",
       "2     -1.115766 -0.511548 -0.065278  1.467347  1.536210  0.105558 -0.532529  \n",
       "3     -0.844421 -0.180099  0.774417 -0.422997 -0.497456  0.557420  0.644898  \n",
       "4     -0.274744 -0.240125  0.948021 -0.026208  0.051163  0.376393  0.401757  \n",
       "...         ...       ...       ...       ...       ...       ...       ...  \n",
       "23809 -0.453098 -0.987932  1.650052 -1.709162  0.274779 -0.316664 -0.103777  \n",
       "23810  0.760582 -0.413170 -0.988060 -1.773566 -0.038574  0.344984 -0.023455  \n",
       "23811 -0.840293 -0.690562 -0.172586  0.728383 -0.955340  0.545388 -0.434309  \n",
       "23812 -0.336896 -0.675034  0.703557 -0.019859 -0.184609 -0.928936 -0.047285  \n",
       "23813  0.916041  1.505893 -0.484610  0.356615 -2.208863  2.106624 -1.478129  \n",
       "\n",
       "[23814 rows x 268 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_pca.drop(['cp_type'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#还没用\n",
    "# Function to extract common stats features\n",
    "def fe_stats(train, test):\n",
    "    \n",
    "    features_g = list(train.columns[4:776])\n",
    "    features_c = list(train.columns[776:876])\n",
    "    \n",
    "    for df in [train, test]:\n",
    "        df['g_mean'] = df[features_g].mean(axis = 1)\n",
    "        df['g_std'] = df[features_g].std(axis = 1)\n",
    "        df['c_mean'] = df[features_c].mean(axis = 1)\n",
    "        df['c_std'] = df[features_c].std(axis = 1)\n",
    "        df['gc_mean'] = df[features_g + features_c].mean(axis = 1)\n",
    "        df['gc_std'] = df[features_g + features_c].std(axis = 1)\n",
    "\n",
    "        \n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2:1\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    train_pca, train_targets_scored.iloc[:,1:], test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An intuitive approach to solving multi-label problem is to decompose it into multiple independent binary classification problems \n",
    "In an “one-to-rest” strategy, one could build multiple independent classifiers and, for an unseen instance, choose the class for which the confidence is maximized.\n",
    "The main assumption here is that the labels are mutually exclusive. You do not consider any underlying correlation between the classes in this method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn_classifier = KNeighborsClassifier()\n",
    "knn_classifier.fit(X_train, y_train)\n",
    "knn_predictions_prob = knn_classifier.predict_proba(X_test)\n",
    "knn_predictions = knn_classifier.predict(X_test)\n",
    "from sklearn.metrics import confusion_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[7822,    1],\n",
       "       [  22,   14]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "\n",
    "example = multilabel_confusion_matrix(y_test, knn_predictions)[-12]\n",
    "example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "try:\n",
    "    roc_auc_score(y_test,knn_predictions)\n",
    "except ValueError:\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[7822,    0],\n",
       "       [  11,   26]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example[0][1] = 0\n",
    "example[1][0] = 11\n",
    "example[1][1] = 26\n",
    "example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7859, 206)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yianding/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_ranking.py:811: UndefinedMetricWarning: No positive samples in y_true, true positive value should be meaningless\n",
      "  warnings.warn(\"No positive samples in y_true, \"\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import auc\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "for i in range(y_test.shape[1]):\n",
    "    fpr[i], tpr[i], _ = roc_curve(y_test.iloc[:, i], knn_predictions[:, i])\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatically created module for IPython interactive environment\n"
     ]
    }
   ],
   "source": [
    "print(__doc__)\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import cycle\n",
    "\n",
    "from sklearn import svm, datasets\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from scipy import interp\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# # Import some data to play with\n",
    "# iris = datasets.load_iris()\n",
    "# X = iris.data\n",
    "# y = iris.target\n",
    "\n",
    "# Binarize the output\n",
    "y = label_binarize(train_targets_scored.iloc[:,1:], classes=classe)\n",
    "n_classes = y.shape[1]\n",
    "random_state = np.random.RandomState(39)\n",
    "\n",
    "# shuffle and split training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    train_pca, y, test_size=0.33, random_state=42)\n",
    "\n",
    "# Learn to predict each class against the other\n",
    "classifier = OneVsRestClassifier(svm.SVC(kernel='linear', probability=True,\n",
    "                                 random_state=random_state))\n",
    "y_score = classifier.fit(X_train, y_train).decision_function(X_test)\n",
    "\n",
    "# Compute ROC curve and ROC area for each class\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "for i in range(n_classes):\n",
    "    fpr[i], tpr[i], _ = roc_curve(y_test[:, i], y_score[:, i])\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "classe = []\n",
    "for i in range(206):\n",
    "    classe.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Processing 5-alpha_reductase_inhibitor comments...**\n",
      "Test accuracy is 0.9991601931555742\n",
      "[[[   0    4]\n",
      "  [   0 4759]]\n",
      "\n",
      " [[4759    0]\n",
      "  [   4    0]]]\n",
      "\n",
      "\n",
      "**Processing 11-beta-hsd1_inhibitor comments...**\n",
      "Test accuracy is 0.9985303380222549\n",
      "[[[   0    7]\n",
      "  [   0 4756]]\n",
      "\n",
      " [[4756    0]\n",
      "  [   7    0]]]\n",
      "\n",
      "\n",
      "**Processing acat_inhibitor comments...**\n",
      "Test accuracy is 0.9993701448666806\n",
      "[[[   0    3]\n",
      "  [   0 4760]]\n",
      "\n",
      " [[4760    0]\n",
      "  [   3    0]]]\n",
      "\n",
      "\n",
      "**Processing acetylcholine_receptor_agonist comments...**\n",
      "Test accuracy is 0.9924417384001679\n",
      "[[[   0   35]\n",
      "  [   1 4727]]\n",
      "\n",
      " [[4727    1]\n",
      "  [  35    0]]]\n",
      "\n",
      "\n",
      "**Processing acetylcholine_receptor_antagonist comments...**\n",
      "Test accuracy is 0.9863531387780811\n",
      "[[[   0   65]\n",
      "  [   0 4698]]\n",
      "\n",
      " [[4698    0]\n",
      "  [  65    0]]]\n",
      "\n",
      "\n",
      "**Processing acetylcholinesterase_inhibitor comments...**\n",
      "Test accuracy is 0.9968507243334033\n",
      "[[[   0   15]\n",
      "  [   0 4748]]\n",
      "\n",
      " [[4748    0]\n",
      "  [  15    0]]]\n",
      "\n",
      "\n",
      "**Processing adenosine_receptor_agonist comments...**\n",
      "Test accuracy is 0.9972706277556163\n",
      "[[[   0   13]\n",
      "  [   0 4750]]\n",
      "\n",
      " [[4750    0]\n",
      "  [  13    0]]]\n",
      "\n",
      "\n",
      "**Processing adenosine_receptor_antagonist comments...**\n",
      "Test accuracy is 0.9951711106445518\n",
      "[[[   0   23]\n",
      "  [   0 4740]]\n",
      "\n",
      " [[4740    0]\n",
      "  [  23    0]]]\n",
      "\n",
      "\n",
      "**Processing adenylyl_cyclase_activator comments...**\n",
      "Test accuracy is 0.9989502414444678\n",
      "[[[   0    5]\n",
      "  [   0 4758]]\n",
      "\n",
      " [[4758    0]\n",
      "  [   5    0]]]\n",
      "\n",
      "\n",
      "**Processing adrenergic_receptor_agonist comments...**\n",
      "Test accuracy is 0.9876128490447197\n",
      "[[[   1   58]\n",
      "  [   1 4703]]\n",
      "\n",
      " [[4703    1]\n",
      "  [  58    1]]]\n",
      "\n",
      "\n",
      "**Processing adrenergic_receptor_antagonist comments...**\n",
      "Test accuracy is 0.9874028973336133\n",
      "[[[   0   60]\n",
      "  [   0 4703]]\n",
      "\n",
      " [[4703    0]\n",
      "  [  60    0]]]\n",
      "\n",
      "\n",
      "**Processing akt_inhibitor comments...**\n",
      "Test accuracy is 0.9974805794667226\n",
      "[[[   1   10]\n",
      "  [   2 4750]]\n",
      "\n",
      " [[4750    2]\n",
      "  [  10    1]]]\n",
      "\n",
      "\n",
      "**Processing aldehyde_dehydrogenase_inhibitor comments...**\n",
      "Test accuracy is 0.9997900482888935\n",
      "[[[   0    1]\n",
      "  [   0 4762]]\n",
      "\n",
      " [[4762    0]\n",
      "  [   1    0]]]\n",
      "\n",
      "\n",
      "**Processing alk_inhibitor comments...**\n",
      "Test accuracy is 0.9974805794667226\n",
      "[[[   0   11]\n",
      "  [   1 4751]]\n",
      "\n",
      " [[4751    1]\n",
      "  [  11    0]]]\n",
      "\n",
      "\n",
      "**Processing ampk_activator comments...**\n",
      "Test accuracy is 0.9997900482888935\n",
      "[[[   0    1]\n",
      "  [   0 4762]]\n",
      "\n",
      " [[4762    0]\n",
      "  [   1    0]]]\n",
      "\n",
      "\n",
      "**Processing analgesic comments...**\n",
      "Test accuracy is 0.9997900482888935\n",
      "[[[   0    1]\n",
      "  [   0 4762]]\n",
      "\n",
      " [[4762    0]\n",
      "  [   1    0]]]\n",
      "\n",
      "\n",
      "**Processing androgen_receptor_agonist comments...**\n",
      "Test accuracy is 0.9983203863111484\n",
      "[[[   0    8]\n",
      "  [   0 4755]]\n",
      "\n",
      " [[4755    0]\n",
      "  [   8    0]]]\n",
      "\n",
      "\n",
      "**Processing androgen_receptor_antagonist comments...**\n",
      "Test accuracy is 0.9955910140667646\n",
      "[[[   0   21]\n",
      "  [   0 4742]]\n",
      "\n",
      " [[4742    0]\n",
      "  [  21    0]]]\n",
      "\n",
      "\n",
      "**Processing anesthetic_-_local comments...**\n",
      "Test accuracy is 0.9979004828889355\n",
      "[[[   0    8]\n",
      "  [   2 4753]]\n",
      "\n",
      " [[4753    2]\n",
      "  [   8    0]]]\n",
      "\n",
      "\n",
      "**Processing angiogenesis_inhibitor comments...**\n",
      "Test accuracy is 0.9989502414444678\n",
      "[[[   0    5]\n",
      "  [   0 4758]]\n",
      "\n",
      " [[4758    0]\n",
      "  [   5    0]]]\n",
      "\n",
      "\n",
      "**Processing angiotensin_receptor_antagonist comments...**\n",
      "Test accuracy is 0.9979004828889355\n",
      "[[[   0    9]\n",
      "  [   1 4753]]\n",
      "\n",
      " [[4753    1]\n",
      "  [   9    0]]]\n",
      "\n",
      "\n",
      "**Processing anti-inflammatory comments...**\n",
      "Test accuracy is 0.9966407726222969\n",
      "[[[   0   14]\n",
      "  [   2 4747]]\n",
      "\n",
      " [[4747    2]\n",
      "  [  14    0]]]\n",
      "\n",
      "\n",
      "**Processing antiarrhythmic comments...**\n",
      "Test accuracy is 1.0\n",
      "[[[   0    0]\n",
      "  [   0 4763]]]\n",
      "\n",
      "\n",
      "**Processing antibiotic comments...**\n",
      "Test accuracy is 0.9976905311778291\n",
      "[[[   1    9]\n",
      "  [   2 4751]]\n",
      "\n",
      " [[4751    2]\n",
      "  [   9    1]]]\n",
      "\n",
      "\n",
      "**Processing anticonvulsant comments...**\n",
      "Test accuracy is 0.9995800965777871\n",
      "[[[   0    2]\n",
      "  [   0 4761]]\n",
      "\n",
      " [[4761    0]\n",
      "  [   2    0]]]\n",
      "\n",
      "\n",
      "**Processing antifungal comments...**\n",
      "Test accuracy is 1.0\n",
      "[[[   0    0]\n",
      "  [   0 4763]]]\n",
      "\n",
      "\n",
      "**Processing antihistamine comments...**\n",
      "Test accuracy is 0.9995800965777871\n",
      "[[[   0    2]\n",
      "  [   0 4761]]\n",
      "\n",
      " [[4761    0]\n",
      "  [   2    0]]]\n",
      "\n",
      "\n",
      "**Processing antimalarial comments...**\n",
      "Test accuracy is 0.9995800965777871\n",
      "[[[   0    2]\n",
      "  [   0 4761]]\n",
      "\n",
      " [[4761    0]\n",
      "  [   2    0]]]\n",
      "\n",
      "\n",
      "**Processing antioxidant comments...**\n",
      "Test accuracy is 0.996220869200084\n",
      "[[[   0   16]\n",
      "  [   2 4745]]\n",
      "\n",
      " [[4745    2]\n",
      "  [  16    0]]]\n",
      "\n",
      "\n",
      "**Processing antiprotozoal comments...**\n",
      "Test accuracy is 0.9995800965777871\n",
      "[[[   0    2]\n",
      "  [   0 4761]]\n",
      "\n",
      " [[4761    0]\n",
      "  [   2    0]]]\n",
      "\n",
      "\n",
      "**Processing antiviral comments...**\n",
      "Test accuracy is 0.9989502414444678\n",
      "[[[   0    5]\n",
      "  [   0 4758]]\n",
      "\n",
      " [[4758    0]\n",
      "  [   5    0]]]\n",
      "\n",
      "\n",
      "**Processing apoptosis_stimulant comments...**\n",
      "Test accuracy is 0.9970606760445098\n",
      "[[[   0   11]\n",
      "  [   3 4749]]\n",
      "\n",
      " [[4749    3]\n",
      "  [  11    0]]]\n",
      "\n",
      "\n",
      "**Processing aromatase_inhibitor comments...**\n",
      "Test accuracy is 0.9972706277556163\n",
      "[[[   0   13]\n",
      "  [   0 4750]]\n",
      "\n",
      " [[4750    0]\n",
      "  [  13    0]]]\n",
      "\n",
      "\n",
      "**Processing atm_kinase_inhibitor comments...**\n",
      "Test accuracy is 0.9995800965777871\n",
      "[[[   0    2]\n",
      "  [   0 4761]]\n",
      "\n",
      " [[4761    0]\n",
      "  [   2    0]]]\n",
      "\n",
      "\n",
      "**Processing atp-sensitive_potassium_channel_antagonist comments...**\n",
      "Test accuracy is 1.0\n",
      "[[[   0    0]\n",
      "  [   0 4763]]]\n",
      "\n",
      "\n",
      "**Processing atp_synthase_inhibitor comments...**\n",
      "Test accuracy is 0.9995800965777871\n",
      "[[[   0    2]\n",
      "  [   0 4761]]\n",
      "\n",
      " [[4761    0]\n",
      "  [   2    0]]]\n",
      "\n",
      "\n",
      "**Processing atpase_inhibitor comments...**\n",
      "Test accuracy is 0.9958009657778711\n",
      "[[[   3   15]\n",
      "  [   5 4740]]\n",
      "\n",
      " [[4740    5]\n",
      "  [  15    3]]]\n",
      "\n",
      "\n",
      "**Processing atr_kinase_inhibitor comments...**\n",
      "Test accuracy is 0.9993701448666806\n",
      "[[[   1    3]\n",
      "  [   0 4759]]\n",
      "\n",
      " [[4759    0]\n",
      "  [   3    1]]]\n",
      "\n",
      "\n",
      "**Processing aurora_kinase_inhibitor comments...**\n",
      "Test accuracy is 0.9966407726222969\n",
      "[[[   7   16]\n",
      "  [   0 4740]]\n",
      "\n",
      " [[4740    0]\n",
      "  [  16    7]]]\n",
      "\n",
      "\n",
      "**Processing autotaxin_inhibitor comments...**\n",
      "Test accuracy is 0.9997900482888935\n",
      "[[[   0    1]\n",
      "  [   0 4762]]\n",
      "\n",
      " [[4762    0]\n",
      "  [   1    0]]]\n",
      "\n",
      "\n",
      "**Processing bacterial_30s_ribosomal_subunit_inhibitor comments...**\n",
      "Test accuracy is 0.9972706277556163\n",
      "[[[   0   13]\n",
      "  [   0 4750]]\n",
      "\n",
      " [[4750    0]\n",
      "  [  13    0]]]\n",
      "\n",
      "\n",
      "**Processing bacterial_50s_ribosomal_subunit_inhibitor comments...**\n",
      "Test accuracy is 0.9964308209111904\n",
      "[[[   0   16]\n",
      "  [   1 4746]]\n",
      "\n",
      " [[4746    1]\n",
      "  [  16    0]]]\n",
      "\n",
      "\n",
      "**Processing bacterial_antifolate comments...**\n",
      "Test accuracy is 0.9991601931555742\n",
      "[[[   0    4]\n",
      "  [   0 4759]]\n",
      "\n",
      " [[4759    0]\n",
      "  [   4    0]]]\n",
      "\n",
      "\n",
      "**Processing bacterial_cell_wall_synthesis_inhibitor comments...**\n",
      "Test accuracy is 0.9934914969557002\n",
      "[[[   0   30]\n",
      "  [   1 4732]]\n",
      "\n",
      " [[4732    1]\n",
      "  [  30    0]]]\n",
      "\n",
      "\n",
      "**Processing bacterial_dna_gyrase_inhibitor comments...**\n",
      "Test accuracy is 0.996220869200084\n",
      "[[[   0   18]\n",
      "  [   0 4745]]\n",
      "\n",
      " [[4745    0]\n",
      "  [  18    0]]]\n",
      "\n",
      "\n",
      "**Processing bacterial_dna_inhibitor comments...**\n",
      "Test accuracy is 0.994331303800126\n",
      "[[[   0   27]\n",
      "  [   0 4736]]\n",
      "\n",
      " [[4736    0]\n",
      "  [  27    0]]]\n",
      "\n",
      "\n",
      "**Processing bacterial_membrane_integrity_inhibitor comments...**\n",
      "Test accuracy is 1.0\n",
      "[[[   0    0]\n",
      "  [   0 4763]]]\n",
      "\n",
      "\n",
      "**Processing bcl_inhibitor comments...**\n",
      "Test accuracy is 0.998110434600042\n",
      "[[[   0    9]\n",
      "  [   0 4754]]\n",
      "\n",
      " [[4754    0]\n",
      "  [   9    0]]]\n",
      "\n",
      "\n",
      "**Processing bcr-abl_inhibitor comments...**\n",
      "Test accuracy is 0.9993701448666806\n",
      "[[[   2    3]\n",
      "  [   0 4758]]\n",
      "\n",
      " [[4758    0]\n",
      "  [   3    2]]]\n",
      "\n",
      "\n",
      "**Processing benzodiazepine_receptor_agonist comments...**\n",
      "Test accuracy is 0.9972706277556163\n",
      "[[[   0   13]\n",
      "  [   0 4750]]\n",
      "\n",
      " [[4750    0]\n",
      "  [  13    0]]]\n",
      "\n",
      "\n",
      "**Processing beta_amyloid_inhibitor comments...**\n",
      "Test accuracy is 0.9989502414444678\n",
      "[[[   0    4]\n",
      "  [   1 4758]]\n",
      "\n",
      " [[4758    1]\n",
      "  [   4    0]]]\n",
      "\n",
      "\n",
      "**Processing bromodomain_inhibitor comments...**\n",
      "Test accuracy is 0.9976905311778291\n",
      "[[[   4    8]\n",
      "  [   3 4748]]\n",
      "\n",
      " [[4748    3]\n",
      "  [   8    4]]]\n",
      "\n",
      "\n",
      "**Processing btk_inhibitor comments...**\n",
      "Test accuracy is 0.9989502414444678\n",
      "[[[   0    5]\n",
      "  [   0 4758]]\n",
      "\n",
      " [[4758    0]\n",
      "  [   5    0]]]\n",
      "\n",
      "\n",
      "**Processing calcineurin_inhibitor comments...**\n",
      "Test accuracy is 0.9997900482888935\n",
      "[[[   0    1]\n",
      "  [   0 4762]]\n",
      "\n",
      " [[4762    0]\n",
      "  [   1    0]]]\n",
      "\n",
      "\n",
      "**Processing calcium_channel_blocker comments...**\n",
      "Test accuracy is 0.9903422212891035\n",
      "[[[   0   45]\n",
      "  [   1 4717]]\n",
      "\n",
      " [[4717    1]\n",
      "  [  45    0]]]\n",
      "\n",
      "\n",
      "**Processing cannabinoid_receptor_agonist comments...**\n",
      "Test accuracy is 0.9987402897333614\n",
      "[[[   0    6]\n",
      "  [   0 4757]]\n",
      "\n",
      " [[4757    0]\n",
      "  [   6    0]]]\n",
      "\n",
      "\n",
      "**Processing cannabinoid_receptor_antagonist comments...**\n",
      "Test accuracy is 0.9993701448666806\n",
      "[[[   0    3]\n",
      "  [   0 4760]]\n",
      "\n",
      " [[4760    0]\n",
      "  [   3    0]]]\n",
      "\n",
      "\n",
      "**Processing carbonic_anhydrase_inhibitor comments...**\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy is 0.9972706277556163\n",
      "[[[   0   13]\n",
      "  [   0 4750]]\n",
      "\n",
      " [[4750    0]\n",
      "  [  13    0]]]\n",
      "\n",
      "\n",
      "**Processing casein_kinase_inhibitor comments...**\n",
      "Test accuracy is 0.9987402897333614\n",
      "[[[   0    6]\n",
      "  [   0 4757]]\n",
      "\n",
      " [[4757    0]\n",
      "  [   6    0]]]\n",
      "\n",
      "\n",
      "**Processing caspase_activator comments...**\n",
      "Test accuracy is 0.9987402897333614\n",
      "[[[   0    6]\n",
      "  [   0 4757]]\n",
      "\n",
      " [[4757    0]\n",
      "  [   6    0]]]\n",
      "\n",
      "\n",
      "**Processing catechol_o_methyltransferase_inhibitor comments...**\n",
      "Test accuracy is 0.9995800965777871\n",
      "[[[   0    2]\n",
      "  [   0 4761]]\n",
      "\n",
      " [[4761    0]\n",
      "  [   2    0]]]\n",
      "\n",
      "\n",
      "**Processing cc_chemokine_receptor_antagonist comments...**\n",
      "Test accuracy is 0.9951711106445518\n",
      "[[[   0   22]\n",
      "  [   1 4740]]\n",
      "\n",
      " [[4740    1]\n",
      "  [  22    0]]]\n",
      "\n",
      "\n",
      "**Processing cck_receptor_antagonist comments...**\n",
      "Test accuracy is 0.9991601931555742\n",
      "[[[   0    4]\n",
      "  [   0 4759]]\n",
      "\n",
      " [[4759    0]\n",
      "  [   4    0]]]\n",
      "\n",
      "\n",
      "**Processing cdk_inhibitor comments...**\n",
      "Test accuracy is 0.9955910140667646\n",
      "[[[  48   14]\n",
      "  [   7 4694]]\n",
      "\n",
      " [[4694    7]\n",
      "  [  14   48]]]\n",
      "\n",
      "\n",
      "**Processing chelating_agent comments...**\n",
      "Test accuracy is 0.9966407726222969\n",
      "[[[   0   16]\n",
      "  [   0 4747]]\n",
      "\n",
      " [[4747    0]\n",
      "  [  16    0]]]\n",
      "\n",
      "\n",
      "**Processing chk_inhibitor comments...**\n",
      "Test accuracy is 0.9993701448666806\n",
      "[[[   0    3]\n",
      "  [   0 4760]]\n",
      "\n",
      " [[4760    0]\n",
      "  [   3    0]]]\n",
      "\n",
      "\n",
      "**Processing chloride_channel_blocker comments...**\n",
      "Test accuracy is 0.9985303380222549\n",
      "[[[   0    7]\n",
      "  [   0 4756]]\n",
      "\n",
      " [[4756    0]\n",
      "  [   7    0]]]\n",
      "\n",
      "\n",
      "**Processing cholesterol_inhibitor comments...**\n",
      "Test accuracy is 0.998110434600042\n",
      "[[[   0    8]\n",
      "  [   1 4754]]\n",
      "\n",
      " [[4754    1]\n",
      "  [   8    0]]]\n",
      "\n",
      "\n",
      "**Processing cholinergic_receptor_antagonist comments...**\n",
      "Test accuracy is 0.9968507243334033\n",
      "[[[   0   14]\n",
      "  [   1 4748]]\n",
      "\n",
      " [[4748    1]\n",
      "  [  14    0]]]\n",
      "\n",
      "\n",
      "**Processing coagulation_factor_inhibitor comments...**\n",
      "Test accuracy is 0.9991601931555742\n",
      "[[[   0    4]\n",
      "  [   0 4759]]\n",
      "\n",
      " [[4759    0]\n",
      "  [   4    0]]]\n",
      "\n",
      "\n",
      "**Processing corticosteroid_agonist comments...**\n",
      "Test accuracy is 0.9979004828889355\n",
      "[[[   0   10]\n",
      "  [   0 4753]]\n",
      "\n",
      " [[4753    0]\n",
      "  [  10    0]]]\n",
      "\n",
      "\n",
      "**Processing cyclooxygenase_inhibitor comments...**\n",
      "Test accuracy is 0.980684442578207\n",
      "[[[   0   92]\n",
      "  [   0 4671]]\n",
      "\n",
      " [[4671    0]\n",
      "  [  92    0]]]\n",
      "\n",
      "\n",
      "**Processing cytochrome_p450_inhibitor comments...**\n",
      "Test accuracy is 0.9953810623556582\n",
      "[[[   0   20]\n",
      "  [   2 4741]]\n",
      "\n",
      " [[4741    2]\n",
      "  [  20    0]]]\n",
      "\n",
      "\n",
      "**Processing dihydrofolate_reductase_inhibitor comments...**\n",
      "Test accuracy is 0.9987402897333614\n",
      "[[[   1    4]\n",
      "  [   2 4756]]\n",
      "\n",
      " [[4756    2]\n",
      "  [   4    1]]]\n",
      "\n",
      "\n",
      "**Processing dipeptidyl_peptidase_inhibitor comments...**\n",
      "Test accuracy is 0.9989502414444678\n",
      "[[[   0    5]\n",
      "  [   0 4758]]\n",
      "\n",
      " [[4758    0]\n",
      "  [   5    0]]]\n",
      "\n",
      "\n",
      "**Processing diuretic comments...**\n",
      "Test accuracy is 1.0\n",
      "[[[   0    0]\n",
      "  [   0 4763]]]\n",
      "\n",
      "\n",
      "**Processing dna_alkylating_agent comments...**\n",
      "Test accuracy is 0.9976905311778291\n",
      "[[[   0    9]\n",
      "  [   2 4752]]\n",
      "\n",
      " [[4752    2]\n",
      "  [   9    0]]]\n",
      "\n",
      "\n",
      "**Processing dna_inhibitor comments...**\n",
      "Test accuracy is 0.9825740079781651\n",
      "[[[   3   77]\n",
      "  [   6 4677]]\n",
      "\n",
      " [[4677    6]\n",
      "  [  77    3]]]\n",
      "\n",
      "\n",
      "**Processing dopamine_receptor_agonist comments...**\n",
      "Test accuracy is 0.9937014486668067\n",
      "[[[   0   28]\n",
      "  [   2 4733]]\n",
      "\n",
      " [[4733    2]\n",
      "  [  28    0]]]\n",
      "\n",
      "\n",
      "**Processing dopamine_receptor_antagonist comments...**\n",
      "Test accuracy is 0.980684442578207\n",
      "[[[   2   85]\n",
      "  [   7 4669]]\n",
      "\n",
      " [[4669    7]\n",
      "  [  85    2]]]\n",
      "\n",
      "\n",
      "**Processing egfr_inhibitor comments...**\n",
      "Test accuracy is 0.9934914969557002\n",
      "[[[  60   24]\n",
      "  [   7 4672]]\n",
      "\n",
      " [[4672    7]\n",
      "  [  24   60]]]\n",
      "\n",
      "\n",
      "**Processing elastase_inhibitor comments...**\n",
      "Test accuracy is 0.9997900482888935\n",
      "[[[   0    1]\n",
      "  [   0 4762]]\n",
      "\n",
      " [[4762    0]\n",
      "  [   1    0]]]\n",
      "\n",
      "\n",
      "**Processing erbb2_inhibitor comments...**\n",
      "Test accuracy is 0.9997900482888935\n",
      "[[[   0    1]\n",
      "  [   0 4762]]\n",
      "\n",
      " [[4762    0]\n",
      "  [   1    0]]]\n",
      "\n",
      "\n",
      "**Processing estrogen_receptor_agonist comments...**\n",
      "Test accuracy is 0.9926516901112744\n",
      "[[[   0   34]\n",
      "  [   1 4728]]\n",
      "\n",
      " [[4728    1]\n",
      "  [  34    0]]]\n",
      "\n",
      "\n",
      "**Processing estrogen_receptor_antagonist comments...**\n",
      "Test accuracy is 0.9972706277556163\n",
      "[[[   0   11]\n",
      "  [   2 4750]]\n",
      "\n",
      " [[4750    2]\n",
      "  [  11    0]]]\n",
      "\n",
      "\n",
      "**Processing faah_inhibitor comments...**\n",
      "Test accuracy is 0.9985303380222549\n",
      "[[[   0    7]\n",
      "  [   0 4756]]\n",
      "\n",
      " [[4756    0]\n",
      "  [   7    0]]]\n",
      "\n",
      "\n",
      "**Processing farnesyltransferase_inhibitor comments...**\n",
      "Test accuracy is 0.9993701448666806\n",
      "[[[   0    3]\n",
      "  [   0 4760]]\n",
      "\n",
      " [[4760    0]\n",
      "  [   3    0]]]\n",
      "\n",
      "\n",
      "**Processing fatty_acid_receptor_agonist comments...**\n",
      "Test accuracy is 0.9985303380222549\n",
      "[[[   0    7]\n",
      "  [   0 4756]]\n",
      "\n",
      " [[4756    0]\n",
      "  [   7    0]]]\n",
      "\n",
      "\n",
      "**Processing fgfr_inhibitor comments...**\n",
      "Test accuracy is 0.9989502414444678\n",
      "[[[   0    5]\n",
      "  [   0 4758]]\n",
      "\n",
      " [[4758    0]\n",
      "  [   5    0]]]\n",
      "\n",
      "\n",
      "**Processing flt3_inhibitor comments...**\n",
      "Test accuracy is 0.9905521730002099\n",
      "[[[  25   29]\n",
      "  [  16 4693]]\n",
      "\n",
      " [[4693   16]\n",
      "  [  29   25]]]\n",
      "\n",
      "\n",
      "**Processing focal_adhesion_kinase_inhibitor comments...**\n",
      "Test accuracy is 0.9993701448666806\n",
      "[[[   1    3]\n",
      "  [   0 4759]]\n",
      "\n",
      " [[4759    0]\n",
      "  [   3    1]]]\n",
      "\n",
      "\n",
      "**Processing free_radical_scavenger comments...**\n",
      "Test accuracy is 0.9991601931555742\n",
      "[[[   0    4]\n",
      "  [   0 4759]]\n",
      "\n",
      " [[4759    0]\n",
      "  [   4    0]]]\n",
      "\n",
      "\n",
      "**Processing fungal_squalene_epoxidase_inhibitor comments...**\n",
      "Test accuracy is 0.9993701448666806\n",
      "[[[   0    2]\n",
      "  [   1 4760]]\n",
      "\n",
      " [[4760    1]\n",
      "  [   2    0]]]\n",
      "\n",
      "\n",
      "**Processing gaba_receptor_agonist comments...**\n",
      "Test accuracy is 0.9958009657778711\n",
      "[[[   0   18]\n",
      "  [   2 4743]]\n",
      "\n",
      " [[4743    2]\n",
      "  [  18    0]]]\n",
      "\n",
      "\n",
      "**Processing gaba_receptor_antagonist comments...**\n",
      "Test accuracy is 0.9937014486668067\n",
      "[[[   0   30]\n",
      "  [   0 4733]]\n",
      "\n",
      " [[4733    0]\n",
      "  [  30    0]]]\n",
      "\n",
      "\n",
      "**Processing gamma_secretase_inhibitor comments...**\n",
      "Test accuracy is 0.9985303380222549\n",
      "[[[   0    5]\n",
      "  [   2 4756]]\n",
      "\n",
      " [[4756    2]\n",
      "  [   5    0]]]\n",
      "\n",
      "\n",
      "**Processing glucocorticoid_receptor_agonist comments...**\n",
      "Test accuracy is 0.9949611589334453\n",
      "[[[  38    9]\n",
      "  [  15 4701]]\n",
      "\n",
      " [[4701   15]\n",
      "  [   9   38]]]\n",
      "\n",
      "\n",
      "**Processing glutamate_inhibitor comments...**\n",
      "Test accuracy is 0.9995800965777871\n",
      "[[[   0    2]\n",
      "  [   0 4761]]\n",
      "\n",
      " [[4761    0]\n",
      "  [   2    0]]]\n",
      "\n",
      "\n",
      "**Processing glutamate_receptor_agonist comments...**\n",
      "Test accuracy is 0.9966407726222969\n",
      "[[[   0   16]\n",
      "  [   0 4747]]\n",
      "\n",
      " [[4747    0]\n",
      "  [  16    0]]]\n",
      "\n",
      "\n",
      "**Processing glutamate_receptor_antagonist comments...**\n",
      "Test accuracy is 0.9859332353558682\n",
      "[[[   0   65]\n",
      "  [   2 4696]]\n",
      "\n",
      " [[4696    2]\n",
      "  [  65    0]]]\n",
      "\n",
      "\n",
      "**Processing gonadotropin_receptor_agonist comments...**\n",
      "Test accuracy is 0.9995800965777871\n",
      "[[[   0    2]\n",
      "  [   0 4761]]\n",
      "\n",
      " [[4761    0]\n",
      "  [   2    0]]]\n",
      "\n",
      "\n",
      "**Processing gsk_inhibitor comments...**\n",
      "Test accuracy is 0.9974805794667226\n",
      "[[[   5   10]\n",
      "  [   2 4746]]\n",
      "\n",
      " [[4746    2]\n",
      "  [  10    5]]]\n",
      "\n",
      "\n",
      "**Processing hcv_inhibitor comments...**\n",
      "Test accuracy is 0.9968507243334033\n",
      "[[[   0   15]\n",
      "  [   0 4748]]\n",
      "\n",
      " [[4748    0]\n",
      "  [  15    0]]]\n",
      "\n",
      "\n",
      "**Processing hdac_inhibitor comments...**\n",
      "Test accuracy is 0.9979004828889355\n",
      "[[[  12    8]\n",
      "  [   2 4741]]\n",
      "\n",
      " [[4741    2]\n",
      "  [   8   12]]]\n",
      "\n",
      "\n",
      "**Processing histamine_receptor_agonist comments...**\n",
      "Test accuracy is 0.9968507243334033\n",
      "[[[   0   15]\n",
      "  [   0 4748]]\n",
      "\n",
      " [[4748    0]\n",
      "  [  15    0]]]\n",
      "\n",
      "\n",
      "**Processing histamine_receptor_antagonist comments...**\n",
      "Test accuracy is 0.9892924627335713\n",
      "[[[   0   51]\n",
      "  [   0 4712]]\n",
      "\n",
      " [[4712    0]\n",
      "  [  51    0]]]\n",
      "\n",
      "\n",
      "**Processing histone_lysine_demethylase_inhibitor comments...**\n",
      "Test accuracy is 0.9995800965777871\n",
      "[[[   0    2]\n",
      "  [   0 4761]]\n",
      "\n",
      " [[4761    0]\n",
      "  [   2    0]]]\n",
      "\n",
      "\n",
      "**Processing histone_lysine_methyltransferase_inhibitor comments...**\n",
      "Test accuracy is 0.9983203863111484\n",
      "[[[   0    8]\n",
      "  [   0 4755]]\n",
      "\n",
      " [[4755    0]\n",
      "  [   8    0]]]\n",
      "\n",
      "\n",
      "**Processing hiv_inhibitor comments...**\n",
      "Test accuracy is 0.9953810623556582\n",
      "[[[   0   21]\n",
      "  [   1 4741]]\n",
      "\n",
      " [[4741    1]\n",
      "  [  21    0]]]\n",
      "\n",
      "\n",
      "**Processing hmgcr_inhibitor comments...**\n",
      "Test accuracy is 0.9949611589334453\n",
      "[[[  40   17]\n",
      "  [   7 4699]]\n",
      "\n",
      " [[4699    7]\n",
      "  [  17   40]]]\n",
      "\n",
      "\n",
      "**Processing hsp_inhibitor comments...**\n",
      "Test accuracy is 0.9985303380222549\n",
      "[[[  19    5]\n",
      "  [   2 4737]]\n",
      "\n",
      " [[4737    2]\n",
      "  [   5   19]]]\n",
      "\n",
      "\n",
      "**Processing igf-1_inhibitor comments...**\n",
      "Test accuracy is 0.9985303380222549\n",
      "[[[   0    7]\n",
      "  [   0 4756]]\n",
      "\n",
      " [[4756    0]\n",
      "  [   7    0]]]\n",
      "\n",
      "\n",
      "**Processing ikk_inhibitor comments...**\n",
      "Test accuracy is 0.9985303380222549\n",
      "[[[   0    7]\n",
      "  [   0 4756]]\n",
      "\n",
      " [[4756    0]\n",
      "  [   7    0]]]\n",
      "\n",
      "\n",
      "**Processing imidazoline_receptor_agonist comments...**\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy is 0.9987402897333614\n",
      "[[[   0    6]\n",
      "  [   0 4757]]\n",
      "\n",
      " [[4757    0]\n",
      "  [   6    0]]]\n",
      "\n",
      "\n",
      "**Processing immunosuppressant comments...**\n",
      "Test accuracy is 0.9974805794667226\n",
      "[[[   1   11]\n",
      "  [   1 4750]]\n",
      "\n",
      " [[4750    1]\n",
      "  [  11    1]]]\n",
      "\n",
      "\n",
      "**Processing insulin_secretagogue comments...**\n",
      "Test accuracy is 0.9985303380222549\n",
      "[[[   0    6]\n",
      "  [   1 4756]]\n",
      "\n",
      " [[4756    1]\n",
      "  [   6    0]]]\n",
      "\n",
      "\n",
      "**Processing insulin_sensitizer comments...**\n",
      "Test accuracy is 0.998110434600042\n",
      "[[[   0    6]\n",
      "  [   3 4754]]\n",
      "\n",
      " [[4754    3]\n",
      "  [   6    0]]]\n",
      "\n",
      "\n",
      "**Processing integrin_inhibitor comments...**\n",
      "Test accuracy is 0.9979004828889355\n",
      "[[[   0   10]\n",
      "  [   0 4753]]\n",
      "\n",
      " [[4753    0]\n",
      "  [  10    0]]]\n",
      "\n",
      "\n",
      "**Processing jak_inhibitor comments...**\n",
      "Test accuracy is 0.9953810623556582\n",
      "[[[   9   19]\n",
      "  [   3 4732]]\n",
      "\n",
      " [[4732    3]\n",
      "  [  19    9]]]\n",
      "\n",
      "\n",
      "**Processing kit_inhibitor comments...**\n",
      "Test accuracy is 0.994331303800126\n",
      "[[[  33   18]\n",
      "  [   9 4703]]\n",
      "\n",
      " [[4703    9]\n",
      "  [  18   33]]]\n",
      "\n",
      "\n",
      "**Processing laxative comments...**\n",
      "Test accuracy is 0.9997900482888935\n",
      "[[[   0    1]\n",
      "  [   0 4762]]\n",
      "\n",
      " [[4762    0]\n",
      "  [   1    0]]]\n",
      "\n",
      "\n",
      "**Processing leukotriene_inhibitor comments...**\n",
      "Test accuracy is 0.9997900482888935\n",
      "[[[   0    1]\n",
      "  [   0 4762]]\n",
      "\n",
      " [[4762    0]\n",
      "  [   1    0]]]\n",
      "\n",
      "\n",
      "**Processing leukotriene_receptor_antagonist comments...**\n",
      "Test accuracy is 0.9976905311778291\n",
      "[[[   0   11]\n",
      "  [   0 4752]]\n",
      "\n",
      " [[4752    0]\n",
      "  [  11    0]]]\n",
      "\n",
      "\n",
      "**Processing lipase_inhibitor comments...**\n",
      "Test accuracy is 0.9997900482888935\n",
      "[[[   0    1]\n",
      "  [   0 4762]]\n",
      "\n",
      " [[4762    0]\n",
      "  [   1    0]]]\n",
      "\n",
      "\n",
      "**Processing lipoxygenase_inhibitor comments...**\n",
      "Test accuracy is 0.9989502414444678\n",
      "[[[   1    5]\n",
      "  [   0 4757]]\n",
      "\n",
      " [[4757    0]\n",
      "  [   5    1]]]\n",
      "\n",
      "\n",
      "**Processing lxr_agonist comments...**\n",
      "Test accuracy is 0.9993701448666806\n",
      "[[[   0    3]\n",
      "  [   0 4760]]\n",
      "\n",
      " [[4760    0]\n",
      "  [   3    0]]]\n",
      "\n",
      "\n",
      "**Processing mdm_inhibitor comments...**\n",
      "Test accuracy is 0.9989502414444678\n",
      "[[[   0    4]\n",
      "  [   1 4758]]\n",
      "\n",
      " [[4758    1]\n",
      "  [   4    0]]]\n",
      "\n",
      "\n",
      "**Processing mek_inhibitor comments...**\n",
      "Test accuracy is 0.9983203863111484\n",
      "[[[   8    8]\n",
      "  [   0 4747]]\n",
      "\n",
      " [[4747    0]\n",
      "  [   8    8]]]\n",
      "\n",
      "\n",
      "**Processing membrane_integrity_inhibitor comments...**\n",
      "Test accuracy is 0.996220869200084\n",
      "[[[   0   18]\n",
      "  [   0 4745]]\n",
      "\n",
      " [[4745    0]\n",
      "  [  18    0]]]\n",
      "\n",
      "\n",
      "**Processing mineralocorticoid_receptor_antagonist comments...**\n",
      "Test accuracy is 0.9991601931555742\n",
      "[[[   0    4]\n",
      "  [   0 4759]]\n",
      "\n",
      " [[4759    0]\n",
      "  [   4    0]]]\n",
      "\n",
      "\n",
      "**Processing monoacylglycerol_lipase_inhibitor comments...**\n",
      "Test accuracy is 0.9997900482888935\n",
      "[[[   0    1]\n",
      "  [   0 4762]]\n",
      "\n",
      " [[4762    0]\n",
      "  [   1    0]]]\n",
      "\n",
      "\n",
      "**Processing monoamine_oxidase_inhibitor comments...**\n",
      "Test accuracy is 0.9947512072223389\n",
      "[[[   0   23]\n",
      "  [   2 4738]]\n",
      "\n",
      " [[4738    2]\n",
      "  [  23    0]]]\n",
      "\n",
      "\n",
      "**Processing monopolar_spindle_1_kinase_inhibitor comments...**\n",
      "Test accuracy is 0.9987402897333614\n",
      "[[[   0    6]\n",
      "  [   0 4757]]\n",
      "\n",
      " [[4757    0]\n",
      "  [   6    0]]]\n",
      "\n",
      "\n",
      "**Processing mtor_inhibitor comments...**\n",
      "Test accuracy is 0.9958009657778711\n",
      "[[[   6   16]\n",
      "  [   4 4737]]\n",
      "\n",
      " [[4737    4]\n",
      "  [  16    6]]]\n",
      "\n",
      "\n",
      "**Processing mucolytic_agent comments...**\n",
      "Test accuracy is 0.9985303380222549\n",
      "[[[   0    6]\n",
      "  [   1 4756]]\n",
      "\n",
      " [[4756    1]\n",
      "  [   6    0]]]\n",
      "\n",
      "\n",
      "**Processing neuropeptide_receptor_antagonist comments...**\n",
      "Test accuracy is 0.9987402897333614\n",
      "[[[   0    5]\n",
      "  [   1 4757]]\n",
      "\n",
      " [[4757    1]\n",
      "  [   5    0]]]\n",
      "\n",
      "\n",
      "**Processing nfkb_inhibitor comments...**\n",
      "Test accuracy is 0.9941213520890195\n",
      "[[[ 134   21]\n",
      "  [   7 4601]]\n",
      "\n",
      " [[4601    7]\n",
      "  [  21  134]]]\n",
      "\n",
      "\n",
      "**Processing nicotinic_receptor_agonist comments...**\n",
      "Test accuracy is 0.9997900482888935\n",
      "[[[   0    1]\n",
      "  [   0 4762]]\n",
      "\n",
      " [[4762    0]\n",
      "  [   1    0]]]\n",
      "\n",
      "\n",
      "**Processing nitric_oxide_donor comments...**\n",
      "Test accuracy is 0.9983203863111484\n",
      "[[[   0    8]\n",
      "  [   0 4755]]\n",
      "\n",
      " [[4755    0]\n",
      "  [   8    0]]]\n",
      "\n",
      "\n",
      "**Processing nitric_oxide_production_inhibitor comments...**\n",
      "Test accuracy is 0.9995800965777871\n",
      "[[[   0    2]\n",
      "  [   0 4761]]\n",
      "\n",
      " [[4761    0]\n",
      "  [   2    0]]]\n",
      "\n",
      "\n",
      "**Processing nitric_oxide_synthase_inhibitor comments...**\n",
      "Test accuracy is 0.9991601931555742\n",
      "[[[   0    4]\n",
      "  [   0 4759]]\n",
      "\n",
      " [[4759    0]\n",
      "  [   4    0]]]\n",
      "\n",
      "\n",
      "**Processing norepinephrine_reuptake_inhibitor comments...**\n",
      "Test accuracy is 0.9997900482888935\n",
      "[[[   0    1]\n",
      "  [   0 4762]]\n",
      "\n",
      " [[4762    0]\n",
      "  [   1    0]]]\n",
      "\n",
      "\n",
      "**Processing nrf2_activator comments...**\n",
      "Test accuracy is 0.9997900482888935\n",
      "[[[   0    1]\n",
      "  [   0 4762]]\n",
      "\n",
      " [[4762    0]\n",
      "  [   1    0]]]\n",
      "\n",
      "\n",
      "**Processing opioid_receptor_agonist comments...**\n",
      "Test accuracy is 0.9979004828889355\n",
      "[[[   0   10]\n",
      "  [   0 4753]]\n",
      "\n",
      " [[4753    0]\n",
      "  [  10    0]]]\n",
      "\n",
      "\n",
      "**Processing opioid_receptor_antagonist comments...**\n",
      "Test accuracy is 0.9968507243334033\n",
      "[[[   0   15]\n",
      "  [   0 4748]]\n",
      "\n",
      " [[4748    0]\n",
      "  [  15    0]]]\n",
      "\n",
      "\n",
      "**Processing orexin_receptor_antagonist comments...**\n",
      "Test accuracy is 0.9983203863111484\n",
      "[[[   0    8]\n",
      "  [   0 4755]]\n",
      "\n",
      " [[4755    0]\n",
      "  [   8    0]]]\n",
      "\n",
      "\n",
      "**Processing p38_mapk_inhibitor comments...**\n",
      "Test accuracy is 0.9970606760445098\n",
      "[[[   4   13]\n",
      "  [   1 4745]]\n",
      "\n",
      " [[4745    1]\n",
      "  [  13    4]]]\n",
      "\n",
      "\n",
      "**Processing p-glycoprotein_inhibitor comments...**\n",
      "Test accuracy is 0.9989502414444678\n",
      "[[[   0    4]\n",
      "  [   1 4758]]\n",
      "\n",
      " [[4758    1]\n",
      "  [   4    0]]]\n",
      "\n",
      "\n",
      "**Processing parp_inhibitor comments...**\n",
      "Test accuracy is 0.9968507243334033\n",
      "[[[   0   14]\n",
      "  [   1 4748]]\n",
      "\n",
      " [[4748    1]\n",
      "  [  14    0]]]\n",
      "\n",
      "\n",
      "**Processing pdgfr_inhibitor comments...**\n",
      "Test accuracy is 0.9932815452445938\n",
      "[[[  34   22]\n",
      "  [  10 4697]]\n",
      "\n",
      " [[4697   10]\n",
      "  [  22   34]]]\n",
      "\n",
      "\n",
      "**Processing pdk_inhibitor comments...**\n",
      "Test accuracy is 0.9997900482888935\n",
      "[[[   0    1]\n",
      "  [   0 4762]]\n",
      "\n",
      " [[4762    0]\n",
      "  [   1    0]]]\n",
      "\n",
      "\n",
      "**Processing phosphodiesterase_inhibitor comments...**\n",
      "Test accuracy is 0.9880327524669326\n",
      "[[[   0   56]\n",
      "  [   1 4706]]\n",
      "\n",
      " [[4706    1]\n",
      "  [  56    0]]]\n",
      "\n",
      "\n",
      "**Processing phospholipase_inhibitor comments...**\n",
      "Test accuracy is 0.9989502414444678\n",
      "[[[   0    4]\n",
      "  [   1 4758]]\n",
      "\n",
      " [[4758    1]\n",
      "  [   4    0]]]\n",
      "\n",
      "\n",
      "**Processing pi3k_inhibitor comments...**\n",
      "Test accuracy is 0.9928616418223809\n",
      "[[[  11   28]\n",
      "  [   6 4718]]\n",
      "\n",
      " [[4718    6]\n",
      "  [  28   11]]]\n",
      "\n",
      "\n",
      "**Processing pkc_inhibitor comments...**\n",
      "Test accuracy is 0.9989502414444678\n",
      "[[[   0    5]\n",
      "  [   0 4758]]\n",
      "\n",
      " [[4758    0]\n",
      "  [   5    0]]]\n",
      "\n",
      "\n",
      "**Processing potassium_channel_activator comments...**\n",
      "Test accuracy is 0.9972706277556163\n",
      "[[[   0   13]\n",
      "  [   0 4750]]\n",
      "\n",
      " [[4750    0]\n",
      "  [  13    0]]]\n",
      "\n",
      "\n",
      "**Processing potassium_channel_antagonist comments...**\n",
      "Test accuracy is 0.9964308209111904\n",
      "[[[   0   17]\n",
      "  [   0 4746]]\n",
      "\n",
      " [[4746    0]\n",
      "  [  17    0]]]\n",
      "\n",
      "\n",
      "**Processing ppar_receptor_agonist comments...**\n",
      "Test accuracy is 0.996220869200084\n",
      "[[[   0   14]\n",
      "  [   4 4745]]\n",
      "\n",
      " [[4745    4]\n",
      "  [  14    0]]]\n",
      "\n",
      "\n",
      "**Processing ppar_receptor_antagonist comments...**\n",
      "Test accuracy is 0.9987402897333614\n",
      "[[[   0    5]\n",
      "  [   1 4757]]\n",
      "\n",
      " [[4757    1]\n",
      "  [   5    0]]]\n",
      "\n",
      "\n",
      "**Processing progesterone_receptor_agonist comments...**\n",
      "Test accuracy is 0.994331303800126\n",
      "[[[   0   26]\n",
      "  [   1 4736]]\n",
      "\n",
      " [[4736    1]\n",
      "  [  26    0]]]\n",
      "\n",
      "\n",
      "**Processing progesterone_receptor_antagonist comments...**\n",
      "Test accuracy is 0.9993701448666806\n",
      "[[[   1    3]\n",
      "  [   0 4759]]\n",
      "\n",
      " [[4759    0]\n",
      "  [   3    1]]]\n",
      "\n",
      "\n",
      "**Processing prostaglandin_inhibitor comments...**\n",
      "Test accuracy is 0.9989502414444678\n",
      "[[[   0    5]\n",
      "  [   0 4758]]\n",
      "\n",
      " [[4758    0]\n",
      "  [   5    0]]]\n",
      "\n",
      "\n",
      "**Processing prostanoid_receptor_antagonist comments...**\n",
      "Test accuracy is 0.9960109174889775\n",
      "[[[   0   18]\n",
      "  [   1 4744]]\n",
      "\n",
      " [[4744    1]\n",
      "  [  18    0]]]\n",
      "\n",
      "\n",
      "**Processing proteasome_inhibitor comments...**\n",
      "Test accuracy is 0.9993701448666806\n",
      "[[[ 134    1]\n",
      "  [   2 4626]]\n",
      "\n",
      " [[4626    2]\n",
      "  [   1  134]]]\n",
      "\n",
      "\n",
      "**Processing protein_kinase_inhibitor comments...**\n",
      "Test accuracy is 0.9979004828889355\n",
      "[[[   0    7]\n",
      "  [   3 4753]]\n",
      "\n",
      " [[4753    3]\n",
      "  [   7    0]]]\n",
      "\n",
      "\n",
      "**Processing protein_phosphatase_inhibitor comments...**\n",
      "Test accuracy is 0.9997900482888935\n",
      "[[[   0    1]\n",
      "  [   0 4762]]\n",
      "\n",
      " [[4762    0]\n",
      "  [   1    0]]]\n",
      "\n",
      "\n",
      "**Processing protein_synthesis_inhibitor comments...**\n",
      "Test accuracy is 0.9976905311778291\n",
      "[[[   7    8]\n",
      "  [   3 4745]]\n",
      "\n",
      " [[4745    3]\n",
      "  [   8    7]]]\n",
      "\n",
      "\n",
      "**Processing protein_tyrosine_kinase_inhibitor comments...**\n",
      "Test accuracy is 0.9987402897333614\n",
      "[[[   0    6]\n",
      "  [   0 4757]]\n",
      "\n",
      " [[4757    0]\n",
      "  [   6    0]]]\n",
      "\n",
      "\n",
      "**Processing radiopaque_medium comments...**\n",
      "Test accuracy is 0.9968507243334033\n",
      "[[[   0   13]\n",
      "  [   2 4748]]\n",
      "\n",
      " [[4748    2]\n",
      "  [  13    0]]]\n",
      "\n",
      "\n",
      "**Processing raf_inhibitor comments...**\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy is 0.9991601931555742\n",
      "[[[  40    2]\n",
      "  [   2 4719]]\n",
      "\n",
      " [[4719    2]\n",
      "  [   2   40]]]\n",
      "\n",
      "\n",
      "**Processing ras_gtpase_inhibitor comments...**\n",
      "Test accuracy is 0.9995800965777871\n",
      "[[[   0    2]\n",
      "  [   0 4761]]\n",
      "\n",
      " [[4761    0]\n",
      "  [   2    0]]]\n",
      "\n",
      "\n",
      "**Processing retinoid_receptor_agonist comments...**\n",
      "Test accuracy is 0.9958009657778711\n",
      "[[[   0   11]\n",
      "  [   9 4743]]\n",
      "\n",
      " [[4743    9]\n",
      "  [  11    0]]]\n",
      "\n",
      "\n",
      "**Processing retinoid_receptor_antagonist comments...**\n",
      "Test accuracy is 0.9997900482888935\n",
      "[[[   0    1]\n",
      "  [   0 4762]]\n",
      "\n",
      " [[4762    0]\n",
      "  [   1    0]]]\n",
      "\n",
      "\n",
      "**Processing rho_associated_kinase_inhibitor comments...**\n",
      "Test accuracy is 0.9976905311778291\n",
      "[[[   0    9]\n",
      "  [   2 4752]]\n",
      "\n",
      " [[4752    2]\n",
      "  [   9    0]]]\n",
      "\n",
      "\n",
      "**Processing ribonucleoside_reductase_inhibitor comments...**\n",
      "Test accuracy is 0.9970606760445098\n",
      "[[[   2   10]\n",
      "  [   4 4747]]\n",
      "\n",
      " [[4747    4]\n",
      "  [  10    2]]]\n",
      "\n",
      "\n",
      "**Processing rna_polymerase_inhibitor comments...**\n",
      "Test accuracy is 0.9987402897333614\n",
      "[[[   0    6]\n",
      "  [   0 4757]]\n",
      "\n",
      " [[4757    0]\n",
      "  [   6    0]]]\n",
      "\n",
      "\n",
      "**Processing serotonin_receptor_agonist comments...**\n",
      "Test accuracy is 0.9899223178668907\n",
      "[[[   0   46]\n",
      "  [   2 4715]]\n",
      "\n",
      " [[4715    2]\n",
      "  [  46    0]]]\n",
      "\n",
      "\n",
      "**Processing serotonin_receptor_antagonist comments...**\n",
      "Test accuracy is 0.9817342011337392\n",
      "[[[   0   83]\n",
      "  [   4 4676]]\n",
      "\n",
      " [[4676    4]\n",
      "  [  83    0]]]\n",
      "\n",
      "\n",
      "**Processing serotonin_reuptake_inhibitor comments...**\n",
      "Test accuracy is 0.9974805794667226\n",
      "[[[   0   12]\n",
      "  [   0 4751]]\n",
      "\n",
      " [[4751    0]\n",
      "  [  12    0]]]\n",
      "\n",
      "\n",
      "**Processing sigma_receptor_agonist comments...**\n",
      "Test accuracy is 0.9983203863111484\n",
      "[[[   0    7]\n",
      "  [   1 4755]]\n",
      "\n",
      " [[4755    1]\n",
      "  [   7    0]]]\n",
      "\n",
      "\n",
      "**Processing sigma_receptor_antagonist comments...**\n",
      "Test accuracy is 0.998110434600042\n",
      "[[[   0    9]\n",
      "  [   0 4754]]\n",
      "\n",
      " [[4754    0]\n",
      "  [   9    0]]]\n",
      "\n",
      "\n",
      "**Processing smoothened_receptor_antagonist comments...**\n",
      "Test accuracy is 1.0\n",
      "[[[   0    0]\n",
      "  [   0 4763]]]\n",
      "\n",
      "\n",
      "**Processing sodium_channel_inhibitor comments...**\n",
      "Test accuracy is 0.9888725593113584\n",
      "[[[   0   51]\n",
      "  [   2 4710]]\n",
      "\n",
      " [[4710    2]\n",
      "  [  51    0]]]\n",
      "\n",
      "\n",
      "**Processing sphingosine_receptor_agonist comments...**\n",
      "Test accuracy is 0.9983203863111484\n",
      "[[[   0    8]\n",
      "  [   0 4755]]\n",
      "\n",
      " [[4755    0]\n",
      "  [   8    0]]]\n",
      "\n",
      "\n",
      "**Processing src_inhibitor comments...**\n",
      "Test accuracy is 0.9972706277556163\n",
      "[[[   2   11]\n",
      "  [   2 4748]]\n",
      "\n",
      " [[4748    2]\n",
      "  [  11    2]]]\n",
      "\n",
      "\n",
      "**Processing steroid comments...**\n",
      "Test accuracy is 0.9991601931555742\n",
      "[[[   0    4]\n",
      "  [   0 4759]]\n",
      "\n",
      " [[4759    0]\n",
      "  [   4    0]]]\n",
      "\n",
      "\n",
      "**Processing syk_inhibitor comments...**\n",
      "Test accuracy is 0.9995800965777871\n",
      "[[[   0    2]\n",
      "  [   0 4761]]\n",
      "\n",
      " [[4761    0]\n",
      "  [   2    0]]]\n",
      "\n",
      "\n",
      "**Processing tachykinin_antagonist comments...**\n",
      "Test accuracy is 0.9976905311778291\n",
      "[[[   0   11]\n",
      "  [   0 4752]]\n",
      "\n",
      " [[4752    0]\n",
      "  [  11    0]]]\n",
      "\n",
      "\n",
      "**Processing tgf-beta_receptor_inhibitor comments...**\n",
      "Test accuracy is 0.9989502414444678\n",
      "[[[   2    4]\n",
      "  [   1 4756]]\n",
      "\n",
      " [[4756    1]\n",
      "  [   4    2]]]\n",
      "\n",
      "\n",
      "**Processing thrombin_inhibitor comments...**\n",
      "Test accuracy is 0.9987402897333614\n",
      "[[[   0    6]\n",
      "  [   0 4757]]\n",
      "\n",
      " [[4757    0]\n",
      "  [   6    0]]]\n",
      "\n",
      "\n",
      "**Processing thymidylate_synthase_inhibitor comments...**\n",
      "Test accuracy is 0.9979004828889355\n",
      "[[[   0    9]\n",
      "  [   1 4753]]\n",
      "\n",
      " [[4753    1]\n",
      "  [   9    0]]]\n",
      "\n",
      "\n",
      "**Processing tlr_agonist comments...**\n",
      "Test accuracy is 0.9991601931555742\n",
      "[[[   0    3]\n",
      "  [   1 4759]]\n",
      "\n",
      " [[4759    1]\n",
      "  [   3    0]]]\n",
      "\n",
      "\n",
      "**Processing tlr_antagonist comments...**\n",
      "Test accuracy is 0.9997900482888935\n",
      "[[[   0    1]\n",
      "  [   0 4762]]\n",
      "\n",
      " [[4762    0]\n",
      "  [   1    0]]]\n",
      "\n",
      "\n",
      "**Processing tnf_inhibitor comments...**\n",
      "Test accuracy is 0.9985303380222549\n",
      "[[[   0    7]\n",
      "  [   0 4756]]\n",
      "\n",
      " [[4756    0]\n",
      "  [   7    0]]]\n",
      "\n",
      "\n",
      "**Processing topoisomerase_inhibitor comments...**\n",
      "Test accuracy is 0.9968507243334033\n",
      "[[[  15   11]\n",
      "  [   4 4733]]\n",
      "\n",
      " [[4733    4]\n",
      "  [  11   15]]]\n",
      "\n",
      "\n",
      "**Processing transient_receptor_potential_channel_antagonist comments...**\n",
      "Test accuracy is 0.9991601931555742\n",
      "[[[   0    4]\n",
      "  [   0 4759]]\n",
      "\n",
      " [[4759    0]\n",
      "  [   4    0]]]\n",
      "\n",
      "\n",
      "**Processing tropomyosin_receptor_kinase_inhibitor comments...**\n",
      "Test accuracy is 0.9995800965777871\n",
      "[[[   0    2]\n",
      "  [   0 4761]]\n",
      "\n",
      " [[4761    0]\n",
      "  [   2    0]]]\n",
      "\n",
      "\n",
      "**Processing trpv_agonist comments...**\n",
      "Test accuracy is 0.998110434600042\n",
      "[[[   0    9]\n",
      "  [   0 4754]]\n",
      "\n",
      " [[4754    0]\n",
      "  [   9    0]]]\n",
      "\n",
      "\n",
      "**Processing trpv_antagonist comments...**\n",
      "Test accuracy is 0.9987402897333614\n",
      "[[[   0    6]\n",
      "  [   0 4757]]\n",
      "\n",
      " [[4757    0]\n",
      "  [   6    0]]]\n",
      "\n",
      "\n",
      "**Processing tubulin_inhibitor comments...**\n",
      "Test accuracy is 0.9934914969557002\n",
      "[[[  38   19]\n",
      "  [  12 4694]]\n",
      "\n",
      " [[4694   12]\n",
      "  [  19   38]]]\n",
      "\n",
      "\n",
      "**Processing tyrosine_kinase_inhibitor comments...**\n",
      "Test accuracy is 0.9968507243334033\n",
      "[[[   0   10]\n",
      "  [   5 4748]]\n",
      "\n",
      " [[4748    5]\n",
      "  [  10    0]]]\n",
      "\n",
      "\n",
      "**Processing ubiquitin_specific_protease_inhibitor comments...**\n",
      "Test accuracy is 0.9995800965777871\n",
      "[[[   0    2]\n",
      "  [   0 4761]]\n",
      "\n",
      " [[4761    0]\n",
      "  [   2    0]]]\n",
      "\n",
      "\n",
      "**Processing vegfr_inhibitor comments...**\n",
      "Test accuracy is 0.9913919798446358\n",
      "[[[   3   36]\n",
      "  [   5 4719]]\n",
      "\n",
      " [[4719    5]\n",
      "  [  36    3]]]\n",
      "\n",
      "\n",
      "**Processing vitamin_b comments...**\n",
      "Test accuracy is 0.9983203863111484\n",
      "[[[   0    8]\n",
      "  [   0 4755]]\n",
      "\n",
      " [[4755    0]\n",
      "  [   8    0]]]\n",
      "\n",
      "\n",
      "**Processing vitamin_d_receptor_agonist comments...**\n",
      "Test accuracy is 0.9993701448666806\n",
      "[[[   2    3]\n",
      "  [   0 4758]]\n",
      "\n",
      " [[4758    0]\n",
      "  [   3    2]]]\n",
      "\n",
      "\n",
      "**Processing wnt_inhibitor comments...**\n",
      "Test accuracy is 0.9983203863111484\n",
      "[[[   0    7]\n",
      "  [   1 4755]]\n",
      "\n",
      " [[4755    1]\n",
      "  [   7    0]]]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_train, X_holdout, y_train, y_holdout = train_test_split(\n",
    "    train_pca, train_targets_scored.iloc[:,1:], test_size=0.2, random_state=74)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_train, y_train, test_size=0.33, random_state=42)\n",
    "\n",
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "categories = list(y_train.columns)\n",
    "# Using pipeline for applying logistic regression and one vs rest classifier\n",
    "LogReg_pipeline = Pipeline([\n",
    "                ('clf', OneVsRestClassifier(LogisticRegression(solver='sag'), n_jobs=-1)),\n",
    "            ])\n",
    "matrix = []\n",
    "for category in categories:\n",
    "    print('**Processing {} comments...**'.format(category))\n",
    "    \n",
    "    # Training logistic regression model on train data\n",
    "    LogReg_pipeline.fit(X_train, y_train[category])\n",
    "    \n",
    "    # calculating test accuracy\n",
    "    prediction = LogReg_pipeline.predict(X_holdout)\n",
    "    print('Test accuracy is {}'.format(accuracy_score(y_holdout[category], prediction)))\n",
    "    m = multilabel_confusion_matrix(y_holdout[category], prediction)\n",
    "    print(m)\n",
    "    a = 1\n",
    "    if len(m) <2:\n",
    "        a = 0\n",
    "    matrix.append(multilabel_confusion_matrix(y_holdout[category], prediction)[a])\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[   0,    0],\n",
       "        [   0, 4763]]])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multilabel_confusion_matrix(y_holdout[category], prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([0.9991601931555742, 0.9985303380222549, 0.9993701448666806, 0.9926501469970601, 0.9863531387780811, 0.9968507243334033, 0.9972706277556163, 0.9951711106445518, 0.9989502414444678, 0.9878176853602184, 0.9874028973336133, 0.9978991596638656, 0.9997900482888935, 0.997690046199076, 0.9997900482888935, 0.9997900482888935, 0.9983203863111484, 0.9955910140667646, 0.9983196807393405, 0.9989502414444678, 0.998110037799244, 0.9970594412938458, 0, 0.998109243697479, 0.9995800965777871, 0, 0.9995800965777871, 0.9995800965777871, 0.9966393614786809, 0.9995800965777871, 0.9989502414444678, 0.9976890756302521, 0.9972706277556163, 0.9995800965777871, 0, 0.9995800965777871, 0.9968454258675079, 0.999370012599748, 0.9966358284272497, 0.9997900482888935, 0.9972706277556163, 0.996640067198656, 0.9991601931555742, 0.99370012599748, 0.996220869200084, 0.994331303800126, 0, 0.998110434600042, 0.9993698802772527, 0.9972706277556163, 0.999160016799664, 0.9983179142136249, 0.9989502414444678, 0.9997900482888935, 0.99055018899622, 0.9987402897333614, 0.9993701448666806, 0.9972706277556163, 0.9987402897333614, 0.9987402897333614, 0.9995800965777871, 0.9953800923981521, 0.9991601931555742, 0.9970263381478335, 0.9966407726222969, 0.9993701448666806, 0.9985303380222549, 0.998320033599328, 0.997060058798824, 0.9991601931555742, 0.9979004828889355, 0.980684442578207, 0.9957992018483511, 0.9991596638655462, 0.9989502414444678, 0, 0.998109640831758, 0.9838031131678586, 0.9941188825876917, 0.9821203197307531, 0.9948892674616695, 0.9997900482888935, 0.9997900482888935, 0.9928601427971441, 0.9976895610165931, 0.9985303380222549, 0.9993701448666806, 0.9985303380222549, 0.9989502414444678, 0.9938585345192715, 0.999370012599748, 0.9991601931555742, 0.999580008399832, 0.996219281663516, 0.9937014486668067, 0.9989498004620878, 0.9980891719745223, 0.9995800965777871, 0.9966407726222969, 0.9863474060071413, 0.9995800965777871, 0.9978973927670312, 0.9968507243334033, 0.9983154348283849, 0.9968507243334033, 0.9892924627335713, 0.9995800965777871, 0.9983203863111484, 0.995590088198236, 0.9963952502120441, 0.9989455925769718, 0.9985303380222549, 0.9985303380222549, 0.9987402897333614, 0.9976895610165931, 0.998740025199496, 0.9987394957983193, 0.9979004828889355, 0.9960008419280152, 0.9961872484643084, 0.9997900482888935, 0.9997900482888935, 0.9976905311778291, 0.9997900482888935, 0.99895002099958, 0.9993701448666806, 0.999160016799664, 0.9983175604626708, 0.996220869200084, 0.9991601931555742, 0.9997900482888935, 0.9951690821256038, 0.9987402897333614, 0.9966337050284031, 0.998740025199496, 0.99895002099958, 0.9954565123323237, 0.9997900482888935, 0.9983203863111484, 0.9995800965777871, 0.9991601931555742, 0.9997900482888935, 0.9997900482888935, 0.9979004828889355, 0.9968507243334033, 0.9983203863111484, 0.9972677595628415, 0.999160016799664, 0.997060058798824, 0.9953379953379954, 0.9997900482888935, 0.9882402351952961, 0.999160016799664, 0.9941002949852508, 0.9989502414444678, 0.9972706277556163, 0.9964308209111904, 0.9970582055053583, 0.99895002099958, 0.994540109197816, 0.999370012599748, 0.9989502414444678, 0.996220075598488, 0.9997838772422736, 0.9985294117647059, 0.9997900482888935, 0.9983168525142015, 0.9987402897333614, 0.9972694812014282, 0.9995763609404787, 0.9995800965777871, 0.9976861590239798, 0.9997900482888935, 0.998109640831758, 0.9978978347698129, 0.9987402897333614, 0.9903381642512077, 0.9825593612103383, 0.9974805794667226, 0.998530029399412, 0.998110434600042, 0, 0.9892879647132955, 0.9983203863111484, 0.9976885900399244, 0.9991601931555742, 0.9995800965777871, 0.9976905311778291, 0.9991596638655462, 0.9987402897333614, 0.998110037799244, 0.999370012599748, 0.9997900482888935, 0.9985303380222549, 0.997681281618887, 0.9991601931555742, 0.9995800965777871, 0.998110434600042, 0.9987402897333614, 0.9959685974962869, 0.9978982765868012, 0.9995800965777871, 0.9924290220820189, 0.9983203863111484, 0.9993698802772527, 0.998530029399412], [1.0, 1.0, 1.0, 0.9997884940778342, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9997874149659864, 1.0, 0.9995791245791246, 1.0, 0.9997895622895623, 1.0, 1.0, 1.0, 1.0, 0.9995793901156678, 1.0, 0.9997896508203618, 0.9995788587070963, 0, 0.9995792131285504, 1.0, 0, 1.0, 1.0, 0.9995786812723826, 1.0, 1.0, 0.9993686868686869, 1.0, 1.0, 0, 1.0, 0.9989462592202318, 1.0, 1.0, 1.0, 1.0, 0.9997893406361913, 1.0, 0.999788717515318, 1.0, 1.0, 0, 1.0, 1.0, 1.0, 0.9997898718218113, 0.999368553988634, 1.0, 1.0, 0.9997880457821111, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9997890740350137, 1.0, 0.9985109551159328, 1.0, 1.0, 1.0, 0.9997896950578339, 0.9997894293535481, 1.0, 1.0, 1.0, 0.9995783259540375, 0.9995796553173603, 1.0, 0, 0.9995793016407236, 0.9987187700192185, 0.9995776135163674, 0.9985029940119761, 0.9985039538362898, 1.0, 1.0, 0.9997885388031297, 0.9995791245791246, 1.0, 1.0, 1.0, 1.0, 0.9966022510087067, 1.0, 1.0, 0.9997899600924176, 0.9995785036880928, 1.0, 0.9995796553173603, 0.9968193384223919, 1.0, 1.0, 0.9995742869306088, 1.0, 0.9995787700084247, 1.0, 0.9995783259540375, 1.0, 1.0, 1.0, 1.0, 0.9997891185153943, 0.9985125371865703, 0.9995779700358726, 1.0, 1.0, 1.0, 0.9997895179962113, 0.9997897834769813, 0.9993693504309439, 1.0, 0.9993664202745512, 0.9980899830220713, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9997898718218113, 1.0, 1.0, 1.0, 1.0, 0.9995780590717299, 1.0, 0.9991562961400549, 0.9997897834769813, 0.9997898276586801, 0.9984809027777778, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9997892962494732, 0.9997898718218113, 0.9997894293535481, 0.9978755045676652, 1.0, 0.9997875504567665, 0.9997898718218113, 0.9987298899237934, 1.0, 1.0, 1.0, 0.9991577174141925, 0.9997898276586801, 0.9997888959256913, 1.0, 1.0, 0.9997892518440463, 0.9995678478824547, 0.9993692178301093, 1.0, 0.9993681550126369, 1.0, 0.999578947368421, 0.9995763609404787, 1.0, 0.9981060606060606, 1.0, 0.9995793016407236, 0.9991580719848453, 1.0, 0.9995760016959933, 0.9991452991452991, 1.0, 0.9997897392767031, 1.0, 0, 0.9995755517826825, 1.0, 0.999578947368421, 1.0, 1.0, 1.0, 0.9997897834769813, 1.0, 0.9997896508203618, 0.9997899159663866, 1.0, 1.0, 0.9991555837027655, 1.0, 1.0, 1.0, 1.0, 0.9974500637484063, 0.998948032821376, 1.0, 0.9989415749364945, 1.0, 1.0, 0.9997897392767031], [1, 1, 1, 1, 1, 1, 1, 1, 1, 0.01694915254237288, 1, 0.09090909090909091, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1.0, 0.1, 1, 1.0, 1, 1, 1, 1, 1, 1, 1, 1, 1.0, 1, 0.16666666666666666, 0.25, 0.30434782608695654, 1, 1, 1, 1, 1, 1, 1, 1.0, 1, 0.4, 1, 1, 0.3333333333333333, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0.7741935483870968, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0.2, 1, 1.0, 1, 0.0375, 1, 0.022988505747126436, 0.7142857142857143, 1, 1, 1, 1, 1, 1, 1, 1, 0.46296296296296297, 0.25, 1, 1, 1, 1, 1, 0.8085106382978723, 1, 1, 1, 1, 0.3333333333333333, 1, 0.6, 1, 1, 1, 1, 1, 0.7017543859649122, 0.7916666666666666, 1, 1, 1, 0.08333333333333333, 1, 1, 1, 0.32142857142857145, 0.6470588235294118, 1, 1, 1, 1, 0.16666666666666666, 1, 1, 0.5, 1, 1, 1, 1, 1, 0.2727272727272727, 1, 1, 0.864516129032258, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0.23529411764705882, 1, 1, 0.6071428571428571, 1, 1, 1, 0.28205128205128205, 1, 1, 1, 1, 1, 1, 0.25, 1, 1, 0.9925925925925926, 1, 1, 0.4666666666666667, 1, 1, 0.9523809523809523, 1, 1, 1, 1, 0.16666666666666666, 1, 1, 1, 1, 1, 1, 1.0, 1, 1, 0.15384615384615385, 1, 1, 1, 0.3333333333333333, 1, 1, 1, 1, 1, 0.5769230769230769, 1, 1, 1, 1, 0.6666666666666666, 1, 1, 0.07692307692307693, 1, 0.4, 1])\n"
     ]
    }
   ],
   "source": [
    "def average_stats(matrix):\n",
    "    precision, recall, specificity = [],[],[]\n",
    "    for m in matrix:\n",
    "        divider = 0 if (m[0][0]+m[1][0]) == 0 else (m[0][0]+m[1][0])\n",
    "        if divider == 0:\n",
    "            precision.append(0)\n",
    "        else:\n",
    "            precision.append(m[0][0] / divider)\n",
    "        divider = 0 if (m[0][0]+m[0][1]) == 0 else (m[0][0]+m[0][1])\n",
    "        if divider == 0:\n",
    "            recall.append(0)\n",
    "        else:\n",
    "            recall.append(m[0][0] / divider)\n",
    "        divider = 0 if (m[1][1]+m[1][0]) == 0 else (m[1][1]+m[1][0])\n",
    "        if divider == 0:\n",
    "            specificity.append(1)\n",
    "        elif m[1][1] == 0:\n",
    "            specificity.append(1)\n",
    "        else:\n",
    "            specificity.append(m[1][1]/divider)\n",
    "    return precision, recall, specificity\n",
    "        \n",
    "\n",
    "print(average_stats(matrix))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The overall (average) precision, recall and specificity are\n",
      "0.968269291563848 0.9706157707022307 0.8009708737864077\n"
     ]
    }
   ],
   "source": [
    "print(\"The overall (average) precision, recall and specificity are\")\n",
    "precision, recall, specificity = average_stats(matrix)\n",
    "count = 0\n",
    "CONSTANT = 0\n",
    "for i, num in enumerate(specificity):\n",
    "    if num != 1:\n",
    "        specificity[i] = specificity[i] * CONSTANT\n",
    "        count = count+1\n",
    "print(sum(precision)/len(precision), sum(recall)/len(recall), sum(specificity)/(len(specificity)+count*CONSTANT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[7856,    1],\n",
       "       [   2,    0]])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = list(y_train.columns)\n",
    "# Learn to predict each class against the other\n",
    "classifier = OneVsRestClassifier(LogisticRegression(solver='sag'), n_jobs=-1)\n",
    "y_score = classifier.fit(X_train, y_train).decision_function(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yianding/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_ranking.py:811: UndefinedMetricWarning: No positive samples in y_true, true positive value should be meaningless\n",
      "  warnings.warn(\"No positive samples in y_true, \"\n"
     ]
    }
   ],
   "source": [
    "# Compute ROC curve and ROC area for each class\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "for i in range(len(categories)):\n",
    "    fpr[i], tpr[i], _ = roc_curve(y_test.iloc[:, i], y_score[:, i])\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "This solver needs samples of at least 2 classes in the data, but the data contains only one class: 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-203-f93dde21435f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultioutput\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMultiOutputClassifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mlogistic\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mMultiOutputClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLogisticRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrandom_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m999\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mlogistic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#Fitting the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m#Generating predictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/multioutput.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, Y, sample_weight, **fit_params)\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0mself\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    350\u001b[0m         \"\"\"\n\u001b[0;32m--> 351\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    352\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mestimator\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimators_\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/multioutput.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, **fit_params)\u001b[0m\n\u001b[1;32m    170\u001b[0m         \u001b[0mfit_params_validated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_fit_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m         self.estimators_ = Parallel(n_jobs=self.n_jobs)(\n\u001b[0m\u001b[1;32m    173\u001b[0m             delayed(_fit_estimator)(\n\u001b[1;32m    174\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1030\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1031\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1032\u001b[0;31m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1033\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    845\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    846\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 847\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    848\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    763\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    764\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 765\u001b[0;31m             \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    766\u001b[0m             \u001b[0;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    767\u001b[0m             \u001b[0;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    206\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    570\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    571\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 572\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    573\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    250\u001b[0m         \u001b[0;31m# change the default number of processes to -1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 252\u001b[0;31m             return [func(*args, **kwargs)\n\u001b[0m\u001b[1;32m    253\u001b[0m                     for func, args, kwargs in self.items]\n\u001b[1;32m    254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    250\u001b[0m         \u001b[0;31m# change the default number of processes to -1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 252\u001b[0;31m             return [func(*args, **kwargs)\n\u001b[0m\u001b[1;32m    253\u001b[0m                     for func, args, kwargs in self.items]\n\u001b[1;32m    254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/multioutput.py\u001b[0m in \u001b[0;36m_fit_estimator\u001b[0;34m(estimator, X, y, sample_weight, **fit_params)\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m         \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   1370\u001b[0m         \u001b[0mclasses_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1371\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mn_classes\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1372\u001b[0;31m             raise ValueError(\"This solver needs samples of at least 2 classes\"\n\u001b[0m\u001b[1;32m   1373\u001b[0m                              \u001b[0;34m\" in the data, but the data contains only one\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1374\u001b[0m                              \" class: %r\" % classes_[0])\n",
      "\u001b[0;31mValueError\u001b[0m: This solver needs samples of at least 2 classes in the data, but the data contains only one class: 0"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "logistic= MultiOutputClassifier(LogisticRegression(max_iter=10000, tol=0.1, C = 0.5,verbose=0,random_state = 999))\n",
    "logistic.fit(X_train,y_train)#Fitting the model \n",
    "\n",
    "#Generating predictions\n",
    "pred_log_proba=logistic.predict_proba(X_test)\n",
    "pred_log_proba_t=pred_transform(pred_log_proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/kushal1506/deciding-n-components-in-pca?select=test_features.csv\n",
    "https://www.kaggle.com/arpitsolanki14/moa-exploratory-analysis-pca-ensemble-models#Dimensionality-Reduction---Principal-Component-Analysis-"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
