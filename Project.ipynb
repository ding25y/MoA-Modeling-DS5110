{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "from sklearn.decomposition import PCA\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler \n",
    "# from sklearn.model_selection import StratifiedKFold\n",
    "# from sklearn.metrics import log_loss\n",
    "# from tqdm.notebook import tqdm\n",
    "# from time import time\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.multiclass import OneVsRestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir('lish-moa')\n",
    "train_features = pd.read_csv('lish-moa/train_features.csv')\n",
    "train_targets_scored = pd.read_csv('lish-moa/train_targets_scored.csv')\n",
    "train_targets_nonscored = pd.read_csv('lish-moa/train_targets_nonscored.csv')\n",
    "test_features = pd.read_csv('lish-moa/test_features.csv')\n",
    "submission = pd.read_csv('lish-moa/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cp_type\n",
       "ctl_vehicle        0\n",
       "trt_cp         16844\n",
       "dtype: int64"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is inspired by the discussion here: https://www.kaggle.com/c/lish-moa/discussion/180165\n",
    "train = train_features.merge(train_targets_scored, on = 'sig_id')\n",
    "cols = [c for c in train_targets_scored.columns] + ['cp_type']\n",
    "train[cols].groupby('cp_type').sum().sum(1)\n",
    "# As a result, if cp_type is ctl_vehicle, we predict all to be zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing and Feature Engineering\n",
    "my plan here is to try PCA or tsne, then use Random Forest and XGBoost. Lastly, Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sig_id</th>\n",
       "      <th>cp_type</th>\n",
       "      <th>cp_time</th>\n",
       "      <th>cp_dose</th>\n",
       "      <th>g-0</th>\n",
       "      <th>g-1</th>\n",
       "      <th>g-2</th>\n",
       "      <th>g-3</th>\n",
       "      <th>g-4</th>\n",
       "      <th>g-5</th>\n",
       "      <th>...</th>\n",
       "      <th>c-90</th>\n",
       "      <th>c-91</th>\n",
       "      <th>c-92</th>\n",
       "      <th>c-93</th>\n",
       "      <th>c-94</th>\n",
       "      <th>c-95</th>\n",
       "      <th>c-96</th>\n",
       "      <th>c-97</th>\n",
       "      <th>c-98</th>\n",
       "      <th>c-99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id_000644bb2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0620</td>\n",
       "      <td>0.5577</td>\n",
       "      <td>-0.2479</td>\n",
       "      <td>-0.6208</td>\n",
       "      <td>-0.1944</td>\n",
       "      <td>-1.0120</td>\n",
       "      <td>...</td>\n",
       "      <td>0.2862</td>\n",
       "      <td>0.2584</td>\n",
       "      <td>0.8076</td>\n",
       "      <td>0.5523</td>\n",
       "      <td>-0.1912</td>\n",
       "      <td>0.6584</td>\n",
       "      <td>-0.3981</td>\n",
       "      <td>0.2139</td>\n",
       "      <td>0.3801</td>\n",
       "      <td>0.4176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id_000779bfc</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0743</td>\n",
       "      <td>0.4087</td>\n",
       "      <td>0.2991</td>\n",
       "      <td>0.0604</td>\n",
       "      <td>1.0190</td>\n",
       "      <td>0.5207</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.4265</td>\n",
       "      <td>0.7543</td>\n",
       "      <td>0.4708</td>\n",
       "      <td>0.0230</td>\n",
       "      <td>0.2957</td>\n",
       "      <td>0.4899</td>\n",
       "      <td>0.1522</td>\n",
       "      <td>0.1241</td>\n",
       "      <td>0.6077</td>\n",
       "      <td>0.7371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id_000a6266a</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.6280</td>\n",
       "      <td>0.5817</td>\n",
       "      <td>1.5540</td>\n",
       "      <td>-0.0764</td>\n",
       "      <td>-0.0323</td>\n",
       "      <td>1.2390</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.7250</td>\n",
       "      <td>-0.6297</td>\n",
       "      <td>0.6103</td>\n",
       "      <td>0.0223</td>\n",
       "      <td>-1.3240</td>\n",
       "      <td>-0.3174</td>\n",
       "      <td>-0.6417</td>\n",
       "      <td>-0.2187</td>\n",
       "      <td>-1.4080</td>\n",
       "      <td>0.6931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id_0015fd391</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.5138</td>\n",
       "      <td>-0.2491</td>\n",
       "      <td>-0.2656</td>\n",
       "      <td>0.5288</td>\n",
       "      <td>4.0620</td>\n",
       "      <td>-0.8095</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.0990</td>\n",
       "      <td>-0.6441</td>\n",
       "      <td>-5.6300</td>\n",
       "      <td>-1.3780</td>\n",
       "      <td>-0.8632</td>\n",
       "      <td>-1.2880</td>\n",
       "      <td>-1.6210</td>\n",
       "      <td>-0.8784</td>\n",
       "      <td>-0.3876</td>\n",
       "      <td>-0.8154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id_001626bd3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.3254</td>\n",
       "      <td>-0.4009</td>\n",
       "      <td>0.9700</td>\n",
       "      <td>0.6919</td>\n",
       "      <td>1.4180</td>\n",
       "      <td>-0.8244</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0042</td>\n",
       "      <td>0.0048</td>\n",
       "      <td>0.6670</td>\n",
       "      <td>1.0690</td>\n",
       "      <td>0.5523</td>\n",
       "      <td>-0.3031</td>\n",
       "      <td>0.1094</td>\n",
       "      <td>0.2885</td>\n",
       "      <td>-0.3786</td>\n",
       "      <td>0.7125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23809</th>\n",
       "      <td>id_fffb1ceed</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1394</td>\n",
       "      <td>-0.0636</td>\n",
       "      <td>-0.1112</td>\n",
       "      <td>-0.5080</td>\n",
       "      <td>-0.4713</td>\n",
       "      <td>0.7201</td>\n",
       "      <td>...</td>\n",
       "      <td>0.1969</td>\n",
       "      <td>0.0262</td>\n",
       "      <td>-0.8121</td>\n",
       "      <td>0.3434</td>\n",
       "      <td>0.5372</td>\n",
       "      <td>-0.3246</td>\n",
       "      <td>0.0631</td>\n",
       "      <td>0.9171</td>\n",
       "      <td>0.5258</td>\n",
       "      <td>0.4680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23810</th>\n",
       "      <td>id_fffb70c0c</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.3260</td>\n",
       "      <td>0.3478</td>\n",
       "      <td>-0.3743</td>\n",
       "      <td>0.9905</td>\n",
       "      <td>-0.7178</td>\n",
       "      <td>0.6621</td>\n",
       "      <td>...</td>\n",
       "      <td>0.4286</td>\n",
       "      <td>0.4426</td>\n",
       "      <td>0.0423</td>\n",
       "      <td>-0.3195</td>\n",
       "      <td>-0.8086</td>\n",
       "      <td>-0.9798</td>\n",
       "      <td>-0.2084</td>\n",
       "      <td>-0.1224</td>\n",
       "      <td>-0.2715</td>\n",
       "      <td>0.3689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23811</th>\n",
       "      <td>id_fffc1c3f4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.3942</td>\n",
       "      <td>0.3756</td>\n",
       "      <td>0.3109</td>\n",
       "      <td>-0.7389</td>\n",
       "      <td>0.5505</td>\n",
       "      <td>-0.0159</td>\n",
       "      <td>...</td>\n",
       "      <td>0.5409</td>\n",
       "      <td>0.3755</td>\n",
       "      <td>0.7343</td>\n",
       "      <td>0.2807</td>\n",
       "      <td>0.4116</td>\n",
       "      <td>0.6422</td>\n",
       "      <td>0.2256</td>\n",
       "      <td>0.7592</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.3808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23812</th>\n",
       "      <td>id_fffcb9e7c</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.6660</td>\n",
       "      <td>0.2324</td>\n",
       "      <td>0.4392</td>\n",
       "      <td>0.2044</td>\n",
       "      <td>0.8531</td>\n",
       "      <td>-0.0343</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.1105</td>\n",
       "      <td>0.4258</td>\n",
       "      <td>-0.2012</td>\n",
       "      <td>0.1506</td>\n",
       "      <td>1.5230</td>\n",
       "      <td>0.7101</td>\n",
       "      <td>0.1732</td>\n",
       "      <td>0.7015</td>\n",
       "      <td>-0.6290</td>\n",
       "      <td>0.0740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23813</th>\n",
       "      <td>id_ffffdd77b</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.8598</td>\n",
       "      <td>1.0240</td>\n",
       "      <td>-0.1361</td>\n",
       "      <td>0.7952</td>\n",
       "      <td>-0.3611</td>\n",
       "      <td>-3.6750</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.3890</td>\n",
       "      <td>-1.7450</td>\n",
       "      <td>-6.6300</td>\n",
       "      <td>-4.0950</td>\n",
       "      <td>-7.3860</td>\n",
       "      <td>-1.4160</td>\n",
       "      <td>-3.5770</td>\n",
       "      <td>-0.4775</td>\n",
       "      <td>-2.1500</td>\n",
       "      <td>-4.2520</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>23814 rows × 876 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             sig_id  cp_type  cp_time  cp_dose     g-0     g-1     g-2  \\\n",
       "0      id_000644bb2        0        0        0  1.0620  0.5577 -0.2479   \n",
       "1      id_000779bfc        0        2        0  0.0743  0.4087  0.2991   \n",
       "2      id_000a6266a        0        1        0  0.6280  0.5817  1.5540   \n",
       "3      id_0015fd391        0        1        0 -0.5138 -0.2491 -0.2656   \n",
       "4      id_001626bd3        0        2        1 -0.3254 -0.4009  0.9700   \n",
       "...             ...      ...      ...      ...     ...     ...     ...   \n",
       "23809  id_fffb1ceed        0        0        1  0.1394 -0.0636 -0.1112   \n",
       "23810  id_fffb70c0c        0        0        1 -1.3260  0.3478 -0.3743   \n",
       "23811  id_fffc1c3f4        1        1        1  0.3942  0.3756  0.3109   \n",
       "23812  id_fffcb9e7c        0        0        0  0.6660  0.2324  0.4392   \n",
       "23813  id_ffffdd77b        0        2        0 -0.8598  1.0240 -0.1361   \n",
       "\n",
       "          g-3     g-4     g-5  ...    c-90    c-91    c-92    c-93    c-94  \\\n",
       "0     -0.6208 -0.1944 -1.0120  ...  0.2862  0.2584  0.8076  0.5523 -0.1912   \n",
       "1      0.0604  1.0190  0.5207  ... -0.4265  0.7543  0.4708  0.0230  0.2957   \n",
       "2     -0.0764 -0.0323  1.2390  ... -0.7250 -0.6297  0.6103  0.0223 -1.3240   \n",
       "3      0.5288  4.0620 -0.8095  ... -2.0990 -0.6441 -5.6300 -1.3780 -0.8632   \n",
       "4      0.6919  1.4180 -0.8244  ...  0.0042  0.0048  0.6670  1.0690  0.5523   \n",
       "...       ...     ...     ...  ...     ...     ...     ...     ...     ...   \n",
       "23809 -0.5080 -0.4713  0.7201  ...  0.1969  0.0262 -0.8121  0.3434  0.5372   \n",
       "23810  0.9905 -0.7178  0.6621  ...  0.4286  0.4426  0.0423 -0.3195 -0.8086   \n",
       "23811 -0.7389  0.5505 -0.0159  ...  0.5409  0.3755  0.7343  0.2807  0.4116   \n",
       "23812  0.2044  0.8531 -0.0343  ... -0.1105  0.4258 -0.2012  0.1506  1.5230   \n",
       "23813  0.7952 -0.3611 -3.6750  ... -3.3890 -1.7450 -6.6300 -4.0950 -7.3860   \n",
       "\n",
       "         c-95    c-96    c-97    c-98    c-99  \n",
       "0      0.6584 -0.3981  0.2139  0.3801  0.4176  \n",
       "1      0.4899  0.1522  0.1241  0.6077  0.7371  \n",
       "2     -0.3174 -0.6417 -0.2187 -1.4080  0.6931  \n",
       "3     -1.2880 -1.6210 -0.8784 -0.3876 -0.8154  \n",
       "4     -0.3031  0.1094  0.2885 -0.3786  0.7125  \n",
       "...       ...     ...     ...     ...     ...  \n",
       "23809 -0.3246  0.0631  0.9171  0.5258  0.4680  \n",
       "23810 -0.9798 -0.2084 -0.1224 -0.2715  0.3689  \n",
       "23811  0.6422  0.2256  0.7592  0.6656  0.3808  \n",
       "23812  0.7101  0.1732  0.7015 -0.6290  0.0740  \n",
       "23813 -1.4160 -3.5770 -0.4775 -2.1500 -4.2520  \n",
       "\n",
       "[23814 rows x 876 columns]"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# removing cases when cp_type is ctl_vehicle\n",
    "#train = train[train['cp_type']!='ctl_vehicle'].reset_index(drop=True)\n",
    "# transform the TWO categorical variables into numerical variables\n",
    "# this is label encoding, may try one hot encoding later\n",
    "train['cp_time'] = train['cp_time'].map({24:0, 48:1, 72:2})\n",
    "train['cp_dose'] = train['cp_dose'].map({'D1':0,'D2':1})\n",
    "train['cp_type'] = train['cp_type'].map({'trt_cp':0,'ctl_vehicle':1})\n",
    "#make id index\n",
    "#train = train.set_index(['sig_id'])\n",
    "\n",
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sig_id</th>\n",
       "      <th>cp_type</th>\n",
       "      <th>cp_time</th>\n",
       "      <th>cp_dose</th>\n",
       "      <th>g-0</th>\n",
       "      <th>g-1</th>\n",
       "      <th>g-2</th>\n",
       "      <th>g-3</th>\n",
       "      <th>g-4</th>\n",
       "      <th>g-5</th>\n",
       "      <th>...</th>\n",
       "      <th>c-90</th>\n",
       "      <th>c-91</th>\n",
       "      <th>c-92</th>\n",
       "      <th>c-93</th>\n",
       "      <th>c-94</th>\n",
       "      <th>c-95</th>\n",
       "      <th>c-96</th>\n",
       "      <th>c-97</th>\n",
       "      <th>c-98</th>\n",
       "      <th>c-99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id_000644bb2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0620</td>\n",
       "      <td>0.5577</td>\n",
       "      <td>-0.2479</td>\n",
       "      <td>-0.6208</td>\n",
       "      <td>-0.1944</td>\n",
       "      <td>-1.0120</td>\n",
       "      <td>...</td>\n",
       "      <td>0.2862</td>\n",
       "      <td>0.2584</td>\n",
       "      <td>0.8076</td>\n",
       "      <td>0.5523</td>\n",
       "      <td>-0.1912</td>\n",
       "      <td>0.6584</td>\n",
       "      <td>-0.3981</td>\n",
       "      <td>0.2139</td>\n",
       "      <td>0.3801</td>\n",
       "      <td>0.4176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id_000779bfc</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0743</td>\n",
       "      <td>0.4087</td>\n",
       "      <td>0.2991</td>\n",
       "      <td>0.0604</td>\n",
       "      <td>1.0190</td>\n",
       "      <td>0.5207</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.4265</td>\n",
       "      <td>0.7543</td>\n",
       "      <td>0.4708</td>\n",
       "      <td>0.0230</td>\n",
       "      <td>0.2957</td>\n",
       "      <td>0.4899</td>\n",
       "      <td>0.1522</td>\n",
       "      <td>0.1241</td>\n",
       "      <td>0.6077</td>\n",
       "      <td>0.7371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id_000a6266a</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.6280</td>\n",
       "      <td>0.5817</td>\n",
       "      <td>1.5540</td>\n",
       "      <td>-0.0764</td>\n",
       "      <td>-0.0323</td>\n",
       "      <td>1.2390</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.7250</td>\n",
       "      <td>-0.6297</td>\n",
       "      <td>0.6103</td>\n",
       "      <td>0.0223</td>\n",
       "      <td>-1.3240</td>\n",
       "      <td>-0.3174</td>\n",
       "      <td>-0.6417</td>\n",
       "      <td>-0.2187</td>\n",
       "      <td>-1.4080</td>\n",
       "      <td>0.6931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id_0015fd391</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.5138</td>\n",
       "      <td>-0.2491</td>\n",
       "      <td>-0.2656</td>\n",
       "      <td>0.5288</td>\n",
       "      <td>4.0620</td>\n",
       "      <td>-0.8095</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.0990</td>\n",
       "      <td>-0.6441</td>\n",
       "      <td>-5.6300</td>\n",
       "      <td>-1.3780</td>\n",
       "      <td>-0.8632</td>\n",
       "      <td>-1.2880</td>\n",
       "      <td>-1.6210</td>\n",
       "      <td>-0.8784</td>\n",
       "      <td>-0.3876</td>\n",
       "      <td>-0.8154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id_001626bd3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.3254</td>\n",
       "      <td>-0.4009</td>\n",
       "      <td>0.9700</td>\n",
       "      <td>0.6919</td>\n",
       "      <td>1.4180</td>\n",
       "      <td>-0.8244</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0042</td>\n",
       "      <td>0.0048</td>\n",
       "      <td>0.6670</td>\n",
       "      <td>1.0690</td>\n",
       "      <td>0.5523</td>\n",
       "      <td>-0.3031</td>\n",
       "      <td>0.1094</td>\n",
       "      <td>0.2885</td>\n",
       "      <td>-0.3786</td>\n",
       "      <td>0.7125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23809</th>\n",
       "      <td>id_fffb1ceed</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1394</td>\n",
       "      <td>-0.0636</td>\n",
       "      <td>-0.1112</td>\n",
       "      <td>-0.5080</td>\n",
       "      <td>-0.4713</td>\n",
       "      <td>0.7201</td>\n",
       "      <td>...</td>\n",
       "      <td>0.1969</td>\n",
       "      <td>0.0262</td>\n",
       "      <td>-0.8121</td>\n",
       "      <td>0.3434</td>\n",
       "      <td>0.5372</td>\n",
       "      <td>-0.3246</td>\n",
       "      <td>0.0631</td>\n",
       "      <td>0.9171</td>\n",
       "      <td>0.5258</td>\n",
       "      <td>0.4680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23810</th>\n",
       "      <td>id_fffb70c0c</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.3260</td>\n",
       "      <td>0.3478</td>\n",
       "      <td>-0.3743</td>\n",
       "      <td>0.9905</td>\n",
       "      <td>-0.7178</td>\n",
       "      <td>0.6621</td>\n",
       "      <td>...</td>\n",
       "      <td>0.4286</td>\n",
       "      <td>0.4426</td>\n",
       "      <td>0.0423</td>\n",
       "      <td>-0.3195</td>\n",
       "      <td>-0.8086</td>\n",
       "      <td>-0.9798</td>\n",
       "      <td>-0.2084</td>\n",
       "      <td>-0.1224</td>\n",
       "      <td>-0.2715</td>\n",
       "      <td>0.3689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23811</th>\n",
       "      <td>id_fffc1c3f4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.3942</td>\n",
       "      <td>0.3756</td>\n",
       "      <td>0.3109</td>\n",
       "      <td>-0.7389</td>\n",
       "      <td>0.5505</td>\n",
       "      <td>-0.0159</td>\n",
       "      <td>...</td>\n",
       "      <td>0.5409</td>\n",
       "      <td>0.3755</td>\n",
       "      <td>0.7343</td>\n",
       "      <td>0.2807</td>\n",
       "      <td>0.4116</td>\n",
       "      <td>0.6422</td>\n",
       "      <td>0.2256</td>\n",
       "      <td>0.7592</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.3808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23812</th>\n",
       "      <td>id_fffcb9e7c</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.6660</td>\n",
       "      <td>0.2324</td>\n",
       "      <td>0.4392</td>\n",
       "      <td>0.2044</td>\n",
       "      <td>0.8531</td>\n",
       "      <td>-0.0343</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.1105</td>\n",
       "      <td>0.4258</td>\n",
       "      <td>-0.2012</td>\n",
       "      <td>0.1506</td>\n",
       "      <td>1.5230</td>\n",
       "      <td>0.7101</td>\n",
       "      <td>0.1732</td>\n",
       "      <td>0.7015</td>\n",
       "      <td>-0.6290</td>\n",
       "      <td>0.0740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23813</th>\n",
       "      <td>id_ffffdd77b</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.8598</td>\n",
       "      <td>1.0240</td>\n",
       "      <td>-0.1361</td>\n",
       "      <td>0.7952</td>\n",
       "      <td>-0.3611</td>\n",
       "      <td>-3.6750</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.3890</td>\n",
       "      <td>-1.7450</td>\n",
       "      <td>-6.6300</td>\n",
       "      <td>-4.0950</td>\n",
       "      <td>-7.3860</td>\n",
       "      <td>-1.4160</td>\n",
       "      <td>-3.5770</td>\n",
       "      <td>-0.4775</td>\n",
       "      <td>-2.1500</td>\n",
       "      <td>-4.2520</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>23814 rows × 876 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             sig_id  cp_type  cp_time  cp_dose     g-0     g-1     g-2  \\\n",
       "0      id_000644bb2        0        0        0  1.0620  0.5577 -0.2479   \n",
       "1      id_000779bfc        0        2        0  0.0743  0.4087  0.2991   \n",
       "2      id_000a6266a        0        1        0  0.6280  0.5817  1.5540   \n",
       "3      id_0015fd391        0        1        0 -0.5138 -0.2491 -0.2656   \n",
       "4      id_001626bd3        0        2        1 -0.3254 -0.4009  0.9700   \n",
       "...             ...      ...      ...      ...     ...     ...     ...   \n",
       "23809  id_fffb1ceed        0        0        1  0.1394 -0.0636 -0.1112   \n",
       "23810  id_fffb70c0c        0        0        1 -1.3260  0.3478 -0.3743   \n",
       "23811  id_fffc1c3f4        1        1        1  0.3942  0.3756  0.3109   \n",
       "23812  id_fffcb9e7c        0        0        0  0.6660  0.2324  0.4392   \n",
       "23813  id_ffffdd77b        0        2        0 -0.8598  1.0240 -0.1361   \n",
       "\n",
       "          g-3     g-4     g-5  ...    c-90    c-91    c-92    c-93    c-94  \\\n",
       "0     -0.6208 -0.1944 -1.0120  ...  0.2862  0.2584  0.8076  0.5523 -0.1912   \n",
       "1      0.0604  1.0190  0.5207  ... -0.4265  0.7543  0.4708  0.0230  0.2957   \n",
       "2     -0.0764 -0.0323  1.2390  ... -0.7250 -0.6297  0.6103  0.0223 -1.3240   \n",
       "3      0.5288  4.0620 -0.8095  ... -2.0990 -0.6441 -5.6300 -1.3780 -0.8632   \n",
       "4      0.6919  1.4180 -0.8244  ...  0.0042  0.0048  0.6670  1.0690  0.5523   \n",
       "...       ...     ...     ...  ...     ...     ...     ...     ...     ...   \n",
       "23809 -0.5080 -0.4713  0.7201  ...  0.1969  0.0262 -0.8121  0.3434  0.5372   \n",
       "23810  0.9905 -0.7178  0.6621  ...  0.4286  0.4426  0.0423 -0.3195 -0.8086   \n",
       "23811 -0.7389  0.5505 -0.0159  ...  0.5409  0.3755  0.7343  0.2807  0.4116   \n",
       "23812  0.2044  0.8531 -0.0343  ... -0.1105  0.4258 -0.2012  0.1506  1.5230   \n",
       "23813  0.7952 -0.3611 -3.6750  ... -3.3890 -1.7450 -6.6300 -4.0950 -7.3860   \n",
       "\n",
       "         c-95    c-96    c-97    c-98    c-99  \n",
       "0      0.6584 -0.3981  0.2139  0.3801  0.4176  \n",
       "1      0.4899  0.1522  0.1241  0.6077  0.7371  \n",
       "2     -0.3174 -0.6417 -0.2187 -1.4080  0.6931  \n",
       "3     -1.2880 -1.6210 -0.8784 -0.3876 -0.8154  \n",
       "4     -0.3031  0.1094  0.2885 -0.3786  0.7125  \n",
       "...       ...     ...     ...     ...     ...  \n",
       "23809 -0.3246  0.0631  0.9171  0.5258  0.4680  \n",
       "23810 -0.9798 -0.2084 -0.1224 -0.2715  0.3689  \n",
       "23811  0.6422  0.2256  0.7592  0.6656  0.3808  \n",
       "23812  0.7101  0.1732  0.7015 -0.6290  0.0740  \n",
       "23813 -1.4160 -3.5770 -0.4775 -2.1500 -4.2520  \n",
       "\n",
       "[23814 rows x 876 columns]"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GENES = [col for col in train_features.columns if col.startswith('g-')]\n",
    "CELLS = [col for col in train_features.columns if col.startswith('c-')]\n",
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #not utilized\n",
    "# #standardize data\n",
    "# for col in (GENES + CELLS):\n",
    "#     transformer = QuantileTransformer(random_state=86, output_distribution=\"normal\")\n",
    "#     vec = train[col].values.reshape(len(train[col].values), 1)\n",
    "#     transformer.fit(vec)\n",
    "\n",
    "#     train[col] = transformer.transform(vec).reshape(1, len(train[col].values))[0]\n",
    "# #this step is utterly unnecessary, I guess this values were standardized before releasing\n",
    "# train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# this part is for presetation\n",
    "PCA-In our data we have about 775 gene expression variables and 100 cell viability variables and 23000+ records in the training dataset. Building an ensemble learning model on this dataset would take a large amount of time, and also we noticed that a lot of cell viability variables are correlated to each other. Therefore we would be looking at dimensionality reduction to overcome these issues. Here we would be implementing Principal Component Analysis(PCA) to achieve dimensionality reduction.\n",
    "When to use PCA?\n",
    "When we want to reduce the number of independent variables but still don't want to lose the information available from those variables.\n",
    "When we want to ensure our variables are independent of each other\n",
    "When we are comfortable with making our independent variables less interpretable\n",
    "Steps we will follow to implement PCA -\n",
    "Remove outliers and standardize the variables\n",
    "Covariance Matrix computation and calculation of Eigen Values. The PCA function in sklearn package takes care of these details and we don't have to worry about them\n",
    "Plot the explained variance by Principal Components and select the number of principal components to include."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pca = PCA().fit(train)\n",
    "\n",
    "# fig, ax = plt.subplots()\n",
    "# xi = np.arange(0,874, step=1)\n",
    "# y = np.cumsum(pca.explained_variance_ratio_)\n",
    "\n",
    "# plt.ylim(0.0,1.2)\n",
    "# plt.plot(xi, y)\n",
    "\n",
    "# plt.xlabel('Number of Components')\n",
    "# #plt.xticks(np.arange(0, 750, step=50)) #change from 0-based array index to 1-based human-readable label\n",
    "# plt.ylabel('Cumulative variance (%)')\n",
    "# plt.title('The number of components needed to explain variance')\n",
    "\n",
    "# plt.axhline(y=0.90, linestyle='-')\n",
    "# plt.text(0.75, 0.3, '95% cut-off threshold')#, color = 'red', fontsize=16)\n",
    "\n",
    "# ax.grid(axis='x')\n",
    "# plt.show()\n",
    "# #we need ~550 pc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-191-77a055c1dc10>:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  col[col>3]=3\n",
      "<ipython-input-191-77a055c1dc10>:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  col[col<-3]=-3\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cp_type</th>\n",
       "      <th>cp_time</th>\n",
       "      <th>cp_dose</th>\n",
       "      <th>g-0</th>\n",
       "      <th>g-1</th>\n",
       "      <th>g-2</th>\n",
       "      <th>g-3</th>\n",
       "      <th>g-4</th>\n",
       "      <th>g-5</th>\n",
       "      <th>g-6</th>\n",
       "      <th>...</th>\n",
       "      <th>c-90</th>\n",
       "      <th>c-91</th>\n",
       "      <th>c-92</th>\n",
       "      <th>c-93</th>\n",
       "      <th>c-94</th>\n",
       "      <th>c-95</th>\n",
       "      <th>c-96</th>\n",
       "      <th>c-97</th>\n",
       "      <th>c-98</th>\n",
       "      <th>c-99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0620</td>\n",
       "      <td>0.5577</td>\n",
       "      <td>-0.2479</td>\n",
       "      <td>-0.6208</td>\n",
       "      <td>-0.1944</td>\n",
       "      <td>-1.0120</td>\n",
       "      <td>-1.0220</td>\n",
       "      <td>...</td>\n",
       "      <td>0.2862</td>\n",
       "      <td>0.2584</td>\n",
       "      <td>0.8076</td>\n",
       "      <td>0.5523</td>\n",
       "      <td>-0.1912</td>\n",
       "      <td>0.6584</td>\n",
       "      <td>-0.3981</td>\n",
       "      <td>0.2139</td>\n",
       "      <td>0.3801</td>\n",
       "      <td>0.4176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0743</td>\n",
       "      <td>0.4087</td>\n",
       "      <td>0.2991</td>\n",
       "      <td>0.0604</td>\n",
       "      <td>1.0190</td>\n",
       "      <td>0.5207</td>\n",
       "      <td>0.2341</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.4265</td>\n",
       "      <td>0.7543</td>\n",
       "      <td>0.4708</td>\n",
       "      <td>0.0230</td>\n",
       "      <td>0.2957</td>\n",
       "      <td>0.4899</td>\n",
       "      <td>0.1522</td>\n",
       "      <td>0.1241</td>\n",
       "      <td>0.6077</td>\n",
       "      <td>0.7371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.6280</td>\n",
       "      <td>0.5817</td>\n",
       "      <td>1.5540</td>\n",
       "      <td>-0.0764</td>\n",
       "      <td>-0.0323</td>\n",
       "      <td>1.2390</td>\n",
       "      <td>0.1715</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.7250</td>\n",
       "      <td>-0.6297</td>\n",
       "      <td>0.6103</td>\n",
       "      <td>0.0223</td>\n",
       "      <td>-1.3240</td>\n",
       "      <td>-0.3174</td>\n",
       "      <td>-0.6417</td>\n",
       "      <td>-0.2187</td>\n",
       "      <td>-1.4080</td>\n",
       "      <td>0.6931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.5138</td>\n",
       "      <td>-0.2491</td>\n",
       "      <td>-0.2656</td>\n",
       "      <td>0.5288</td>\n",
       "      <td>3.0000</td>\n",
       "      <td>-0.8095</td>\n",
       "      <td>-1.9590</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.0990</td>\n",
       "      <td>-0.6441</td>\n",
       "      <td>-3.0000</td>\n",
       "      <td>-1.3780</td>\n",
       "      <td>-0.8632</td>\n",
       "      <td>-1.2880</td>\n",
       "      <td>-1.6210</td>\n",
       "      <td>-0.8784</td>\n",
       "      <td>-0.3876</td>\n",
       "      <td>-0.8154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.3254</td>\n",
       "      <td>-0.4009</td>\n",
       "      <td>0.9700</td>\n",
       "      <td>0.6919</td>\n",
       "      <td>1.4180</td>\n",
       "      <td>-0.8244</td>\n",
       "      <td>-0.2800</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0042</td>\n",
       "      <td>0.0048</td>\n",
       "      <td>0.6670</td>\n",
       "      <td>1.0690</td>\n",
       "      <td>0.5523</td>\n",
       "      <td>-0.3031</td>\n",
       "      <td>0.1094</td>\n",
       "      <td>0.2885</td>\n",
       "      <td>-0.3786</td>\n",
       "      <td>0.7125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23809</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1394</td>\n",
       "      <td>-0.0636</td>\n",
       "      <td>-0.1112</td>\n",
       "      <td>-0.5080</td>\n",
       "      <td>-0.4713</td>\n",
       "      <td>0.7201</td>\n",
       "      <td>0.5773</td>\n",
       "      <td>...</td>\n",
       "      <td>0.1969</td>\n",
       "      <td>0.0262</td>\n",
       "      <td>-0.8121</td>\n",
       "      <td>0.3434</td>\n",
       "      <td>0.5372</td>\n",
       "      <td>-0.3246</td>\n",
       "      <td>0.0631</td>\n",
       "      <td>0.9171</td>\n",
       "      <td>0.5258</td>\n",
       "      <td>0.4680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23810</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.3260</td>\n",
       "      <td>0.3478</td>\n",
       "      <td>-0.3743</td>\n",
       "      <td>0.9905</td>\n",
       "      <td>-0.7178</td>\n",
       "      <td>0.6621</td>\n",
       "      <td>-0.2252</td>\n",
       "      <td>...</td>\n",
       "      <td>0.4286</td>\n",
       "      <td>0.4426</td>\n",
       "      <td>0.0423</td>\n",
       "      <td>-0.3195</td>\n",
       "      <td>-0.8086</td>\n",
       "      <td>-0.9798</td>\n",
       "      <td>-0.2084</td>\n",
       "      <td>-0.1224</td>\n",
       "      <td>-0.2715</td>\n",
       "      <td>0.3689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23811</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.3942</td>\n",
       "      <td>0.3756</td>\n",
       "      <td>0.3109</td>\n",
       "      <td>-0.7389</td>\n",
       "      <td>0.5505</td>\n",
       "      <td>-0.0159</td>\n",
       "      <td>-0.2541</td>\n",
       "      <td>...</td>\n",
       "      <td>0.5409</td>\n",
       "      <td>0.3755</td>\n",
       "      <td>0.7343</td>\n",
       "      <td>0.2807</td>\n",
       "      <td>0.4116</td>\n",
       "      <td>0.6422</td>\n",
       "      <td>0.2256</td>\n",
       "      <td>0.7592</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.3808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23812</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.6660</td>\n",
       "      <td>0.2324</td>\n",
       "      <td>0.4392</td>\n",
       "      <td>0.2044</td>\n",
       "      <td>0.8531</td>\n",
       "      <td>-0.0343</td>\n",
       "      <td>0.0323</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.1105</td>\n",
       "      <td>0.4258</td>\n",
       "      <td>-0.2012</td>\n",
       "      <td>0.1506</td>\n",
       "      <td>1.5230</td>\n",
       "      <td>0.7101</td>\n",
       "      <td>0.1732</td>\n",
       "      <td>0.7015</td>\n",
       "      <td>-0.6290</td>\n",
       "      <td>0.0740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23813</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.8598</td>\n",
       "      <td>1.0240</td>\n",
       "      <td>-0.1361</td>\n",
       "      <td>0.7952</td>\n",
       "      <td>-0.3611</td>\n",
       "      <td>-3.0000</td>\n",
       "      <td>-1.2420</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.0000</td>\n",
       "      <td>-1.7450</td>\n",
       "      <td>-3.0000</td>\n",
       "      <td>-3.0000</td>\n",
       "      <td>-3.0000</td>\n",
       "      <td>-1.4160</td>\n",
       "      <td>-3.0000</td>\n",
       "      <td>-0.4775</td>\n",
       "      <td>-2.1500</td>\n",
       "      <td>-3.0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>23814 rows × 875 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       cp_type  cp_time  cp_dose     g-0     g-1     g-2     g-3     g-4  \\\n",
       "0            0        0        0  1.0620  0.5577 -0.2479 -0.6208 -0.1944   \n",
       "1            0        2        0  0.0743  0.4087  0.2991  0.0604  1.0190   \n",
       "2            0        1        0  0.6280  0.5817  1.5540 -0.0764 -0.0323   \n",
       "3            0        1        0 -0.5138 -0.2491 -0.2656  0.5288  3.0000   \n",
       "4            0        2        1 -0.3254 -0.4009  0.9700  0.6919  1.4180   \n",
       "...        ...      ...      ...     ...     ...     ...     ...     ...   \n",
       "23809        0        0        1  0.1394 -0.0636 -0.1112 -0.5080 -0.4713   \n",
       "23810        0        0        1 -1.3260  0.3478 -0.3743  0.9905 -0.7178   \n",
       "23811        1        1        1  0.3942  0.3756  0.3109 -0.7389  0.5505   \n",
       "23812        0        0        0  0.6660  0.2324  0.4392  0.2044  0.8531   \n",
       "23813        0        2        0 -0.8598  1.0240 -0.1361  0.7952 -0.3611   \n",
       "\n",
       "          g-5     g-6  ...    c-90    c-91    c-92    c-93    c-94    c-95  \\\n",
       "0     -1.0120 -1.0220  ...  0.2862  0.2584  0.8076  0.5523 -0.1912  0.6584   \n",
       "1      0.5207  0.2341  ... -0.4265  0.7543  0.4708  0.0230  0.2957  0.4899   \n",
       "2      1.2390  0.1715  ... -0.7250 -0.6297  0.6103  0.0223 -1.3240 -0.3174   \n",
       "3     -0.8095 -1.9590  ... -2.0990 -0.6441 -3.0000 -1.3780 -0.8632 -1.2880   \n",
       "4     -0.8244 -0.2800  ...  0.0042  0.0048  0.6670  1.0690  0.5523 -0.3031   \n",
       "...       ...     ...  ...     ...     ...     ...     ...     ...     ...   \n",
       "23809  0.7201  0.5773  ...  0.1969  0.0262 -0.8121  0.3434  0.5372 -0.3246   \n",
       "23810  0.6621 -0.2252  ...  0.4286  0.4426  0.0423 -0.3195 -0.8086 -0.9798   \n",
       "23811 -0.0159 -0.2541  ...  0.5409  0.3755  0.7343  0.2807  0.4116  0.6422   \n",
       "23812 -0.0343  0.0323  ... -0.1105  0.4258 -0.2012  0.1506  1.5230  0.7101   \n",
       "23813 -3.0000 -1.2420  ... -3.0000 -1.7450 -3.0000 -3.0000 -3.0000 -1.4160   \n",
       "\n",
       "         c-96    c-97    c-98    c-99  \n",
       "0     -0.3981  0.2139  0.3801  0.4176  \n",
       "1      0.1522  0.1241  0.6077  0.7371  \n",
       "2     -0.6417 -0.2187 -1.4080  0.6931  \n",
       "3     -1.6210 -0.8784 -0.3876 -0.8154  \n",
       "4      0.1094  0.2885 -0.3786  0.7125  \n",
       "...       ...     ...     ...     ...  \n",
       "23809  0.0631  0.9171  0.5258  0.4680  \n",
       "23810 -0.2084 -0.1224 -0.2715  0.3689  \n",
       "23811  0.2256  0.7592  0.6656  0.3808  \n",
       "23812  0.1732  0.7015 -0.6290  0.0740  \n",
       "23813 -3.0000 -0.4775 -2.1500 -3.0000  \n",
       "\n",
       "[23814 rows x 875 columns]"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def remove_outliers(col):\n",
    "    col[col>3]=3\n",
    "    col[col<-3]=-3\n",
    "    return col\n",
    "train=train.iloc[:,1:].apply(remove_outliers)\n",
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>c-0</th>\n",
       "      <th>c-1</th>\n",
       "      <th>c-2</th>\n",
       "      <th>c-3</th>\n",
       "      <th>c-4</th>\n",
       "      <th>c-5</th>\n",
       "      <th>c-6</th>\n",
       "      <th>c-7</th>\n",
       "      <th>c-8</th>\n",
       "      <th>c-9</th>\n",
       "      <th>...</th>\n",
       "      <th>c-90</th>\n",
       "      <th>c-91</th>\n",
       "      <th>c-92</th>\n",
       "      <th>c-93</th>\n",
       "      <th>c-94</th>\n",
       "      <th>c-95</th>\n",
       "      <th>c-96</th>\n",
       "      <th>c-97</th>\n",
       "      <th>c-98</th>\n",
       "      <th>c-99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.0600</td>\n",
       "      <td>0.1083</td>\n",
       "      <td>0.6864</td>\n",
       "      <td>0.4043</td>\n",
       "      <td>0.4213</td>\n",
       "      <td>-0.6797</td>\n",
       "      <td>0.2888</td>\n",
       "      <td>0.4323</td>\n",
       "      <td>-0.3381</td>\n",
       "      <td>0.3407</td>\n",
       "      <td>...</td>\n",
       "      <td>0.2862</td>\n",
       "      <td>0.2584</td>\n",
       "      <td>0.8076</td>\n",
       "      <td>0.5523</td>\n",
       "      <td>-0.1912</td>\n",
       "      <td>0.6584</td>\n",
       "      <td>-0.3981</td>\n",
       "      <td>0.2139</td>\n",
       "      <td>0.3801</td>\n",
       "      <td>0.4176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0927</td>\n",
       "      <td>0.2723</td>\n",
       "      <td>0.2772</td>\n",
       "      <td>0.7776</td>\n",
       "      <td>0.3679</td>\n",
       "      <td>0.5696</td>\n",
       "      <td>0.2835</td>\n",
       "      <td>1.4080</td>\n",
       "      <td>0.3745</td>\n",
       "      <td>0.6775</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.4265</td>\n",
       "      <td>0.7543</td>\n",
       "      <td>0.4708</td>\n",
       "      <td>0.0230</td>\n",
       "      <td>0.2957</td>\n",
       "      <td>0.4899</td>\n",
       "      <td>0.1522</td>\n",
       "      <td>0.1241</td>\n",
       "      <td>0.6077</td>\n",
       "      <td>0.7371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.1312</td>\n",
       "      <td>-1.4640</td>\n",
       "      <td>0.3394</td>\n",
       "      <td>-1.7790</td>\n",
       "      <td>0.2188</td>\n",
       "      <td>0.5826</td>\n",
       "      <td>-0.7513</td>\n",
       "      <td>0.0543</td>\n",
       "      <td>0.7182</td>\n",
       "      <td>-0.4159</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.7250</td>\n",
       "      <td>-0.6297</td>\n",
       "      <td>0.6103</td>\n",
       "      <td>0.0223</td>\n",
       "      <td>-1.3240</td>\n",
       "      <td>-0.3174</td>\n",
       "      <td>-0.6417</td>\n",
       "      <td>-0.2187</td>\n",
       "      <td>-1.4080</td>\n",
       "      <td>0.6931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.3998</td>\n",
       "      <td>-3.0000</td>\n",
       "      <td>-2.7350</td>\n",
       "      <td>-1.9630</td>\n",
       "      <td>-2.8610</td>\n",
       "      <td>-1.2670</td>\n",
       "      <td>-2.5830</td>\n",
       "      <td>-0.5036</td>\n",
       "      <td>-3.0000</td>\n",
       "      <td>-1.8510</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.0990</td>\n",
       "      <td>-0.6441</td>\n",
       "      <td>-3.0000</td>\n",
       "      <td>-1.3780</td>\n",
       "      <td>-0.8632</td>\n",
       "      <td>-1.2880</td>\n",
       "      <td>-1.6210</td>\n",
       "      <td>-0.8784</td>\n",
       "      <td>-0.3876</td>\n",
       "      <td>-0.8154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.3774</td>\n",
       "      <td>0.7364</td>\n",
       "      <td>-0.1659</td>\n",
       "      <td>0.2341</td>\n",
       "      <td>1.0060</td>\n",
       "      <td>0.3204</td>\n",
       "      <td>-0.0852</td>\n",
       "      <td>-0.2284</td>\n",
       "      <td>-0.2533</td>\n",
       "      <td>-0.3174</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0042</td>\n",
       "      <td>0.0048</td>\n",
       "      <td>0.6670</td>\n",
       "      <td>1.0690</td>\n",
       "      <td>0.5523</td>\n",
       "      <td>-0.3031</td>\n",
       "      <td>0.1094</td>\n",
       "      <td>0.2885</td>\n",
       "      <td>-0.3786</td>\n",
       "      <td>0.7125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23809</th>\n",
       "      <td>0.4224</td>\n",
       "      <td>0.1871</td>\n",
       "      <td>-0.4822</td>\n",
       "      <td>0.3713</td>\n",
       "      <td>0.4754</td>\n",
       "      <td>0.9512</td>\n",
       "      <td>0.4650</td>\n",
       "      <td>0.3005</td>\n",
       "      <td>0.0338</td>\n",
       "      <td>-0.7734</td>\n",
       "      <td>...</td>\n",
       "      <td>0.1969</td>\n",
       "      <td>0.0262</td>\n",
       "      <td>-0.8121</td>\n",
       "      <td>0.3434</td>\n",
       "      <td>0.5372</td>\n",
       "      <td>-0.3246</td>\n",
       "      <td>0.0631</td>\n",
       "      <td>0.9171</td>\n",
       "      <td>0.5258</td>\n",
       "      <td>0.4680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23810</th>\n",
       "      <td>0.2144</td>\n",
       "      <td>0.4350</td>\n",
       "      <td>0.1174</td>\n",
       "      <td>1.3960</td>\n",
       "      <td>-0.6772</td>\n",
       "      <td>0.2316</td>\n",
       "      <td>-0.5396</td>\n",
       "      <td>0.0581</td>\n",
       "      <td>0.6067</td>\n",
       "      <td>-0.4622</td>\n",
       "      <td>...</td>\n",
       "      <td>0.4286</td>\n",
       "      <td>0.4426</td>\n",
       "      <td>0.0423</td>\n",
       "      <td>-0.3195</td>\n",
       "      <td>-0.8086</td>\n",
       "      <td>-0.9798</td>\n",
       "      <td>-0.2084</td>\n",
       "      <td>-0.1224</td>\n",
       "      <td>-0.2715</td>\n",
       "      <td>0.3689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23811</th>\n",
       "      <td>1.0650</td>\n",
       "      <td>0.6329</td>\n",
       "      <td>0.3742</td>\n",
       "      <td>0.1237</td>\n",
       "      <td>0.6147</td>\n",
       "      <td>0.4589</td>\n",
       "      <td>-0.2372</td>\n",
       "      <td>1.1160</td>\n",
       "      <td>0.4623</td>\n",
       "      <td>0.2830</td>\n",
       "      <td>...</td>\n",
       "      <td>0.5409</td>\n",
       "      <td>0.3755</td>\n",
       "      <td>0.7343</td>\n",
       "      <td>0.2807</td>\n",
       "      <td>0.4116</td>\n",
       "      <td>0.6422</td>\n",
       "      <td>0.2256</td>\n",
       "      <td>0.7592</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.3808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23812</th>\n",
       "      <td>0.5377</td>\n",
       "      <td>1.3240</td>\n",
       "      <td>0.9679</td>\n",
       "      <td>0.1419</td>\n",
       "      <td>1.2230</td>\n",
       "      <td>0.3404</td>\n",
       "      <td>-0.1589</td>\n",
       "      <td>0.8667</td>\n",
       "      <td>0.7749</td>\n",
       "      <td>-0.1458</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.1105</td>\n",
       "      <td>0.4258</td>\n",
       "      <td>-0.2012</td>\n",
       "      <td>0.1506</td>\n",
       "      <td>1.5230</td>\n",
       "      <td>0.7101</td>\n",
       "      <td>0.1732</td>\n",
       "      <td>0.7015</td>\n",
       "      <td>-0.6290</td>\n",
       "      <td>0.0740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23813</th>\n",
       "      <td>-2.3890</td>\n",
       "      <td>-2.0350</td>\n",
       "      <td>-1.1080</td>\n",
       "      <td>-3.0000</td>\n",
       "      <td>-3.0000</td>\n",
       "      <td>-2.0280</td>\n",
       "      <td>-3.0000</td>\n",
       "      <td>-1.3220</td>\n",
       "      <td>-1.9920</td>\n",
       "      <td>-1.6540</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.0000</td>\n",
       "      <td>-1.7450</td>\n",
       "      <td>-3.0000</td>\n",
       "      <td>-3.0000</td>\n",
       "      <td>-3.0000</td>\n",
       "      <td>-1.4160</td>\n",
       "      <td>-3.0000</td>\n",
       "      <td>-0.4775</td>\n",
       "      <td>-2.1500</td>\n",
       "      <td>-3.0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>23814 rows × 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          c-0     c-1     c-2     c-3     c-4     c-5     c-6     c-7     c-8  \\\n",
       "0     -0.0600  0.1083  0.6864  0.4043  0.4213 -0.6797  0.2888  0.4323 -0.3381   \n",
       "1      0.0927  0.2723  0.2772  0.7776  0.3679  0.5696  0.2835  1.4080  0.3745   \n",
       "2     -0.1312 -1.4640  0.3394 -1.7790  0.2188  0.5826 -0.7513  0.0543  0.7182   \n",
       "3     -0.3998 -3.0000 -2.7350 -1.9630 -2.8610 -1.2670 -2.5830 -0.5036 -3.0000   \n",
       "4     -0.3774  0.7364 -0.1659  0.2341  1.0060  0.3204 -0.0852 -0.2284 -0.2533   \n",
       "...       ...     ...     ...     ...     ...     ...     ...     ...     ...   \n",
       "23809  0.4224  0.1871 -0.4822  0.3713  0.4754  0.9512  0.4650  0.3005  0.0338   \n",
       "23810  0.2144  0.4350  0.1174  1.3960 -0.6772  0.2316 -0.5396  0.0581  0.6067   \n",
       "23811  1.0650  0.6329  0.3742  0.1237  0.6147  0.4589 -0.2372  1.1160  0.4623   \n",
       "23812  0.5377  1.3240  0.9679  0.1419  1.2230  0.3404 -0.1589  0.8667  0.7749   \n",
       "23813 -2.3890 -2.0350 -1.1080 -3.0000 -3.0000 -2.0280 -3.0000 -1.3220 -1.9920   \n",
       "\n",
       "          c-9  ...    c-90    c-91    c-92    c-93    c-94    c-95    c-96  \\\n",
       "0      0.3407  ...  0.2862  0.2584  0.8076  0.5523 -0.1912  0.6584 -0.3981   \n",
       "1      0.6775  ... -0.4265  0.7543  0.4708  0.0230  0.2957  0.4899  0.1522   \n",
       "2     -0.4159  ... -0.7250 -0.6297  0.6103  0.0223 -1.3240 -0.3174 -0.6417   \n",
       "3     -1.8510  ... -2.0990 -0.6441 -3.0000 -1.3780 -0.8632 -1.2880 -1.6210   \n",
       "4     -0.3174  ...  0.0042  0.0048  0.6670  1.0690  0.5523 -0.3031  0.1094   \n",
       "...       ...  ...     ...     ...     ...     ...     ...     ...     ...   \n",
       "23809 -0.7734  ...  0.1969  0.0262 -0.8121  0.3434  0.5372 -0.3246  0.0631   \n",
       "23810 -0.4622  ...  0.4286  0.4426  0.0423 -0.3195 -0.8086 -0.9798 -0.2084   \n",
       "23811  0.2830  ...  0.5409  0.3755  0.7343  0.2807  0.4116  0.6422  0.2256   \n",
       "23812 -0.1458  ... -0.1105  0.4258 -0.2012  0.1506  1.5230  0.7101  0.1732   \n",
       "23813 -1.6540  ... -3.0000 -1.7450 -3.0000 -3.0000 -3.0000 -1.4160 -3.0000   \n",
       "\n",
       "         c-97    c-98    c-99  \n",
       "0      0.2139  0.3801  0.4176  \n",
       "1      0.1241  0.6077  0.7371  \n",
       "2     -0.2187 -1.4080  0.6931  \n",
       "3     -0.8784 -0.3876 -0.8154  \n",
       "4      0.2885 -0.3786  0.7125  \n",
       "...       ...     ...     ...  \n",
       "23809  0.9171  0.5258  0.4680  \n",
       "23810 -0.1224 -0.2715  0.3689  \n",
       "23811  0.7592  0.6656  0.3808  \n",
       "23812  0.7015 -0.6290  0.0740  \n",
       "23813 -0.4775 -2.1500 -3.0000  \n",
       "\n",
       "[23814 rows x 100 columns]"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#pc for cells\n",
    "\n",
    "train_c=train.loc[:, CELLS]\n",
    "x = StandardScaler().fit_transform(train_c)\n",
    "pca_c = PCA(n_components=50)\n",
    "principalComponents = pca_c.fit_transform(x)\n",
    "principalDf_c = pd.DataFrame(data = principalComponents)\n",
    "\n",
    "plt.plot(np.cumsum(pca_c.explained_variance_ratio_))\n",
    "plt.xlabel('number of components')\n",
    "plt.ylabel('cumulative explained variance')\n",
    "plt.axhline(y=0.80, linestyle='-')\n",
    "plt.title('Cumulative Explained Variance for Cell Viability Variable PCAs')\n",
    "train_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>g-0</th>\n",
       "      <th>g-1</th>\n",
       "      <th>g-2</th>\n",
       "      <th>g-3</th>\n",
       "      <th>g-4</th>\n",
       "      <th>g-5</th>\n",
       "      <th>g-6</th>\n",
       "      <th>g-7</th>\n",
       "      <th>g-8</th>\n",
       "      <th>g-9</th>\n",
       "      <th>...</th>\n",
       "      <th>g-762</th>\n",
       "      <th>g-763</th>\n",
       "      <th>g-764</th>\n",
       "      <th>g-765</th>\n",
       "      <th>g-766</th>\n",
       "      <th>g-767</th>\n",
       "      <th>g-768</th>\n",
       "      <th>g-769</th>\n",
       "      <th>g-770</th>\n",
       "      <th>g-771</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0620</td>\n",
       "      <td>0.5577</td>\n",
       "      <td>-0.2479</td>\n",
       "      <td>-0.6208</td>\n",
       "      <td>-0.1944</td>\n",
       "      <td>-1.0120</td>\n",
       "      <td>-1.0220</td>\n",
       "      <td>-0.0326</td>\n",
       "      <td>0.5548</td>\n",
       "      <td>-0.0921</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.5055</td>\n",
       "      <td>-0.3167</td>\n",
       "      <td>1.0930</td>\n",
       "      <td>0.0084</td>\n",
       "      <td>0.8611</td>\n",
       "      <td>-0.5582</td>\n",
       "      <td>0.3008</td>\n",
       "      <td>1.6490</td>\n",
       "      <td>0.2968</td>\n",
       "      <td>-0.0224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0743</td>\n",
       "      <td>0.4087</td>\n",
       "      <td>0.2991</td>\n",
       "      <td>0.0604</td>\n",
       "      <td>1.0190</td>\n",
       "      <td>0.5207</td>\n",
       "      <td>0.2341</td>\n",
       "      <td>0.3372</td>\n",
       "      <td>-0.4047</td>\n",
       "      <td>0.8507</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.5338</td>\n",
       "      <td>0.0224</td>\n",
       "      <td>-0.4831</td>\n",
       "      <td>0.2128</td>\n",
       "      <td>-0.6999</td>\n",
       "      <td>-0.1214</td>\n",
       "      <td>-0.1626</td>\n",
       "      <td>-0.3340</td>\n",
       "      <td>-0.3289</td>\n",
       "      <td>-0.2718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.6280</td>\n",
       "      <td>0.5817</td>\n",
       "      <td>1.5540</td>\n",
       "      <td>-0.0764</td>\n",
       "      <td>-0.0323</td>\n",
       "      <td>1.2390</td>\n",
       "      <td>0.1715</td>\n",
       "      <td>0.2155</td>\n",
       "      <td>0.0065</td>\n",
       "      <td>1.2300</td>\n",
       "      <td>...</td>\n",
       "      <td>2.5770</td>\n",
       "      <td>0.2356</td>\n",
       "      <td>1.3230</td>\n",
       "      <td>-1.3730</td>\n",
       "      <td>-0.2682</td>\n",
       "      <td>0.8427</td>\n",
       "      <td>0.5797</td>\n",
       "      <td>0.3143</td>\n",
       "      <td>0.8133</td>\n",
       "      <td>0.7923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.5138</td>\n",
       "      <td>-0.2491</td>\n",
       "      <td>-0.2656</td>\n",
       "      <td>0.5288</td>\n",
       "      <td>3.0000</td>\n",
       "      <td>-0.8095</td>\n",
       "      <td>-1.9590</td>\n",
       "      <td>0.1792</td>\n",
       "      <td>-0.1321</td>\n",
       "      <td>-1.0600</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.1292</td>\n",
       "      <td>3.0000</td>\n",
       "      <td>1.2720</td>\n",
       "      <td>-0.4733</td>\n",
       "      <td>-2.0560</td>\n",
       "      <td>0.5699</td>\n",
       "      <td>0.1996</td>\n",
       "      <td>0.4374</td>\n",
       "      <td>0.1588</td>\n",
       "      <td>-0.0343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.3254</td>\n",
       "      <td>-0.4009</td>\n",
       "      <td>0.9700</td>\n",
       "      <td>0.6919</td>\n",
       "      <td>1.4180</td>\n",
       "      <td>-0.8244</td>\n",
       "      <td>-0.2800</td>\n",
       "      <td>-0.1498</td>\n",
       "      <td>-0.8789</td>\n",
       "      <td>0.8630</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.6904</td>\n",
       "      <td>2.0540</td>\n",
       "      <td>-0.3131</td>\n",
       "      <td>-0.0809</td>\n",
       "      <td>0.3910</td>\n",
       "      <td>1.7660</td>\n",
       "      <td>-1.0020</td>\n",
       "      <td>-0.7534</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>-0.6269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23809</th>\n",
       "      <td>0.1394</td>\n",
       "      <td>-0.0636</td>\n",
       "      <td>-0.1112</td>\n",
       "      <td>-0.5080</td>\n",
       "      <td>-0.4713</td>\n",
       "      <td>0.7201</td>\n",
       "      <td>0.5773</td>\n",
       "      <td>0.3055</td>\n",
       "      <td>-0.4726</td>\n",
       "      <td>0.1269</td>\n",
       "      <td>...</td>\n",
       "      <td>0.7790</td>\n",
       "      <td>0.5393</td>\n",
       "      <td>0.4112</td>\n",
       "      <td>-0.5059</td>\n",
       "      <td>0.0240</td>\n",
       "      <td>-0.2297</td>\n",
       "      <td>0.7221</td>\n",
       "      <td>0.5099</td>\n",
       "      <td>-0.1423</td>\n",
       "      <td>0.3806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23810</th>\n",
       "      <td>-1.3260</td>\n",
       "      <td>0.3478</td>\n",
       "      <td>-0.3743</td>\n",
       "      <td>0.9905</td>\n",
       "      <td>-0.7178</td>\n",
       "      <td>0.6621</td>\n",
       "      <td>-0.2252</td>\n",
       "      <td>-0.5565</td>\n",
       "      <td>0.5112</td>\n",
       "      <td>0.6727</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.0858</td>\n",
       "      <td>0.3606</td>\n",
       "      <td>-0.0248</td>\n",
       "      <td>0.0672</td>\n",
       "      <td>-0.5901</td>\n",
       "      <td>-0.1022</td>\n",
       "      <td>0.5247</td>\n",
       "      <td>0.5438</td>\n",
       "      <td>-0.1875</td>\n",
       "      <td>-0.4751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23811</th>\n",
       "      <td>0.3942</td>\n",
       "      <td>0.3756</td>\n",
       "      <td>0.3109</td>\n",
       "      <td>-0.7389</td>\n",
       "      <td>0.5505</td>\n",
       "      <td>-0.0159</td>\n",
       "      <td>-0.2541</td>\n",
       "      <td>0.1745</td>\n",
       "      <td>-0.0340</td>\n",
       "      <td>0.4865</td>\n",
       "      <td>...</td>\n",
       "      <td>0.1796</td>\n",
       "      <td>0.3488</td>\n",
       "      <td>0.0927</td>\n",
       "      <td>0.5166</td>\n",
       "      <td>-0.3099</td>\n",
       "      <td>-0.5946</td>\n",
       "      <td>0.9778</td>\n",
       "      <td>0.2326</td>\n",
       "      <td>-0.6191</td>\n",
       "      <td>0.3603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23812</th>\n",
       "      <td>0.6660</td>\n",
       "      <td>0.2324</td>\n",
       "      <td>0.4392</td>\n",
       "      <td>0.2044</td>\n",
       "      <td>0.8531</td>\n",
       "      <td>-0.0343</td>\n",
       "      <td>0.0323</td>\n",
       "      <td>0.0463</td>\n",
       "      <td>0.4299</td>\n",
       "      <td>-0.7985</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.1410</td>\n",
       "      <td>1.9590</td>\n",
       "      <td>0.8224</td>\n",
       "      <td>1.2500</td>\n",
       "      <td>-3.0000</td>\n",
       "      <td>-2.8720</td>\n",
       "      <td>0.1794</td>\n",
       "      <td>0.3109</td>\n",
       "      <td>-0.3491</td>\n",
       "      <td>-0.4741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23813</th>\n",
       "      <td>-0.8598</td>\n",
       "      <td>1.0240</td>\n",
       "      <td>-0.1361</td>\n",
       "      <td>0.7952</td>\n",
       "      <td>-0.3611</td>\n",
       "      <td>-3.0000</td>\n",
       "      <td>-1.2420</td>\n",
       "      <td>0.9146</td>\n",
       "      <td>3.0000</td>\n",
       "      <td>1.2460</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.5552</td>\n",
       "      <td>3.0000</td>\n",
       "      <td>0.9145</td>\n",
       "      <td>0.8314</td>\n",
       "      <td>1.0610</td>\n",
       "      <td>-0.4017</td>\n",
       "      <td>1.5410</td>\n",
       "      <td>0.3633</td>\n",
       "      <td>-3.0000</td>\n",
       "      <td>2.2190</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>23814 rows × 772 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          g-0     g-1     g-2     g-3     g-4     g-5     g-6     g-7     g-8  \\\n",
       "0      1.0620  0.5577 -0.2479 -0.6208 -0.1944 -1.0120 -1.0220 -0.0326  0.5548   \n",
       "1      0.0743  0.4087  0.2991  0.0604  1.0190  0.5207  0.2341  0.3372 -0.4047   \n",
       "2      0.6280  0.5817  1.5540 -0.0764 -0.0323  1.2390  0.1715  0.2155  0.0065   \n",
       "3     -0.5138 -0.2491 -0.2656  0.5288  3.0000 -0.8095 -1.9590  0.1792 -0.1321   \n",
       "4     -0.3254 -0.4009  0.9700  0.6919  1.4180 -0.8244 -0.2800 -0.1498 -0.8789   \n",
       "...       ...     ...     ...     ...     ...     ...     ...     ...     ...   \n",
       "23809  0.1394 -0.0636 -0.1112 -0.5080 -0.4713  0.7201  0.5773  0.3055 -0.4726   \n",
       "23810 -1.3260  0.3478 -0.3743  0.9905 -0.7178  0.6621 -0.2252 -0.5565  0.5112   \n",
       "23811  0.3942  0.3756  0.3109 -0.7389  0.5505 -0.0159 -0.2541  0.1745 -0.0340   \n",
       "23812  0.6660  0.2324  0.4392  0.2044  0.8531 -0.0343  0.0323  0.0463  0.4299   \n",
       "23813 -0.8598  1.0240 -0.1361  0.7952 -0.3611 -3.0000 -1.2420  0.9146  3.0000   \n",
       "\n",
       "          g-9  ...   g-762   g-763   g-764   g-765   g-766   g-767   g-768  \\\n",
       "0     -0.0921  ... -0.5055 -0.3167  1.0930  0.0084  0.8611 -0.5582  0.3008   \n",
       "1      0.8507  ... -0.5338  0.0224 -0.4831  0.2128 -0.6999 -0.1214 -0.1626   \n",
       "2      1.2300  ...  2.5770  0.2356  1.3230 -1.3730 -0.2682  0.8427  0.5797   \n",
       "3     -1.0600  ... -0.1292  3.0000  1.2720 -0.4733 -2.0560  0.5699  0.1996   \n",
       "4      0.8630  ... -0.6904  2.0540 -0.3131 -0.0809  0.3910  1.7660 -1.0020   \n",
       "...       ...  ...     ...     ...     ...     ...     ...     ...     ...   \n",
       "23809  0.1269  ...  0.7790  0.5393  0.4112 -0.5059  0.0240 -0.2297  0.7221   \n",
       "23810  0.6727  ... -0.0858  0.3606 -0.0248  0.0672 -0.5901 -0.1022  0.5247   \n",
       "23811  0.4865  ...  0.1796  0.3488  0.0927  0.5166 -0.3099 -0.5946  0.9778   \n",
       "23812 -0.7985  ... -0.1410  1.9590  0.8224  1.2500 -3.0000 -2.8720  0.1794   \n",
       "23813  1.2460  ... -0.5552  3.0000  0.9145  0.8314  1.0610 -0.4017  1.5410   \n",
       "\n",
       "        g-769   g-770   g-771  \n",
       "0      1.6490  0.2968 -0.0224  \n",
       "1     -0.3340 -0.3289 -0.2718  \n",
       "2      0.3143  0.8133  0.7923  \n",
       "3      0.4374  0.1588 -0.0343  \n",
       "4     -0.7534  0.5000 -0.6269  \n",
       "...       ...     ...     ...  \n",
       "23809  0.5099 -0.1423  0.3806  \n",
       "23810  0.5438 -0.1875 -0.4751  \n",
       "23811  0.2326 -0.6191  0.3603  \n",
       "23812  0.3109 -0.3491 -0.4741  \n",
       "23813  0.3633 -3.0000  2.2190  \n",
       "\n",
       "[23814 rows x 772 columns]"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#pc for gene\n",
    "\n",
    "train_g=train.loc[:, GENES]\n",
    "x = StandardScaler().fit_transform(train_g)\n",
    "pca_g = PCA(n_components=500)\n",
    "principalComponents = pca_g.fit_transform(x)\n",
    "principalDf_g = pd.DataFrame(data = principalComponents)\n",
    "\n",
    "plt.plot(np.cumsum(pca_g.explained_variance_ratio_))\n",
    "plt.xlabel('number of components')\n",
    "plt.ylabel('cumulative explained variance')\n",
    "plt.axhline(y=0.80, linestyle='-')\n",
    "plt.title('Cumulative Explained Variance for Gene Expression Variable PCAs')\n",
    "train_g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For now, I will choose 0.8 -- that gives me 240 principle components for genes and 26 pc for cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cp_type</th>\n",
       "      <th>cp_time</th>\n",
       "      <th>cp_dose</th>\n",
       "      <th>pca_c1</th>\n",
       "      <th>pca_c2</th>\n",
       "      <th>pca_c3</th>\n",
       "      <th>pca_c4</th>\n",
       "      <th>pca_c5</th>\n",
       "      <th>pca_c6</th>\n",
       "      <th>pca_c7</th>\n",
       "      <th>...</th>\n",
       "      <th>pca_g231</th>\n",
       "      <th>pca_g232</th>\n",
       "      <th>pca_g233</th>\n",
       "      <th>pca_g234</th>\n",
       "      <th>pca_g235</th>\n",
       "      <th>pca_g236</th>\n",
       "      <th>pca_g237</th>\n",
       "      <th>pca_g238</th>\n",
       "      <th>pca_g239</th>\n",
       "      <th>pca_g240</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-4.817148</td>\n",
       "      <td>1.062650</td>\n",
       "      <td>-0.589212</td>\n",
       "      <td>-0.570276</td>\n",
       "      <td>-1.099414</td>\n",
       "      <td>-0.570338</td>\n",
       "      <td>-0.448394</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.156668</td>\n",
       "      <td>0.153936</td>\n",
       "      <td>-0.875266</td>\n",
       "      <td>0.081307</td>\n",
       "      <td>-0.178192</td>\n",
       "      <td>-0.042645</td>\n",
       "      <td>-0.081751</td>\n",
       "      <td>0.807065</td>\n",
       "      <td>-1.566639</td>\n",
       "      <td>0.412540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>-4.997670</td>\n",
       "      <td>-0.215591</td>\n",
       "      <td>-0.168695</td>\n",
       "      <td>0.231755</td>\n",
       "      <td>-0.276522</td>\n",
       "      <td>-0.077628</td>\n",
       "      <td>-0.294816</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.240866</td>\n",
       "      <td>-0.460979</td>\n",
       "      <td>-0.013192</td>\n",
       "      <td>-0.119110</td>\n",
       "      <td>1.221714</td>\n",
       "      <td>0.193133</td>\n",
       "      <td>-0.155706</td>\n",
       "      <td>0.008112</td>\n",
       "      <td>0.933947</td>\n",
       "      <td>0.158981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.045162</td>\n",
       "      <td>0.530559</td>\n",
       "      <td>0.410562</td>\n",
       "      <td>-0.034347</td>\n",
       "      <td>0.163674</td>\n",
       "      <td>-0.010198</td>\n",
       "      <td>-0.416474</td>\n",
       "      <td>...</td>\n",
       "      <td>0.691793</td>\n",
       "      <td>0.189958</td>\n",
       "      <td>-0.180463</td>\n",
       "      <td>-0.992689</td>\n",
       "      <td>-0.514208</td>\n",
       "      <td>-0.204386</td>\n",
       "      <td>-1.370999</td>\n",
       "      <td>1.607112</td>\n",
       "      <td>0.137311</td>\n",
       "      <td>-0.586660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>14.455510</td>\n",
       "      <td>5.272363</td>\n",
       "      <td>-1.233810</td>\n",
       "      <td>3.433820</td>\n",
       "      <td>-2.366938</td>\n",
       "      <td>0.642758</td>\n",
       "      <td>0.392127</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.225454</td>\n",
       "      <td>1.306040</td>\n",
       "      <td>-0.488189</td>\n",
       "      <td>-0.838830</td>\n",
       "      <td>-0.248623</td>\n",
       "      <td>0.781310</td>\n",
       "      <td>0.275013</td>\n",
       "      <td>-0.518686</td>\n",
       "      <td>0.514427</td>\n",
       "      <td>0.627091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>-3.931571</td>\n",
       "      <td>0.595640</td>\n",
       "      <td>0.260620</td>\n",
       "      <td>-0.506539</td>\n",
       "      <td>0.115986</td>\n",
       "      <td>0.159052</td>\n",
       "      <td>0.167910</td>\n",
       "      <td>...</td>\n",
       "      <td>0.095538</td>\n",
       "      <td>1.407346</td>\n",
       "      <td>-0.321637</td>\n",
       "      <td>-0.204179</td>\n",
       "      <td>-0.321001</td>\n",
       "      <td>0.845610</td>\n",
       "      <td>-0.077394</td>\n",
       "      <td>0.097701</td>\n",
       "      <td>0.451243</td>\n",
       "      <td>0.301276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23809</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-3.921035</td>\n",
       "      <td>-0.099208</td>\n",
       "      <td>-0.464840</td>\n",
       "      <td>1.018396</td>\n",
       "      <td>0.434130</td>\n",
       "      <td>0.141394</td>\n",
       "      <td>-0.907961</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.619300</td>\n",
       "      <td>-0.528104</td>\n",
       "      <td>1.138083</td>\n",
       "      <td>-0.477547</td>\n",
       "      <td>-1.087960</td>\n",
       "      <td>1.800052</td>\n",
       "      <td>1.438954</td>\n",
       "      <td>0.302896</td>\n",
       "      <td>-0.378067</td>\n",
       "      <td>-0.197138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23810</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.648615</td>\n",
       "      <td>-0.029944</td>\n",
       "      <td>-0.502991</td>\n",
       "      <td>-2.045705</td>\n",
       "      <td>0.528601</td>\n",
       "      <td>0.665352</td>\n",
       "      <td>-0.262010</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.861024</td>\n",
       "      <td>0.570328</td>\n",
       "      <td>0.776266</td>\n",
       "      <td>0.792949</td>\n",
       "      <td>-0.407870</td>\n",
       "      <td>-0.733214</td>\n",
       "      <td>1.834332</td>\n",
       "      <td>-0.078342</td>\n",
       "      <td>0.360588</td>\n",
       "      <td>-0.023159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23811</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-5.895690</td>\n",
       "      <td>0.718064</td>\n",
       "      <td>-0.828327</td>\n",
       "      <td>0.271867</td>\n",
       "      <td>-0.205808</td>\n",
       "      <td>-0.421497</td>\n",
       "      <td>0.021284</td>\n",
       "      <td>...</td>\n",
       "      <td>1.199434</td>\n",
       "      <td>0.028999</td>\n",
       "      <td>0.798122</td>\n",
       "      <td>-0.853362</td>\n",
       "      <td>-0.708087</td>\n",
       "      <td>-0.158663</td>\n",
       "      <td>-0.728970</td>\n",
       "      <td>-0.915131</td>\n",
       "      <td>0.559975</td>\n",
       "      <td>-0.358601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23812</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-5.389047</td>\n",
       "      <td>0.856107</td>\n",
       "      <td>-0.875155</td>\n",
       "      <td>-0.401162</td>\n",
       "      <td>0.945486</td>\n",
       "      <td>-0.981073</td>\n",
       "      <td>1.119168</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.150939</td>\n",
       "      <td>-0.342677</td>\n",
       "      <td>-1.001548</td>\n",
       "      <td>-0.291744</td>\n",
       "      <td>-0.709145</td>\n",
       "      <td>0.706372</td>\n",
       "      <td>-0.117069</td>\n",
       "      <td>-0.239175</td>\n",
       "      <td>-0.918925</td>\n",
       "      <td>-0.063217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23813</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>20.052371</td>\n",
       "      <td>1.379889</td>\n",
       "      <td>1.238010</td>\n",
       "      <td>0.005108</td>\n",
       "      <td>-0.798199</td>\n",
       "      <td>1.386231</td>\n",
       "      <td>0.397383</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.855996</td>\n",
       "      <td>0.329484</td>\n",
       "      <td>0.173500</td>\n",
       "      <td>0.856650</td>\n",
       "      <td>1.504512</td>\n",
       "      <td>-0.436691</td>\n",
       "      <td>-0.334252</td>\n",
       "      <td>-2.174778</td>\n",
       "      <td>2.082244</td>\n",
       "      <td>-1.600195</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>23814 rows × 269 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       cp_type  cp_time  cp_dose     pca_c1    pca_c2    pca_c3    pca_c4  \\\n",
       "0            0        0        0  -4.817148  1.062650 -0.589212 -0.570276   \n",
       "1            0        2        0  -4.997670 -0.215591 -0.168695  0.231755   \n",
       "2            0        1        0   0.045162  0.530559  0.410562 -0.034347   \n",
       "3            0        1        0  14.455510  5.272363 -1.233810  3.433820   \n",
       "4            0        2        1  -3.931571  0.595640  0.260620 -0.506539   \n",
       "...        ...      ...      ...        ...       ...       ...       ...   \n",
       "23809        0        0        1  -3.921035 -0.099208 -0.464840  1.018396   \n",
       "23810        0        0        1  -1.648615 -0.029944 -0.502991 -2.045705   \n",
       "23811        1        1        1  -5.895690  0.718064 -0.828327  0.271867   \n",
       "23812        0        0        0  -5.389047  0.856107 -0.875155 -0.401162   \n",
       "23813        0        2        0  20.052371  1.379889  1.238010  0.005108   \n",
       "\n",
       "         pca_c5    pca_c6    pca_c7  ...  pca_g231  pca_g232  pca_g233  \\\n",
       "0     -1.099414 -0.570338 -0.448394  ... -0.156668  0.153936 -0.875266   \n",
       "1     -0.276522 -0.077628 -0.294816  ... -0.240866 -0.460979 -0.013192   \n",
       "2      0.163674 -0.010198 -0.416474  ...  0.691793  0.189958 -0.180463   \n",
       "3     -2.366938  0.642758  0.392127  ... -1.225454  1.306040 -0.488189   \n",
       "4      0.115986  0.159052  0.167910  ...  0.095538  1.407346 -0.321637   \n",
       "...         ...       ...       ...  ...       ...       ...       ...   \n",
       "23809  0.434130  0.141394 -0.907961  ... -0.619300 -0.528104  1.138083   \n",
       "23810  0.528601  0.665352 -0.262010  ... -0.861024  0.570328  0.776266   \n",
       "23811 -0.205808 -0.421497  0.021284  ...  1.199434  0.028999  0.798122   \n",
       "23812  0.945486 -0.981073  1.119168  ... -0.150939 -0.342677 -1.001548   \n",
       "23813 -0.798199  1.386231  0.397383  ... -0.855996  0.329484  0.173500   \n",
       "\n",
       "       pca_g234  pca_g235  pca_g236  pca_g237  pca_g238  pca_g239  pca_g240  \n",
       "0      0.081307 -0.178192 -0.042645 -0.081751  0.807065 -1.566639  0.412540  \n",
       "1     -0.119110  1.221714  0.193133 -0.155706  0.008112  0.933947  0.158981  \n",
       "2     -0.992689 -0.514208 -0.204386 -1.370999  1.607112  0.137311 -0.586660  \n",
       "3     -0.838830 -0.248623  0.781310  0.275013 -0.518686  0.514427  0.627091  \n",
       "4     -0.204179 -0.321001  0.845610 -0.077394  0.097701  0.451243  0.301276  \n",
       "...         ...       ...       ...       ...       ...       ...       ...  \n",
       "23809 -0.477547 -1.087960  1.800052  1.438954  0.302896 -0.378067 -0.197138  \n",
       "23810  0.792949 -0.407870 -0.733214  1.834332 -0.078342  0.360588 -0.023159  \n",
       "23811 -0.853362 -0.708087 -0.158663 -0.728970 -0.915131  0.559975 -0.358601  \n",
       "23812 -0.291744 -0.709145  0.706372 -0.117069 -0.239175 -0.918925 -0.063217  \n",
       "23813  0.856650  1.504512 -0.436691 -0.334252 -2.174778  2.082244 -1.600195  \n",
       "\n",
       "[23814 rows x 269 columns]"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#this part was inspried by one of the notebooks\n",
    "pca_c=principalDf_c.iloc[:,:26]\n",
    "names=[]\n",
    "for i in range(1,27):\n",
    "    var='pca_c'+str(i)\n",
    "    names.append(var)\n",
    "pca_c.columns=names\n",
    "\n",
    "pca_g=principalDf_g.iloc[:,:240]\n",
    "names=[]\n",
    "for i in range(1,241):\n",
    "    var='pca_g'+str(i)\n",
    "    names.append(var)\n",
    "pca_g.columns=names\n",
    "\n",
    "\n",
    "pca_cg=pd.merge(pca_c, pca_g, left_index=True, right_index=True)\n",
    "train_pca = pd.merge(train.iloc[:,:3], pca_cg, left_index=True, right_index=True)\n",
    "train_pca"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOW THAT WE HAVE CREATED OUR PRINCIPLE COMPONENTS, IT'S TIME TO START MODEL FITTING\n",
    "I WILL FIRST TRY LOGISTIC REGRESSION, THE MOST BASIC CLASSIFER. \n",
    "NOTE THAT LOGISTIC REGRESSION ONLY SUPPORTS BINARY CLASSIFICATION AS DEFAULT, BUT WE CAN SOLVE THAT EASILY BY USING  MultiOutputClassifier wrapper in sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cp_time</th>\n",
       "      <th>cp_dose</th>\n",
       "      <th>g-0</th>\n",
       "      <th>g-1</th>\n",
       "      <th>g-2</th>\n",
       "      <th>g-3</th>\n",
       "      <th>g-4</th>\n",
       "      <th>g-5</th>\n",
       "      <th>g-6</th>\n",
       "      <th>g-7</th>\n",
       "      <th>...</th>\n",
       "      <th>c-90</th>\n",
       "      <th>c-91</th>\n",
       "      <th>c-92</th>\n",
       "      <th>c-93</th>\n",
       "      <th>c-94</th>\n",
       "      <th>c-95</th>\n",
       "      <th>c-96</th>\n",
       "      <th>c-97</th>\n",
       "      <th>c-98</th>\n",
       "      <th>c-99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0620</td>\n",
       "      <td>0.5577</td>\n",
       "      <td>-0.2479</td>\n",
       "      <td>-0.6208</td>\n",
       "      <td>-0.1944</td>\n",
       "      <td>-1.0120</td>\n",
       "      <td>-1.0220</td>\n",
       "      <td>-0.0326</td>\n",
       "      <td>...</td>\n",
       "      <td>0.2862</td>\n",
       "      <td>0.2584</td>\n",
       "      <td>0.8076</td>\n",
       "      <td>0.5523</td>\n",
       "      <td>-0.1912</td>\n",
       "      <td>0.6584</td>\n",
       "      <td>-0.3981</td>\n",
       "      <td>0.2139</td>\n",
       "      <td>0.3801</td>\n",
       "      <td>0.4176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0743</td>\n",
       "      <td>0.4087</td>\n",
       "      <td>0.2991</td>\n",
       "      <td>0.0604</td>\n",
       "      <td>1.0190</td>\n",
       "      <td>0.5207</td>\n",
       "      <td>0.2341</td>\n",
       "      <td>0.3372</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.4265</td>\n",
       "      <td>0.7543</td>\n",
       "      <td>0.4708</td>\n",
       "      <td>0.0230</td>\n",
       "      <td>0.2957</td>\n",
       "      <td>0.4899</td>\n",
       "      <td>0.1522</td>\n",
       "      <td>0.1241</td>\n",
       "      <td>0.6077</td>\n",
       "      <td>0.7371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.6280</td>\n",
       "      <td>0.5817</td>\n",
       "      <td>1.5540</td>\n",
       "      <td>-0.0764</td>\n",
       "      <td>-0.0323</td>\n",
       "      <td>1.2390</td>\n",
       "      <td>0.1715</td>\n",
       "      <td>0.2155</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.7250</td>\n",
       "      <td>-0.6297</td>\n",
       "      <td>0.6103</td>\n",
       "      <td>0.0223</td>\n",
       "      <td>-1.3240</td>\n",
       "      <td>-0.3174</td>\n",
       "      <td>-0.6417</td>\n",
       "      <td>-0.2187</td>\n",
       "      <td>-1.4080</td>\n",
       "      <td>0.6931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.5138</td>\n",
       "      <td>-0.2491</td>\n",
       "      <td>-0.2656</td>\n",
       "      <td>0.5288</td>\n",
       "      <td>3.0000</td>\n",
       "      <td>-0.8095</td>\n",
       "      <td>-1.9590</td>\n",
       "      <td>0.1792</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.0990</td>\n",
       "      <td>-0.6441</td>\n",
       "      <td>-3.0000</td>\n",
       "      <td>-1.3780</td>\n",
       "      <td>-0.8632</td>\n",
       "      <td>-1.2880</td>\n",
       "      <td>-1.6210</td>\n",
       "      <td>-0.8784</td>\n",
       "      <td>-0.3876</td>\n",
       "      <td>-0.8154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.3254</td>\n",
       "      <td>-0.4009</td>\n",
       "      <td>0.9700</td>\n",
       "      <td>0.6919</td>\n",
       "      <td>1.4180</td>\n",
       "      <td>-0.8244</td>\n",
       "      <td>-0.2800</td>\n",
       "      <td>-0.1498</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0042</td>\n",
       "      <td>0.0048</td>\n",
       "      <td>0.6670</td>\n",
       "      <td>1.0690</td>\n",
       "      <td>0.5523</td>\n",
       "      <td>-0.3031</td>\n",
       "      <td>0.1094</td>\n",
       "      <td>0.2885</td>\n",
       "      <td>-0.3786</td>\n",
       "      <td>0.7125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23809</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1394</td>\n",
       "      <td>-0.0636</td>\n",
       "      <td>-0.1112</td>\n",
       "      <td>-0.5080</td>\n",
       "      <td>-0.4713</td>\n",
       "      <td>0.7201</td>\n",
       "      <td>0.5773</td>\n",
       "      <td>0.3055</td>\n",
       "      <td>...</td>\n",
       "      <td>0.1969</td>\n",
       "      <td>0.0262</td>\n",
       "      <td>-0.8121</td>\n",
       "      <td>0.3434</td>\n",
       "      <td>0.5372</td>\n",
       "      <td>-0.3246</td>\n",
       "      <td>0.0631</td>\n",
       "      <td>0.9171</td>\n",
       "      <td>0.5258</td>\n",
       "      <td>0.4680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23810</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.3260</td>\n",
       "      <td>0.3478</td>\n",
       "      <td>-0.3743</td>\n",
       "      <td>0.9905</td>\n",
       "      <td>-0.7178</td>\n",
       "      <td>0.6621</td>\n",
       "      <td>-0.2252</td>\n",
       "      <td>-0.5565</td>\n",
       "      <td>...</td>\n",
       "      <td>0.4286</td>\n",
       "      <td>0.4426</td>\n",
       "      <td>0.0423</td>\n",
       "      <td>-0.3195</td>\n",
       "      <td>-0.8086</td>\n",
       "      <td>-0.9798</td>\n",
       "      <td>-0.2084</td>\n",
       "      <td>-0.1224</td>\n",
       "      <td>-0.2715</td>\n",
       "      <td>0.3689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23811</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.3942</td>\n",
       "      <td>0.3756</td>\n",
       "      <td>0.3109</td>\n",
       "      <td>-0.7389</td>\n",
       "      <td>0.5505</td>\n",
       "      <td>-0.0159</td>\n",
       "      <td>-0.2541</td>\n",
       "      <td>0.1745</td>\n",
       "      <td>...</td>\n",
       "      <td>0.5409</td>\n",
       "      <td>0.3755</td>\n",
       "      <td>0.7343</td>\n",
       "      <td>0.2807</td>\n",
       "      <td>0.4116</td>\n",
       "      <td>0.6422</td>\n",
       "      <td>0.2256</td>\n",
       "      <td>0.7592</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.3808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23812</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.6660</td>\n",
       "      <td>0.2324</td>\n",
       "      <td>0.4392</td>\n",
       "      <td>0.2044</td>\n",
       "      <td>0.8531</td>\n",
       "      <td>-0.0343</td>\n",
       "      <td>0.0323</td>\n",
       "      <td>0.0463</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.1105</td>\n",
       "      <td>0.4258</td>\n",
       "      <td>-0.2012</td>\n",
       "      <td>0.1506</td>\n",
       "      <td>1.5230</td>\n",
       "      <td>0.7101</td>\n",
       "      <td>0.1732</td>\n",
       "      <td>0.7015</td>\n",
       "      <td>-0.6290</td>\n",
       "      <td>0.0740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23813</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.8598</td>\n",
       "      <td>1.0240</td>\n",
       "      <td>-0.1361</td>\n",
       "      <td>0.7952</td>\n",
       "      <td>-0.3611</td>\n",
       "      <td>-3.0000</td>\n",
       "      <td>-1.2420</td>\n",
       "      <td>0.9146</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.0000</td>\n",
       "      <td>-1.7450</td>\n",
       "      <td>-3.0000</td>\n",
       "      <td>-3.0000</td>\n",
       "      <td>-3.0000</td>\n",
       "      <td>-1.4160</td>\n",
       "      <td>-3.0000</td>\n",
       "      <td>-0.4775</td>\n",
       "      <td>-2.1500</td>\n",
       "      <td>-3.0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>23814 rows × 874 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       cp_time  cp_dose     g-0     g-1     g-2     g-3     g-4     g-5  \\\n",
       "0            0        0  1.0620  0.5577 -0.2479 -0.6208 -0.1944 -1.0120   \n",
       "1            2        0  0.0743  0.4087  0.2991  0.0604  1.0190  0.5207   \n",
       "2            1        0  0.6280  0.5817  1.5540 -0.0764 -0.0323  1.2390   \n",
       "3            1        0 -0.5138 -0.2491 -0.2656  0.5288  3.0000 -0.8095   \n",
       "4            2        1 -0.3254 -0.4009  0.9700  0.6919  1.4180 -0.8244   \n",
       "...        ...      ...     ...     ...     ...     ...     ...     ...   \n",
       "23809        0        1  0.1394 -0.0636 -0.1112 -0.5080 -0.4713  0.7201   \n",
       "23810        0        1 -1.3260  0.3478 -0.3743  0.9905 -0.7178  0.6621   \n",
       "23811        1        1  0.3942  0.3756  0.3109 -0.7389  0.5505 -0.0159   \n",
       "23812        0        0  0.6660  0.2324  0.4392  0.2044  0.8531 -0.0343   \n",
       "23813        2        0 -0.8598  1.0240 -0.1361  0.7952 -0.3611 -3.0000   \n",
       "\n",
       "          g-6     g-7  ...    c-90    c-91    c-92    c-93    c-94    c-95  \\\n",
       "0     -1.0220 -0.0326  ...  0.2862  0.2584  0.8076  0.5523 -0.1912  0.6584   \n",
       "1      0.2341  0.3372  ... -0.4265  0.7543  0.4708  0.0230  0.2957  0.4899   \n",
       "2      0.1715  0.2155  ... -0.7250 -0.6297  0.6103  0.0223 -1.3240 -0.3174   \n",
       "3     -1.9590  0.1792  ... -2.0990 -0.6441 -3.0000 -1.3780 -0.8632 -1.2880   \n",
       "4     -0.2800 -0.1498  ...  0.0042  0.0048  0.6670  1.0690  0.5523 -0.3031   \n",
       "...       ...     ...  ...     ...     ...     ...     ...     ...     ...   \n",
       "23809  0.5773  0.3055  ...  0.1969  0.0262 -0.8121  0.3434  0.5372 -0.3246   \n",
       "23810 -0.2252 -0.5565  ...  0.4286  0.4426  0.0423 -0.3195 -0.8086 -0.9798   \n",
       "23811 -0.2541  0.1745  ...  0.5409  0.3755  0.7343  0.2807  0.4116  0.6422   \n",
       "23812  0.0323  0.0463  ... -0.1105  0.4258 -0.2012  0.1506  1.5230  0.7101   \n",
       "23813 -1.2420  0.9146  ... -3.0000 -1.7450 -3.0000 -3.0000 -3.0000 -1.4160   \n",
       "\n",
       "         c-96    c-97    c-98    c-99  \n",
       "0     -0.3981  0.2139  0.3801  0.4176  \n",
       "1      0.1522  0.1241  0.6077  0.7371  \n",
       "2     -0.6417 -0.2187 -1.4080  0.6931  \n",
       "3     -1.6210 -0.8784 -0.3876 -0.8154  \n",
       "4      0.1094  0.2885 -0.3786  0.7125  \n",
       "...       ...     ...     ...     ...  \n",
       "23809  0.0631  0.9171  0.5258  0.4680  \n",
       "23810 -0.2084 -0.1224 -0.2715  0.3689  \n",
       "23811  0.2256  0.7592  0.6656  0.3808  \n",
       "23812  0.1732  0.7015 -0.6290  0.0740  \n",
       "23813 -3.0000 -0.4775 -2.1500 -3.0000  \n",
       "\n",
       "[23814 rows x 874 columns]"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#还没用\n",
    "# Function to extract common stats features\n",
    "def fe_stats(train, test):\n",
    "    \n",
    "    features_g = list(train.columns[4:776])\n",
    "    features_c = list(train.columns[776:876])\n",
    "    \n",
    "    for df in [train, test]:\n",
    "        df['g_sum'] = df[features_g].sum(axis = 1)\n",
    "        df['g_mean'] = df[features_g].mean(axis = 1)\n",
    "        df['g_std'] = df[features_g].std(axis = 1)\n",
    "        df['g_kurt'] = df[features_g].kurtosis(axis = 1)\n",
    "        df['g_skew'] = df[features_g].skew(axis = 1)\n",
    "        df['c_sum'] = df[features_c].sum(axis = 1)\n",
    "        df['c_mean'] = df[features_c].mean(axis = 1)\n",
    "        df['c_std'] = df[features_c].std(axis = 1)\n",
    "        df['c_kurt'] = df[features_c].kurtosis(axis = 1)\n",
    "        df['c_skew'] = df[features_c].skew(axis = 1)\n",
    "        df['gc_sum'] = df[features_g + features_c].sum(axis = 1)\n",
    "        df['gc_mean'] = df[features_g + features_c].mean(axis = 1)\n",
    "        df['gc_std'] = df[features_g + features_c].std(axis = 1)\n",
    "        df['gc_kurt'] = df[features_g + features_c].kurtosis(axis = 1)\n",
    "        df['gc_skew'] = df[features_g + features_c].skew(axis = 1)\n",
    "        \n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2:1\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    train_pca, train_targets_scored.iloc[:,1:], test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An intuitive approach to solving multi-label problem is to decompose it into multiple independent binary classification problems \n",
    "In an “one-to-rest” strategy, one could build multiple independent classifiers and, for an unseen instance, choose the class for which the confidence is maximized.\n",
    "The main assumption here is that the labels are mutually exclusive. You do not consider any underlying correlation between the classes in this method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "multilabel-indicator is not supported",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-208-d6a4c0820144>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mknn_predictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     71\u001b[0m                           FutureWarning)\n\u001b[1;32m     72\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36mconfusion_matrix\u001b[0;34m(y_true, y_pred, labels, sample_weight, normalize)\u001b[0m\n\u001b[1;32m    276\u001b[0m     \u001b[0my_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my_type\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"binary\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"multiclass\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 278\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%s is not supported\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0my_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    279\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    280\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: multilabel-indicator is not supported"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn_classifier = KNeighborsClassifier()\n",
    "knn_classifier.fit(X_train, y_train)\n",
    "knn_predictions = knn_classifier.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[7855,    0],\n",
       "        [   4,    0]],\n",
       "\n",
       "       [[7853,    0],\n",
       "        [   6,    0]],\n",
       "\n",
       "       [[7854,    0],\n",
       "        [   5,    0]],\n",
       "\n",
       "       [[7792,    0],\n",
       "        [  67,    0]],\n",
       "\n",
       "       [[7758,    1],\n",
       "        [ 100,    0]],\n",
       "\n",
       "       [[7833,    0],\n",
       "        [  26,    0]],\n",
       "\n",
       "       [[7845,    0],\n",
       "        [  14,    0]],\n",
       "\n",
       "       [[7825,    0],\n",
       "        [  34,    0]],\n",
       "\n",
       "       [[7858,    0],\n",
       "        [   1,    0]],\n",
       "\n",
       "       [[7766,    1],\n",
       "        [  92,    0]],\n",
       "\n",
       "       [[7741,    0],\n",
       "        [ 118,    0]],\n",
       "\n",
       "       [[7834,    0],\n",
       "        [  24,    1]],\n",
       "\n",
       "       [[7856,    0],\n",
       "        [   2,    1]],\n",
       "\n",
       "       [[7850,    0],\n",
       "        [   9,    0]],\n",
       "\n",
       "       [[7854,    0],\n",
       "        [   5,    0]],\n",
       "\n",
       "       [[7851,    0],\n",
       "        [   8,    0]],\n",
       "\n",
       "       [[7841,    0],\n",
       "        [  18,    0]],\n",
       "\n",
       "       [[7825,    0],\n",
       "        [  34,    0]],\n",
       "\n",
       "       [[7825,    0],\n",
       "        [  34,    0]],\n",
       "\n",
       "       [[7845,    0],\n",
       "        [  14,    0]],\n",
       "\n",
       "       [[7846,    0],\n",
       "        [  13,    0]],\n",
       "\n",
       "       [[7834,    0],\n",
       "        [  25,    0]],\n",
       "\n",
       "       [[7857,    0],\n",
       "        [   2,    0]],\n",
       "\n",
       "       [[7844,    0],\n",
       "        [  15,    0]],\n",
       "\n",
       "       [[7853,    0],\n",
       "        [   6,    0]],\n",
       "\n",
       "       [[7857,    0],\n",
       "        [   2,    0]],\n",
       "\n",
       "       [[7854,    0],\n",
       "        [   5,    0]],\n",
       "\n",
       "       [[7853,    0],\n",
       "        [   6,    0]],\n",
       "\n",
       "       [[7827,    0],\n",
       "        [  32,    0]],\n",
       "\n",
       "       [[7847,    0],\n",
       "        [  12,    0]],\n",
       "\n",
       "       [[7853,    0],\n",
       "        [   6,    0]],\n",
       "\n",
       "       [[7846,    0],\n",
       "        [  13,    0]],\n",
       "\n",
       "       [[7839,    0],\n",
       "        [  20,    0]],\n",
       "\n",
       "       [[7857,    0],\n",
       "        [   2,    0]],\n",
       "\n",
       "       [[7858,    0],\n",
       "        [   1,    0]],\n",
       "\n",
       "       [[7852,    0],\n",
       "        [   7,    0]],\n",
       "\n",
       "       [[7824,    3],\n",
       "        [  27,    5]],\n",
       "\n",
       "       [[7850,    0],\n",
       "        [   8,    1]],\n",
       "\n",
       "       [[7824,    0],\n",
       "        [  31,    4]],\n",
       "\n",
       "       [[7857,    0],\n",
       "        [   2,    0]],\n",
       "\n",
       "       [[7845,    0],\n",
       "        [  14,    0]],\n",
       "\n",
       "       [[7837,    0],\n",
       "        [  22,    0]],\n",
       "\n",
       "       [[7848,    0],\n",
       "        [  11,    0]],\n",
       "\n",
       "       [[7790,    0],\n",
       "        [  69,    0]],\n",
       "\n",
       "       [[7825,    0],\n",
       "        [  34,    0]],\n",
       "\n",
       "       [[7819,    0],\n",
       "        [  40,    0]],\n",
       "\n",
       "       [[7856,    0],\n",
       "        [   3,    0]],\n",
       "\n",
       "       [[7848,    0],\n",
       "        [  11,    0]],\n",
       "\n",
       "       [[7849,    0],\n",
       "        [  10,    0]],\n",
       "\n",
       "       [[7839,    0],\n",
       "        [  20,    0]],\n",
       "\n",
       "       [[7852,    0],\n",
       "        [   7,    0]],\n",
       "\n",
       "       [[7836,    3],\n",
       "        [  14,    6]],\n",
       "\n",
       "       [[7851,    0],\n",
       "        [   8,    0]],\n",
       "\n",
       "       [[7858,    0],\n",
       "        [   1,    0]],\n",
       "\n",
       "       [[7772,    0],\n",
       "        [  87,    0]],\n",
       "\n",
       "       [[7841,    0],\n",
       "        [  18,    0]],\n",
       "\n",
       "       [[7836,    0],\n",
       "        [  23,    0]],\n",
       "\n",
       "       [[7846,    0],\n",
       "        [  13,    0]],\n",
       "\n",
       "       [[7851,    0],\n",
       "        [   8,    0]],\n",
       "\n",
       "       [[7853,    0],\n",
       "        [   6,    0]],\n",
       "\n",
       "       [[7855,    0],\n",
       "        [   4,    0]],\n",
       "\n",
       "       [[7833,    0],\n",
       "        [  26,    0]],\n",
       "\n",
       "       [[7852,    0],\n",
       "        [   7,    0]],\n",
       "\n",
       "       [[7724,    4],\n",
       "        [  42,   89]],\n",
       "\n",
       "       [[7841,    0],\n",
       "        [  18,    0]],\n",
       "\n",
       "       [[7852,    0],\n",
       "        [   6,    1]],\n",
       "\n",
       "       [[7846,    0],\n",
       "        [  13,    0]],\n",
       "\n",
       "       [[7840,    0],\n",
       "        [  19,    0]],\n",
       "\n",
       "       [[7844,    0],\n",
       "        [  15,    0]],\n",
       "\n",
       "       [[7856,    0],\n",
       "        [   3,    0]],\n",
       "\n",
       "       [[7841,    3],\n",
       "        [  14,    1]],\n",
       "\n",
       "       [[7727,    2],\n",
       "        [ 130,    0]],\n",
       "\n",
       "       [[7815,    2],\n",
       "        [  42,    0]],\n",
       "\n",
       "       [[7850,    0],\n",
       "        [   9,    0]],\n",
       "\n",
       "       [[7855,    0],\n",
       "        [   4,    0]],\n",
       "\n",
       "       [[7855,    0],\n",
       "        [   4,    0]],\n",
       "\n",
       "       [[7844,    0],\n",
       "        [  15,    0]],\n",
       "\n",
       "       [[7727,    2],\n",
       "        [ 128,    2]],\n",
       "\n",
       "       [[7816,    0],\n",
       "        [  43,    0]],\n",
       "\n",
       "       [[7711,    2],\n",
       "        [ 146,    0]],\n",
       "\n",
       "       [[7742,    0],\n",
       "        [  78,   39]],\n",
       "\n",
       "       [[7857,    0],\n",
       "        [   2,    0]],\n",
       "\n",
       "       [[7859,    0],\n",
       "        [   0,    0]],\n",
       "\n",
       "       [[7809,    0],\n",
       "        [  50,    0]],\n",
       "\n",
       "       [[7843,    0],\n",
       "        [  16,    0]],\n",
       "\n",
       "       [[7850,    0],\n",
       "        [   9,    0]],\n",
       "\n",
       "       [[7850,    0],\n",
       "        [   9,    0]],\n",
       "\n",
       "       [[7847,    0],\n",
       "        [  12,    0]],\n",
       "\n",
       "       [[7849,    0],\n",
       "        [  10,    0]],\n",
       "\n",
       "       [[7762,    0],\n",
       "        [  94,    3]],\n",
       "\n",
       "       [[7856,    0],\n",
       "        [   3,    0]],\n",
       "\n",
       "       [[7851,    0],\n",
       "        [   8,    0]],\n",
       "\n",
       "       [[7850,    0],\n",
       "        [   9,    0]],\n",
       "\n",
       "       [[7817,    0],\n",
       "        [  42,    0]],\n",
       "\n",
       "       [[7803,    0],\n",
       "        [  56,    0]],\n",
       "\n",
       "       [[7843,    0],\n",
       "        [  16,    0]],\n",
       "\n",
       "       [[7755,   15],\n",
       "        [  40,   49]],\n",
       "\n",
       "       [[7857,    0],\n",
       "        [   2,    0]],\n",
       "\n",
       "       [[7840,    0],\n",
       "        [  19,    0]],\n",
       "\n",
       "       [[7727,    0],\n",
       "        [ 132,    0]],\n",
       "\n",
       "       [[7858,    0],\n",
       "        [   1,    0]],\n",
       "\n",
       "       [[7834,    0],\n",
       "        [  22,    3]],\n",
       "\n",
       "       [[7840,    0],\n",
       "        [  19,    0]],\n",
       "\n",
       "       [[7824,    0],\n",
       "        [  16,   19]],\n",
       "\n",
       "       [[7840,    0],\n",
       "        [  19,    0]],\n",
       "\n",
       "       [[7772,    1],\n",
       "        [  86,    0]],\n",
       "\n",
       "       [[7848,    0],\n",
       "        [  11,    0]],\n",
       "\n",
       "       [[7852,    0],\n",
       "        [   7,    0]],\n",
       "\n",
       "       [[7839,    0],\n",
       "        [  20,    0]],\n",
       "\n",
       "       [[7769,    2],\n",
       "        [  60,   28]],\n",
       "\n",
       "       [[7822,    0],\n",
       "        [   7,   30]],\n",
       "\n",
       "       [[7849,    0],\n",
       "        [  10,    0]],\n",
       "\n",
       "       [[7850,    0],\n",
       "        [   9,    0]],\n",
       "\n",
       "       [[7845,    0],\n",
       "        [  14,    0]],\n",
       "\n",
       "       [[7837,    0],\n",
       "        [  22,    0]],\n",
       "\n",
       "       [[7847,    0],\n",
       "        [  12,    0]],\n",
       "\n",
       "       [[7843,    2],\n",
       "        [  14,    0]],\n",
       "\n",
       "       [[7847,    0],\n",
       "        [  12,    0]],\n",
       "\n",
       "       [[7829,    0],\n",
       "        [  28,    2]],\n",
       "\n",
       "       [[7763,    0],\n",
       "        [  94,    2]],\n",
       "\n",
       "       [[7857,    0],\n",
       "        [   2,    0]],\n",
       "\n",
       "       [[7856,    0],\n",
       "        [   3,    0]],\n",
       "\n",
       "       [[7835,    0],\n",
       "        [  24,    0]],\n",
       "\n",
       "       [[7855,    0],\n",
       "        [   4,    0]],\n",
       "\n",
       "       [[7838,    0],\n",
       "        [  21,    0]],\n",
       "\n",
       "       [[7857,    0],\n",
       "        [   2,    0]],\n",
       "\n",
       "       [[7852,    0],\n",
       "        [   7,    0]],\n",
       "\n",
       "       [[7834,    2],\n",
       "        [  12,   11]],\n",
       "\n",
       "       [[7839,    1],\n",
       "        [  19,    0]],\n",
       "\n",
       "       [[7851,    0],\n",
       "        [   8,    0]],\n",
       "\n",
       "       [[7855,    0],\n",
       "        [   4,    0]],\n",
       "\n",
       "       [[7827,    0],\n",
       "        [  32,    0]],\n",
       "\n",
       "       [[7854,    0],\n",
       "        [   5,    0]],\n",
       "\n",
       "       [[7809,   10],\n",
       "        [  28,   12]],\n",
       "\n",
       "       [[7842,    0],\n",
       "        [  17,    0]],\n",
       "\n",
       "       [[7846,    0],\n",
       "        [  13,    0]],\n",
       "\n",
       "       [[7586,   19],\n",
       "        [  34,  220]],\n",
       "\n",
       "       [[7858,    0],\n",
       "        [   1,    0]],\n",
       "\n",
       "       [[7851,    0],\n",
       "        [   8,    0]],\n",
       "\n",
       "       [[7856,    0],\n",
       "        [   3,    0]],\n",
       "\n",
       "       [[7844,    0],\n",
       "        [  15,    0]],\n",
       "\n",
       "       [[7857,    0],\n",
       "        [   2,    0]],\n",
       "\n",
       "       [[7854,    0],\n",
       "        [   5,    0]],\n",
       "\n",
       "       [[7838,    0],\n",
       "        [  21,    0]],\n",
       "\n",
       "       [[7833,    0],\n",
       "        [  26,    0]],\n",
       "\n",
       "       [[7843,    2],\n",
       "        [  14,    0]],\n",
       "\n",
       "       [[7840,    0],\n",
       "        [  19,    0]],\n",
       "\n",
       "       [[7851,    0],\n",
       "        [   8,    0]],\n",
       "\n",
       "       [[7835,    0],\n",
       "        [  24,    0]],\n",
       "\n",
       "       [[7760,    0],\n",
       "        [  96,    3]],\n",
       "\n",
       "       [[7853,    0],\n",
       "        [   6,    0]],\n",
       "\n",
       "       [[7766,    0],\n",
       "        [  93,    0]],\n",
       "\n",
       "       [[7853,    0],\n",
       "        [   6,    0]],\n",
       "\n",
       "       [[7808,    3],\n",
       "        [  43,    5]],\n",
       "\n",
       "       [[7846,    0],\n",
       "        [  13,    0]],\n",
       "\n",
       "       [[7840,    0],\n",
       "        [  19,    0]],\n",
       "\n",
       "       [[7827,    0],\n",
       "        [  32,    0]],\n",
       "\n",
       "       [[7833,    0],\n",
       "        [  26,    0]],\n",
       "\n",
       "       [[7851,    0],\n",
       "        [   8,    0]],\n",
       "\n",
       "       [[7818,    0],\n",
       "        [  41,    0]],\n",
       "\n",
       "       [[7856,    0],\n",
       "        [   3,    0]],\n",
       "\n",
       "       [[7847,    0],\n",
       "        [  12,    0]],\n",
       "\n",
       "       [[7829,    0],\n",
       "        [  30,    0]],\n",
       "\n",
       "       [[7620,   16],\n",
       "        [   0,  223]],\n",
       "\n",
       "       [[7840,    0],\n",
       "        [  19,    0]],\n",
       "\n",
       "       [[7858,    0],\n",
       "        [   1,    0]],\n",
       "\n",
       "       [[7823,    2],\n",
       "        [  28,    6]],\n",
       "\n",
       "       [[7852,    0],\n",
       "        [   7,    0]],\n",
       "\n",
       "       [[7836,    0],\n",
       "        [  23,    0]],\n",
       "\n",
       "       [[7786,    0],\n",
       "        [  14,   59]],\n",
       "\n",
       "       [[7855,    0],\n",
       "        [   4,    0]],\n",
       "\n",
       "       [[7831,    0],\n",
       "        [  28,    0]],\n",
       "\n",
       "       [[7858,    0],\n",
       "        [   1,    0]],\n",
       "\n",
       "       [[7849,    0],\n",
       "        [  10,    0]],\n",
       "\n",
       "       [[7841,    0],\n",
       "        [  18,    0]],\n",
       "\n",
       "       [[7849,    0],\n",
       "        [  10,    0]],\n",
       "\n",
       "       [[7775,    0],\n",
       "        [  84,    0]],\n",
       "\n",
       "       [[7721,    4],\n",
       "        [ 134,    0]],\n",
       "\n",
       "       [[7840,    0],\n",
       "        [  19,    0]],\n",
       "\n",
       "       [[7848,    0],\n",
       "        [  11,    0]],\n",
       "\n",
       "       [[7845,    0],\n",
       "        [  14,    0]],\n",
       "\n",
       "       [[7847,    0],\n",
       "        [  12,    0]],\n",
       "\n",
       "       [[7764,    0],\n",
       "        [  95,    0]],\n",
       "\n",
       "       [[7849,    0],\n",
       "        [  10,    0]],\n",
       "\n",
       "       [[7839,    0],\n",
       "        [  18,    2]],\n",
       "\n",
       "       [[7856,    0],\n",
       "        [   3,    0]],\n",
       "\n",
       "       [[7853,    0],\n",
       "        [   6,    0]],\n",
       "\n",
       "       [[7833,    0],\n",
       "        [  26,    0]],\n",
       "\n",
       "       [[7848,    0],\n",
       "        [  10,    1]],\n",
       "\n",
       "       [[7853,    0],\n",
       "        [   6,    0]],\n",
       "\n",
       "       [[7847,    1],\n",
       "        [  11,    0]],\n",
       "\n",
       "       [[7851,    0],\n",
       "        [   8,    0]],\n",
       "\n",
       "       [[7855,    0],\n",
       "        [   4,    0]],\n",
       "\n",
       "       [[7843,    0],\n",
       "        [  16,    0]],\n",
       "\n",
       "       [[7822,    1],\n",
       "        [  22,   14]],\n",
       "\n",
       "       [[7855,    0],\n",
       "        [   4,    0]],\n",
       "\n",
       "       [[7856,    0],\n",
       "        [   3,    0]],\n",
       "\n",
       "       [[7851,    0],\n",
       "        [   7,    1]],\n",
       "\n",
       "       [[7839,    0],\n",
       "        [  20,    0]],\n",
       "\n",
       "       [[7734,   23],\n",
       "        [  31,   71]],\n",
       "\n",
       "       [[7832,    0],\n",
       "        [  27,    0]],\n",
       "\n",
       "       [[7859,    0],\n",
       "        [   0,    0]],\n",
       "\n",
       "       [[7800,    0],\n",
       "        [  59,    0]],\n",
       "\n",
       "       [[7852,    0],\n",
       "        [   7,    0]],\n",
       "\n",
       "       [[7844,    0],\n",
       "        [  15,    0]],\n",
       "\n",
       "       [[7849,    0],\n",
       "        [  10,    0]]])"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "\n",
    "multilabel_confusion_matrix(y_test, knn_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import auc\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "for i in range(n_classes):\n",
    "    fpr[i], tpr[i], _ = roc_curve(y_test[:, i], y_score[:, i])\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatically created module for IPython interactive environment\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yianding/opt/anaconda3/lib/python3.8/site-packages/sklearn/multiclass.py:76: UserWarning: Label not 34 is present in all training examples.\n",
      "  warnings.warn(\"Label %s is present in all training examples.\" %\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-265-fa8f8dbbc347>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     30\u001b[0m classifier = OneVsRestClassifier(svm.SVC(kernel='linear', probability=True,\n\u001b[1;32m     31\u001b[0m                                  random_state=random_state))\n\u001b[0;32m---> 32\u001b[0;31m \u001b[0my_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecision_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;31m# Compute ROC curve and ROC area for each class\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/multiclass.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    239\u001b[0m         \u001b[0;31m# n_jobs > 1 in can results in slower performance due to the overhead\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m         \u001b[0;31m# of spawning threads.  See joblib issue #112.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 241\u001b[0;31m         self.estimators_ = Parallel(n_jobs=self.n_jobs)(delayed(_fit_binary)(\n\u001b[0m\u001b[1;32m    242\u001b[0m             self.estimator, X, column, classes=[\n\u001b[1;32m    243\u001b[0m                 \u001b[0;34m\"not %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel_binarizer_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1030\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1031\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1032\u001b[0;31m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1033\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    845\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    846\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 847\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    848\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    763\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    764\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 765\u001b[0;31m             \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    766\u001b[0m             \u001b[0;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    767\u001b[0m             \u001b[0;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    206\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    570\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    571\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 572\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    573\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    250\u001b[0m         \u001b[0;31m# change the default number of processes to -1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 252\u001b[0;31m             return [func(*args, **kwargs)\n\u001b[0m\u001b[1;32m    253\u001b[0m                     for func, args, kwargs in self.items]\n\u001b[1;32m    254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    250\u001b[0m         \u001b[0;31m# change the default number of processes to -1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 252\u001b[0;31m             return [func(*args, **kwargs)\n\u001b[0m\u001b[1;32m    253\u001b[0m                     for func, args, kwargs in self.items]\n\u001b[1;32m    254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/multiclass.py\u001b[0m in \u001b[0;36m_fit_binary\u001b[0;34m(estimator, X, y, classes)\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0mestimator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m         \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/svm/_base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m         \u001b[0mseed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrnd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miinfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'i'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m         \u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msolver_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_seed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m         \u001b[0;31m# see comment on the other call to np.iinfo in this file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/svm/_base.py\u001b[0m in \u001b[0;36m_dense_fit\u001b[0;34m(self, X, y, sample_weight, solver_type, kernel, random_seed)\u001b[0m\n\u001b[1;32m    266\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msupport_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msupport_vectors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_support\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdual_coef_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintercept_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_probA\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 268\u001b[0;31m             self._probB, self.fit_status_ = libsvm.fit(\n\u001b[0m\u001b[1;32m    269\u001b[0m                 \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m                 \u001b[0msvm_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msolver_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(__doc__)\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import cycle\n",
    "\n",
    "from sklearn import svm, datasets\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from scipy import interp\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# # Import some data to play with\n",
    "# iris = datasets.load_iris()\n",
    "# X = iris.data\n",
    "# y = iris.target\n",
    "\n",
    "# Binarize the output\n",
    "y = label_binarize(train_targets_scored.iloc[:,1:], classes=classe)\n",
    "n_classes = y.shape[1]\n",
    "random_state = np.random.RandomState(39)\n",
    "\n",
    "# shuffle and split training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    train_pca, y, test_size=0.33, random_state=42)\n",
    "\n",
    "# Learn to predict each class against the other\n",
    "classifier = OneVsRestClassifier(svm.SVC(kernel='linear', probability=True,\n",
    "                                 random_state=random_state))\n",
    "y_score = classifier.fit(X_train, y_train).decision_function(X_test)\n",
    "\n",
    "# Compute ROC curve and ROC area for each class\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "for i in range(n_classes):\n",
    "    fpr[i], tpr[i], _ = roc_curve(y_test[:, i], y_score[:, i])\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "classe = []\n",
    "for i in range(206):\n",
    "    classe.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = list(y_train.columns)\n",
    "# Using pipeline for applying logistic regression and one vs rest classifier\n",
    "LogReg_pipeline = Pipeline([\n",
    "                ('clf', OneVsRestClassifier(LogisticRegression(solver='sag'), n_jobs=-1)),\n",
    "            ])\n",
    "for category in categories:\n",
    "    print('**Processing {} comments...**'.format(category))\n",
    "    \n",
    "    # Training logistic regression model on train data\n",
    "    LogReg_pipeline.fit(X_train, y_train[category])\n",
    "    \n",
    "    # calculating test accuracy\n",
    "    prediction = LogReg_pipeline.predict(X_test)\n",
    "    print('Test accuracy is {}'.format(accuracy_score(y_test[category], prediction)))\n",
    "    print(multilabel_confusion_matrix(y_test[category], prediction))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multilabel_confusion_matrix(y_test, knn_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "This solver needs samples of at least 2 classes in the data, but the data contains only one class: 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-203-f93dde21435f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultioutput\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMultiOutputClassifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mlogistic\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mMultiOutputClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLogisticRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrandom_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m999\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mlogistic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#Fitting the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m#Generating predictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/multioutput.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, Y, sample_weight, **fit_params)\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0mself\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    350\u001b[0m         \"\"\"\n\u001b[0;32m--> 351\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    352\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mestimator\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimators_\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/multioutput.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, **fit_params)\u001b[0m\n\u001b[1;32m    170\u001b[0m         \u001b[0mfit_params_validated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_fit_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m         self.estimators_ = Parallel(n_jobs=self.n_jobs)(\n\u001b[0m\u001b[1;32m    173\u001b[0m             delayed(_fit_estimator)(\n\u001b[1;32m    174\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1030\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1031\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1032\u001b[0;31m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1033\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    845\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    846\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 847\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    848\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    763\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    764\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 765\u001b[0;31m             \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    766\u001b[0m             \u001b[0;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    767\u001b[0m             \u001b[0;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    206\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    570\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    571\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 572\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    573\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    250\u001b[0m         \u001b[0;31m# change the default number of processes to -1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 252\u001b[0;31m             return [func(*args, **kwargs)\n\u001b[0m\u001b[1;32m    253\u001b[0m                     for func, args, kwargs in self.items]\n\u001b[1;32m    254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    250\u001b[0m         \u001b[0;31m# change the default number of processes to -1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 252\u001b[0;31m             return [func(*args, **kwargs)\n\u001b[0m\u001b[1;32m    253\u001b[0m                     for func, args, kwargs in self.items]\n\u001b[1;32m    254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/multioutput.py\u001b[0m in \u001b[0;36m_fit_estimator\u001b[0;34m(estimator, X, y, sample_weight, **fit_params)\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m         \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   1370\u001b[0m         \u001b[0mclasses_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1371\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mn_classes\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1372\u001b[0;31m             raise ValueError(\"This solver needs samples of at least 2 classes\"\n\u001b[0m\u001b[1;32m   1373\u001b[0m                              \u001b[0;34m\" in the data, but the data contains only one\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1374\u001b[0m                              \" class: %r\" % classes_[0])\n",
      "\u001b[0;31mValueError\u001b[0m: This solver needs samples of at least 2 classes in the data, but the data contains only one class: 0"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "logistic= MultiOutputClassifier(LogisticRegression(max_iter=10000, tol=0.1, C = 0.5,verbose=0,random_state = 999))\n",
    "logistic.fit(X_train,y_train)#Fitting the model \n",
    "\n",
    "#Generating predictions\n",
    "pred_log_proba=logistic.predict_proba(X_test)\n",
    "pred_log_proba_t=pred_transform(pred_log_proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/kushal1506/deciding-n-components-in-pca?select=test_features.csv\n",
    "https://www.kaggle.com/arpitsolanki14/moa-exploratory-analysis-pca-ensemble-models#Dimensionality-Reduction---Principal-Component-Analysis-\n",
    "https://www.sciencedirect.com/science/article/abs/pii/S0950705115002737\n",
    "https://arxiv.org/pdf/1106.1813.pdf\n",
    "https://github.com/niteshsukhwani/MLSMOTE/blob/master/mlsmote.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
